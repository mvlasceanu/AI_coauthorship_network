conference,year,index,paper_id,given_name,family_name,org,institution,email,title
neurips,2019,0,8547,Coline,Devin,,UC Berkeley,,Compositional Plan Vectors
neurips,2019,1,8547,Daniel,Geng,,UC Berkeley,,Compositional Plan Vectors
neurips,2019,2,8547,Pieter,Abbeel,,UC Berkeley & covariant.ai,,Compositional Plan Vectors
neurips,2019,3,8547,Trevor,Darrell,,UC Berkeley,,Compositional Plan Vectors
neurips,2019,4,8547,Sergey,Levine,,UC Berkeley,,Compositional Plan Vectors
neurips,2019,0,610,LU,LIU,uts,University of Technology Sydney,lu.liu-10@student.uts.edu.au,Learning to Propagate for Graph Meta-Learning
neurips,2019,1,610,Tianyi,Zhou,uw,"University of Washington, Seattle",tianyizh@uw.edu,Learning to Propagate for Graph Meta-Learning
neurips,2019,2,610,Guodong,Long,uts,University of Technology Sydney (UTS),guodong.long@uts.edu.au,Learning to Propagate for Graph Meta-Learning
neurips,2019,3,610,Jing,Jiang,uts,University of Technology Sydney,jing.jiang@uts.edu.au,Learning to Propagate for Graph Meta-Learning
neurips,2019,4,610,Chengqi,Zhang,uts,University of Technology Sydney,chengqi.zhang@uts.edu.au,Learning to Propagate for Graph Meta-Learning
neurips,2019,0,1164,Niv,Nayman,,Alibaba Group,,XNAS: Neural Architecture Search with Expert Advice
neurips,2019,1,1164,Asaf,Noy,,Alibaba,,XNAS: Neural Architecture Search with Expert Advice
neurips,2019,2,1164,Tal,Ridnik,,MIIL Alibaba,,XNAS: Neural Architecture Search with Expert Advice
neurips,2019,3,1164,Itamar,Friedman,,Alibaba,,XNAS: Neural Architecture Search with Expert Advice
neurips,2019,4,1164,Rong,Jin,,Alibaba,,XNAS: Neural Architecture Search with Expert Advice
neurips,2019,5,1164,Lihi,Zelnik,,Alibaba,,XNAS: Neural Architecture Search with Expert Advice
neurips,2019,0,7845,Oliver,Hamelijnck,turing,The Alan Turing Institute,tdamoulas@turing.ac.uk,Multi-resolution Multi-task Gaussian Processes
neurips,2019,1,7845,Theodoros,Damoulas,turing,University of Warwick & The Alan Turing Institute,ohamelijnck@turing.ac.uk,Multi-resolution Multi-task Gaussian Processes
neurips,2019,2,7845,Kangrui,Wang,turing,The Alan Turing Institute,kwang@turing.ac.uk,Multi-resolution Multi-task Gaussian Processes
neurips,2019,3,7845,Mark,Girolami,turing,Imperial College London,mgirolami@turing.ac.uk,Multi-resolution Multi-task Gaussian Processes
neurips,2019,0,348,Shaojie,Bai,,Carnegie Mellon University,,Deep Equilibrium Models
neurips,2019,1,348,J. Zico,Kolter,,Carnegie Mellon University / Bosch Center for AI,,Deep Equilibrium Models
neurips,2019,2,348,Vladlen,Koltun,,Intel Labs,,Deep Equilibrium Models
neurips,2019,0,2213,Ruibing,Hou,ict,Institute of Computing TechnologyChinese Academy,ruibing.hou@vipl.ict.ac.cn,Cross Attention Network for Few-shot Classification
neurips,2019,1,2213,Hong,Chang,ict,"Institute of Computing Technology, Chinese Academy of Sciences",changhong@ict.ac.cn,Cross Attention Network for Few-shot Classification
neurips,2019,2,2213,Bingpeng,MA,ict,University of Chinese Academy of Sciences,sgshan@ict.ac.cn,Cross Attention Network for Few-shot Classification
neurips,2019,3,2213,Shiguang,Shan,ict,Chinese Academy of Sciences,xlchen@ict.ac.cn,Cross Attention Network for Few-shot Classification
neurips,2019,4,2213,Xilin,Chen,ucas,"Institute of Computing Technology, Chinese Academy of Sciences",bpma@ucas.ac.cn,Cross Attention Network for Few-shot Classification
neurips,2019,0,1283,Arsalan,Sharifnassab,gmail,Sharif University of Technology,a.sharifnassab@gmail.com,Order Optimal One-Shot Distributed Learning
neurips,2019,1,1283,Saber,Salehkaleybar,sharif,Sharif University of Technology,saleh@sharif.edu,Order Optimal One-Shot Distributed Learning
neurips,2019,2,1283,S. Jamaloddin,Golestani,sharif,Sharif University of Technology,golestani@sharif.edu,Order Optimal One-Shot Distributed Learning
neurips,2019,0,8279,Ke,Wang,,Cornell University,,Exact Gaussian Processes on a Million Data Points
neurips,2019,1,8279,Geoff,Pleiss,,Cornell University,,Exact Gaussian Processes on a Million Data Points
neurips,2019,2,8279,Jacob,Gardner,,Uber AI Labs,,Exact Gaussian Processes on a Million Data Points
neurips,2019,3,8279,Stephen,Tyree,,NVIDIA,,Exact Gaussian Processes on a Million Data Points
neurips,2019,4,8279,Kilian,Weinberger,,Cornell University / ASAPP Research,,Exact Gaussian Processes on a Million Data Points
neurips,2019,5,8279,Andrew Gordon,Wilson,,New York University,,Exact Gaussian Processes on a Million Data Points
neurips,2019,0,1457,Haowei,He,tsinghua,Tsinghua University,hhw19@mails.tsinghua.edu.cn,Asymmetric Valleys: Beyond Sharp and Flat Local Minima
neurips,2019,1,1457,Gao,Huang,tsinghua,Tsinghua,gaohuang@tsinghua.edu.cn,Asymmetric Valleys: Beyond Sharp and Flat Local Minima
neurips,2019,2,1457,Yang,Yuan,tsinghua,Cornell University,yuanyang@tsinghua.edu.cn,Asymmetric Valleys: Beyond Sharp and Flat Local Minima
neurips,2019,0,7779,Viet Anh,Nguyen,epfl,EPFL,viet-anh.nguyen@epfl.ch,Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization
neurips,2019,1,7779,Soroosh,Shafieezadeh Abadeh,epfl,EPFL,soroosh.shafiee@epfl.ch,Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization
neurips,2019,2,7779,Man-Chung,Yue,polyu,The Hong Kong Polytechnic University,manchung.yue@polyu.edu.hk,Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization
neurips,2019,3,7779,Daniel,Kuhn,epfl,EPFL,daniel.kuhn@epfl.ch,Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization
neurips,2019,4,7779,Wolfram,Wiesemann,imperial,Imperial College,ww@imperial.ac.uk,Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization
neurips,2019,0,6627,Pooria,Joulani,google,DeepMind,pjoulani@google.com,"Think out of the ""Box"": Generically-Constrained Asynchronous Composite Optimization and Hedging"
neurips,2019,1,6627,András,György,google,DeepMind,agyorgy@google.com,"Think out of the ""Box"": Generically-Constrained Asynchronous Composite Optimization and Hedging"
neurips,2019,2,6627,Csaba,Szepesvari,google,DeepMind / University of Alberta,szepi@google.com,"Think out of the ""Box"": Generically-Constrained Asynchronous Composite Optimization and Hedging"
neurips,2019,0,2176,Tuomas,Kynkäänniemi,aalto,NVIDIA; Aalto University,tuomas.kynkaanniemi@aalto.fi,Improved Precision and Recall Metric for Assessing Generative Models
neurips,2019,1,2176,Tero,Karras,nvidia,NVIDIA,jlehtinen@nvidia.com,Improved Precision and Recall Metric for Assessing Generative Models
neurips,2019,2,2176,Samuli,Laine,nvidia,NVIDIA,tkarras@nvidia.com,Improved Precision and Recall Metric for Assessing Generative Models
neurips,2019,3,2176,Jaakko,Lehtinen,nvidia,Aalto University & NVIDIA,slaine@nvidia.com,Improved Precision and Recall Metric for Assessing Generative Models
neurips,2019,4,2176,Timo,Aila,nvidia,NVIDIA,taila@nvidia.com,Improved Precision and Recall Metric for Assessing Generative Models
neurips,2019,0,6067,Arun,Jambulapati,stanford,Stanford University,jmblpati@stanford.edu,A Direct tilde{O}(1/epsilon) Iteration Parallel Algorithm for Optimal Transport
neurips,2019,1,6067,Aaron,Sidford,stanford,Stanford,sidford@stanford.edu,A Direct tilde{O}(1/epsilon) Iteration Parallel Algorithm for Optimal Transport
neurips,2019,2,6067,Kevin,Tian,stanford,Stanford University,kjtian@stanford.edu,A Direct tilde{O}(1/epsilon) Iteration Parallel Algorithm for Optimal Transport
neurips,2019,0,245,Maxime,Bucher,valeo,Valeo.ai,maxime.bucher@valeo.com,Zero-Shot Semantic Segmentation
neurips,2019,1,245,Tuan-Hung,VU,lip6,Valeo.ai,matthieu.cord@lip6.fr,Zero-Shot Semantic Segmentation
neurips,2019,2,245,Matthieu,Cord,valeo,Sorbonne University,tuan-hung.vu@valeo.com,Zero-Shot Semantic Segmentation
neurips,2019,3,245,Patrick,Pérez,valeo,Valeo.ai,patrick.perez@valeo.com,Zero-Shot Semantic Segmentation
neurips,2019,0,841,Pascal,Mettes,,University of Amsterdam,,Hyperspherical Prototype Networks
neurips,2019,1,841,Elise,van der Pol,,University of Amsterdam,,Hyperspherical Prototype Networks
neurips,2019,2,841,Cees,Snoek,,University of Amsterdam,,Hyperspherical Prototype Networks
neurips,2019,0,4082,Arjun Nitin,Bhagoji,princeton,Princeton University,abhagoji@princeton.edu,Lower Bounds on Adversarial Robustness from Optimal Transport
neurips,2019,1,4082,Daniel,Cullina,psu,Penn State University,cullina@psu.edu,Lower Bounds on Adversarial Robustness from Optimal Transport
neurips,2019,2,4082,Prateek,Mittal,princeton,Princeton University,pmittal@princeton.edu,Lower Bounds on Adversarial Robustness from Optimal Transport
neurips,2019,0,2215,Qing,Qu,nyu,New York University,qq213@nyu.edu,A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution
neurips,2019,1,2215,Xiao,Li,cuhk,The Chinese University of Hong Kong,xli@ee.cuhk.edu.hk,A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution
neurips,2019,2,2215,Zhihui,Zhu,jhu,Johns Hopkins University,zzhu29@jhu.edu,A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution
neurips,2019,0,6749,Meire,Fortunato,,DeepMind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,1,6749,Melissa,Tan,,Deepmind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,2,6749,Ryan,Faulkner,,Deepmind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,3,6749,Steven,Hansen,,DeepMind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,4,6749,Adrià,Puigdomènech Badia,,Google DeepMind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,5,6749,Gavin,Buttimore,,DeepMind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,6,6749,Charles,Deck,,Deepmind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,7,6749,Joel,Leibo,,DeepMind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,8,6749,Charles,Blundell,,DeepMind,,Generalization of Reinforcement Learners with Working and Episodic Memory
neurips,2019,0,6218,Xingyu,Cai,,University of Connecticut,,DTWNet: a Dynamic Time Warping Network
neurips,2019,1,6218,Tingyang,Xu,,Tencent AI Lab,,DTWNet: a Dynamic Time Warping Network
neurips,2019,2,6218,Jinfeng,Yi,,JD Research,,DTWNet: a Dynamic Time Warping Network
neurips,2019,3,6218,Junzhou,Huang,,University of Texas at Arlington / Tencent AI Lab,,DTWNet: a Dynamic Time Warping Network
neurips,2019,4,6218,Sanguthevar,Rajasekaran,,University of Connecticut,,DTWNet: a Dynamic Time Warping Network
neurips,2019,0,2751,Xin,Guo,berkeley,"University of California, Berkeley",xinguo@berkeley.edu,Learning Mean-Field Games
neurips,2019,1,2751,Anran,Hu,berkeley,"University of Californian, Berkeley (UC Berkeley)",anran_hu@berkeley.edu,Learning Mean-Field Games
neurips,2019,2,2751,Renyuan,Xu,berkeley,University of Oxford,renyuanxu@berkeley.edu,Learning Mean-Field Games
neurips,2019,3,2751,Junzi,Zhang,stanford,Stanford University,junziz@stanford.edu,Learning Mean-Field Games
neurips,2019,0,195,Zihan,Li,nus,National University of Singapore,lizihan@u.nus.edu,Learning Erdos-Renyi Random Graphs via Edge Detecting Queries
neurips,2019,1,195,Matthias,Fresacher,adelaide,University of Adelaide,matthias.fresacher@adelaide.edu.au,Learning Erdos-Renyi Random Graphs via Edge Detecting Queries
neurips,2019,2,195,Jonathan,Scarlett,nus,National University of Singapore,scarlett@comp.nus.edu.sg,Learning Erdos-Renyi Random Graphs via Edge Detecting Queries
neurips,2019,0,8228,Brandon,Anderson,uchicago,University of Chicago,hytruongson@uchicago.edu,Cormorant: Covariant Molecular Neural Networks
neurips,2019,1,8228,Truong Son,Hy,uchicago,The University of Chicago,risi@uchicago.edu,Cormorant: Covariant Molecular Neural Networks
neurips,2019,2,8228,Risi,Kondor,uchicago,U. Chicago,brandona@jfi.uchicago.edu,Cormorant: Covariant Molecular Neural Networks
neurips,2019,0,8768,Fabio,Vitale,google,University of Lille - INRIA Lille (France),anandbr@google.com,Flattening a Hierarchical Clustering through Active Learning
neurips,2019,1,8768,Anand,Rajagopalan,google,Google,cgentile@google.com,Flattening a Hierarchical Clustering through Active Learning
neurips,2019,2,8768,Claudio,Gentile,inria,Google Research,fabio.vitale@inria.fr,Flattening a Hierarchical Clustering through Active Learning
neurips,2019,0,6991,Stefan,Meintrup,tu-dortmund,TU Dortmund,stefan.meintrup@tu-dortmund.de,Random Projections and Sampling Algorithms for Clustering of High-Dimensional Polygonal Curves
neurips,2019,1,6991,Alexander,Munteanu,tu-dortmund,TU Dortmund,alexander.munteanu@tu-dortmund.de,Random Projections and Sampling Algorithms for Clustering of High-Dimensional Polygonal Curves
neurips,2019,2,6991,Dennis,Rohde,tu-dortmund,TU Dortmund,dennis.rohde@tu-dortmund.de,Random Projections and Sampling Algorithms for Clustering of High-Dimensional Polygonal Curves
neurips,2019,0,5001,Mikael,Henaff,microsoft,Microsoft Research,mihenaff@microsoft.com,Explicit Explore-Exploit Algorithms in Continuous State Spaces
neurips,2019,0,4222,Dennis Maximilian,Elbrächter,,University of Vienna,,How degenerate is the parametrization of neural networks with the ReLU activation function?
neurips,2019,1,4222,Julius,Berner,,University of Vienna,,How degenerate is the parametrization of neural networks with the ReLU activation function?
neurips,2019,2,4222,Philipp,Grohs,,University of Vienna,,How degenerate is the parametrization of neural networks with the ReLU activation function?
neurips,2019,0,2699,Ines,Chami,stanford,Stanford University,chami@cs.stanford.edu,Hyperbolic Graph Convolutional Neural Networks
neurips,2019,1,2699,Zhitao,Ying,stanford,Stanford University,rexying@cs.stanford.edu,Hyperbolic Graph Convolutional Neural Networks
neurips,2019,2,2699,Christopher,Ré,stanford,Stanford,chrismre@cs.stanford.edu,Hyperbolic Graph Convolutional Neural Networks
neurips,2019,3,2699,Jure,Leskovec,stanford,Stanford University and Pinterest,jure@cs.stanford.edu,Hyperbolic Graph Convolutional Neural Networks
neurips,2019,0,4463,Yu,Meng,illinois,University of Illinois at Urbana-Champaign,yumeng5@illinois.edu,Spherical Text Embedding
neurips,2019,1,4463,Jiaxin,Huang,illinois,University of Illinois Urbana-Champaign,jiaxinh3@illinois.edu,Spherical Text Embedding
neurips,2019,2,4463,Guangyuan,Wang,illinois,UIUC,gwang10@illinois.edu,Spherical Text Embedding
neurips,2019,3,4463,Chao,Zhang,illinois,Georgia Institute of Technology,hzhuang3@illinois.edu,Spherical Text Embedding
neurips,2019,4,4463,Honglei,Zhuang,illinois,Google Research,hanj@illinois.edu,Spherical Text Embedding
neurips,2019,5,4463,Lance,Kaplan,gatech,U.S. Army Research Laboratory,chaozhang@gatech.edu,Spherical Text Embedding
neurips,2019,6,4463,Jiawei,Han,mail,UIUC,lance.m.kaplan.civ@mail.mil,Spherical Text Embedding
neurips,2019,0,5087,Shufei,Ge,sfu,Simon Fraser University,shufei_ge@sfu.ca,Random Tessellation Forests
neurips,2019,1,5087,Shijia,Wang,sfu,Nankai University,shijia_wang@sfu.ca,Random Tessellation Forests
neurips,2019,2,5087,Yee Whye,Teh,ox,"University of Oxford, DeepMind",y.w.teh@stats.ox.ac.uk,Random Tessellation Forests
neurips,2019,3,5087,Liangliang,Wang,sfu,Simon Fraser University,liangliang_wang@sfu.ca,Random Tessellation Forests
neurips,2019,4,5087,Lloyd,Elliott,sfu,Simon Fraser University,lloyd_elliott@sfu.ca,Random Tessellation Forests
neurips,2019,0,2754,Igor,Fedorov,arm,Arm Research,igor.fedorov@arm.com,SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers
neurips,2019,1,2754,Ryan,Adams,princeton,Princeton University,rpa@princeton.edu,SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers
neurips,2019,2,2754,Matthew,Mattina,arm,ARM,matthew.mattina@arm.com,SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers
neurips,2019,3,2754,Paul,Whatmough,arm,Arm Research,paul.whatmough@arm.com,SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers
neurips,2019,0,1911,Kamalika,Chaudhuri,ucsd,UCSD,kamalika@cs.ucsd.edu,Capacity Bounded Differential Privacy
neurips,2019,1,1911,Jacob,Imola,ucsd,UCSD,jimola@eng.ucsd.edu,Capacity Bounded Differential Privacy
neurips,2019,2,1911,Ashwin,Machanavajjhala,duke,Duke,ashwin@cs.duke.edu,Capacity Bounded Differential Privacy
neurips,2019,0,5904,Jeffrey,Negrea,,University of Toronto,,Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates
neurips,2019,1,5904,Mahdi,Haghifam,,University of Toronto,,Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates
neurips,2019,2,5904,Gintare Karolina,Dziugaite,,Element AI,,Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates
neurips,2019,3,5904,Ashish,Khisti,,University of Toronto,,Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates
neurips,2019,4,5904,Daniel,Roy,,Univ of Toronto & Vector,,Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates
neurips,2019,0,6893,Kiran,Thekumparampil,illinois,Univ. of Illinois at Urbana-Champaign,EfcientAlgorithmsforSmoothMinimaxOptimizationKiranKoshyThekumprampilUniversityofIllinoisatUrbana-Champaignthekump2@illinois.eduPrateekJainMicrosoftResearch,Efficient Algorithms for Smooth Minimax Optimization
neurips,2019,1,6893,Prateek,Jain,microsoft,Microsoft Research,Indiaprajain@microsoft.comPraneethNetrapalliMicrosoftResearch,Efficient Algorithms for Smooth Minimax Optimization
neurips,2019,2,6893,Praneeth,Netrapalli,microsoft,Microsoft Research,Indiapraneeth@microsoft.comSewoongOhUniversityofWashington,Efficient Algorithms for Smooth Minimax Optimization
neurips,2019,3,6893,Sewoong,Oh,washington,University of Washington,Seattlesewoong@cs.washington.edu,Efficient Algorithms for Smooth Minimax Optimization
neurips,2019,0,6214,Vaishnavh,Nagarajan,cmu,Carnegie Mellon University,vaishnavh@cs.cmu.edu,Uniform convergence may be unable to explain generalization in deep learning
neurips,2019,1,6214,J. Zico,Kolter,cmu,Carnegie Mellon University / Bosch Center for AI,zkolter@cs.cmu.edu,Uniform convergence may be unable to explain generalization in deep learning
neurips,2019,0,1909,Pierre,Bellec,rutgers,Rutgers,pierre.bellec@rutgers.edu,First order expansion of convex regularized estimators
neurips,2019,1,1909,Arun,Kuchibhotla,upenn,Wharton Statistics,arunku@upenn.edu,First order expansion of convex regularized estimators
neurips,2019,0,8832,Jack,Umenberger,uu,Uppsala University,jack.umenberger@it.uu.se,Robust exploration in linear quadratic reinforcement learning
neurips,2019,1,8832,Mina,Ferizbegovic,uu,KTH Royal Institute of Technology,thomas.schon@it.uu.se,Robust exploration in linear quadratic reinforcement learning
neurips,2019,2,8832,Thomas,Schön,kth,Uppsala University,minafe@kth.se,Robust exploration in linear quadratic reinforcement learning
neurips,2019,3,8832,Håkan,Hjalmarsson,kth,KTH,hjalmars@kth.se,Robust exploration in linear quadratic reinforcement learning
neurips,2019,0,2389,Raanan,Yehezkel Rohekar,intel,Intel AI Lab,raanan.yehezkel@intel.com,Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections
neurips,2019,1,2389,Yaniv,Gurwicz,intel,Intel AI Lab,shami.nisimov@intel.com,Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections
neurips,2019,2,2389,Shami,Nisimov,intel,Intel AI Lab,yaniv.gurwicz@intel.com,Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections
neurips,2019,3,2389,Gal,Novik,intel,Intel AI Lab,gal.novik@intel.com,Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections
neurips,2019,0,3383,Aaron,Klein,uni-freiburg,Amazon Berlin,kleinaa@cs.uni-freiburg.de,Meta-Surrogate Benchmarking for Hyperparameter Optimization
neurips,2019,1,3383,Zhenwen,Dai,uni-freiburg,Amazon,fh@cs.uni-freiburg.de,Meta-Surrogate Benchmarking for Hyperparameter Optimization
neurips,2019,2,3383,Frank,Hutter,amazon,University of Freiburg & Bosch,zhenwend@amazon.com,Meta-Surrogate Benchmarking for Hyperparameter Optimization
neurips,2019,3,3383,Neil,Lawrence,amazon,Amazon,gojav@amazon.com,Meta-Surrogate Benchmarking for Hyperparameter Optimization
neurips,2019,4,3383,Javier,Gonzalez,cam,Amazon.com,ndl21@cam.ac.uk,Meta-Surrogate Benchmarking for Hyperparameter Optimization
neurips,2019,0,4630,Surbhi,Goel,utexas,The University of Texas at Austin,surbhi@cs.utexas.edu,Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals
neurips,2019,1,4630,Sushrut,Karmalkar,utexas,The University of Texas at Austin,sushrutk@cs.utexas.edu,Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals
neurips,2019,2,4630,Adam,Klivans,utexas,UT Austin,klivans@cs.utexas.edu,Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals
neurips,2019,0,7725,Sayak,Ray Chowdhury,iisc,Indian Institute of Science,sayak@iisc.ac.in,Bayesian Optimization under Heavy-tailed Payoffs
neurips,2019,1,7725,Aditya,Gopalan,iisc,Indian Institute of Science,aditya@iisc.ac.in,Bayesian Optimization under Heavy-tailed Payoffs
neurips,2019,0,6770,Meera,Pai,iitb,Indian Institute of Technology Bombay,meeravpai@ee.iitb.ac.in,Distribution Learning of a Random Spatial Field with a Location-Unaware Mobile Sensor
neurips,2019,1,6770,Animesh,Kumar,iitb,Indian Institute of Technology Bombay,animesh@ee.iitb.ac.in,Distribution Learning of a Random Spatial Field with a Location-Unaware Mobile Sensor
neurips,2019,0,2520,Yaqi,Duan,,Princeton University,,State Aggregation Learning from Markov Transition Data
neurips,2019,1,2520,Tracy,Ke,,Harvard University,,State Aggregation Learning from Markov Transition Data
neurips,2019,2,2520,Mengdi,Wang,,Princeton University,,State Aggregation Learning from Markov Transition Data
neurips,2019,0,3425,Nicki,Skafte,dtu,Technical University of Denmark,nsde@dtu.dk,Reliable training and estimation of variance networks
neurips,2019,1,3425,Martin,Jørgensen,dtu,Technical University of Denmark,marjor@dtu.dk,Reliable training and estimation of variance networks
neurips,2019,2,3425,Søren,Hauberg,dtu,Technical University of Denmark,sohau@dtu.dk,Reliable training and estimation of variance networks
neurips,2019,0,60,Aravind,Rajeswaran,washington,University of Washington,aravraj@cs.washington.edu,Meta-Learning with Implicit Gradients
neurips,2019,1,60,Chelsea,Finn,stanford,Stanford University,cbfinn@cs.stanford.edu,Meta-Learning with Implicit Gradients
neurips,2019,2,60,Sham,Kakade,washington,University of Washington,sham@cs.washington.edu,Meta-Learning with Implicit Gradients
neurips,2019,3,60,Sergey,Levine,berkeley,UC Berkeley,svlevine@eecs.berkeley.edu,Meta-Learning with Implicit Gradients
neurips,2019,0,2273,Mikko,Heikkilä,helsinki,University of Helsinki,mikko.a.heikkila@helsinki.fi,Differentially Private Markov Chain Monte Carlo
neurips,2019,1,2273,Joonas,Jälkö,aalto,Aalto University,joonas.jalko@aalto.fi,Differentially Private Markov Chain Monte Carlo
neurips,2019,2,2273,Onur,Dikmen,hh,Halmstad University,onur.dikmen@hh.se,Differentially Private Markov Chain Monte Carlo
neurips,2019,3,2273,Antti,Honkela,helsinki,University of Helsinki,antti.honkela@helsinki.fi,Differentially Private Markov Chain Monte Carlo
neurips,2019,0,1914,Trevor,Campbell,ubc,UBC,trevor@stat.ubc.ca,Universal Boosting Variational Inference
neurips,2019,1,1914,Xinglong,Li,ubc,The University of British Columbia,xinglong.li@stat.ubc.ca,Universal Boosting Variational Inference
neurips,2019,0,2448,Yali,Du,ucl,University College London,yali.du@ucl.ac.uk,LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning
neurips,2019,1,2448,Lei,Han,gmail,Tencent AI Lab,leihan.cs@gmail.com,LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning
neurips,2019,2,2448,Meng,Fang,tencent,Tencent,mfang@tencent.com,LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning
neurips,2019,3,2448,Ji,Liu,imperial,"University of Rochester, Tencent AI lab",tianhong.dai15@imperial.ac.uk,LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning
neurips,2019,4,2448,Tianhong,Dai,gmail,Imperial College London,ji.liu.uwisc@gmail.com,LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning
neurips,2019,5,2448,Dacheng,Tao,sydney,University of Sydney,dacheng.tao@sydney.edu.au,LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning
neurips,2019,0,2072,Wenhao,Zhang,pitt,Carnegie Mellon & U. of Pittsburgh,wenhao.zhang@pitt.edu,A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits
neurips,2019,1,2072,Si,Wu,pku,Peking University,siwu@pku.edu.cn,A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits
neurips,2019,2,2072,Brent,Doiron,pitt,University of Pittsburgh,bdoiron@pitt.edu,A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits
neurips,2019,3,2072,Tai Sing,Lee,cmu,Carnegie Mellon University,tai@cnbc.cmu.edu,A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits
neurips,2019,0,9262,Randall,Balestriero,,Rice University,,The Geometry of Deep Networks: Power Diagram Subdivision
neurips,2019,1,9262,Romain,Cosentino,,Rice University,,The Geometry of Deep Networks: Power Diagram Subdivision
neurips,2019,2,9262,Behnaam,Aazhang,,Rice University,,The Geometry of Deep Networks: Power Diagram Subdivision
neurips,2019,3,9262,Richard,Baraniuk,,Rice University,,The Geometry of Deep Networks: Power Diagram Subdivision
neurips,2019,0,4814,Yahav,Bechavod,huji,Hebrew University,yahav.bechavod@cs.huji.ac.il,Equal Opportunity in Online Classification with Partial Feedback
neurips,2019,1,4814,Katrina,Ligett,huji,Hebrew University,katrina@cs.huji.ac.il,Equal Opportunity in Online Classification with Partial Feedback
neurips,2019,2,4814,Aaron,Roth,upenn,University of Pennsylvania,aaroth@cis.upenn.edu,Equal Opportunity in Online Classification with Partial Feedback
neurips,2019,3,4814,Bo,Waggoner,colorado,"U. Colorado, Boulder",bwag@colorado.edu,Equal Opportunity in Online Classification with Partial Feedback
neurips,2019,4,4814,Steven,Wu,umn,University of Minnesota,zsw@umn.edu,Equal Opportunity in Online Classification with Partial Feedback
neurips,2019,0,8611,Victor,Chernozhukov,mit,MIT,mdemirer@mit.edu,Semi-Parametric Efficient Policy Learning with Continuous Actions
neurips,2019,1,8611,Mert,Demirer,microsoft,MIT,vasy@microsoft.com,Semi-Parametric Efficient Policy Learning with Continuous Actions
neurips,2019,2,8611,Greg,Lewis,microsoft,Microsoft Research,glewis@microsoft.com,Semi-Parametric Efficient Policy Learning with Continuous Actions
neurips,2019,3,8611,Vasilis,Syrgkanis,mit,Microsoft Research,vchern@mit.edu,Semi-Parametric Efficient Policy Learning with Continuous Actions
neurips,2019,0,6260,Sanjay P.,Bhat,tcs,Tata Consultancy Services Limited,sanjay.bhat@tcs.com,Concentration of risk measures: A Wasserstein distance approach
neurips,2019,1,6260,Prashanth,L.A.,iitm,IIT Madras,prashla@cse.iitm.ac.in,Concentration of risk measures: A Wasserstein distance approach
neurips,2019,0,3743,DongDong,Ge,shufe,Shanghai University of Finance and Economics,ge.dongdong@mail.shufe.edu.cn,Interior-Point Methods Strike Back: Solving the Wasserstein Barycenter Problem
neurips,2019,1,3743,Haoyue,Wang,fudan,Fudan University,haoyuewang14@fudan.edu.cn,Interior-Point Methods Strike Back: Solving the Wasserstein Barycenter Problem
neurips,2019,2,3743,Zikai,Xiong,fudan,Fudan University,zkxiong16@fudan.edu.cn,Interior-Point Methods Strike Back: Solving the Wasserstein Barycenter Problem
neurips,2019,3,3743,Yinyu,Ye,stanford,Standord,yyye@stanford.edu,Interior-Point Methods Strike Back: Solving the Wasserstein Barycenter Problem
neurips,2019,0,2029,Cheng,Fu,ucsd,"University of California, San Diego",cfu@ucsd.edu,Coda: An End-to-End Neural Program Decompiler
neurips,2019,1,2029,Huili,Chen,ucsd,UCSD,huc044@ucsd.edu,Coda: An End-to-End Neural Program Decompiler
neurips,2019,2,2029,Haolan,Liu,ucsd,UCSD,hal022@ucsd.edu,Coda: An End-to-End Neural Program Decompiler
neurips,2019,3,2029,Xinyun,Chen,berkeley,UC Berkeley,xinyun.chen@berkeley.edu,Coda: An End-to-End Neural Program Decompiler
neurips,2019,4,2029,Yuandong,Tian,fb,Facebook AI Research,yuandong@fb.com,Coda: An End-to-End Neural Program Decompiler
neurips,2019,5,2029,Farinaz,Koushanfar,ucsd,UCSD,farinaz@ucsd.edu,Coda: An End-to-End Neural Program Decompiler
neurips,2019,6,2029,Jishen,Zhao,ucsd,UCSD,jzhao@ucsd.edu,Coda: An End-to-End Neural Program Decompiler
neurips,2019,0,59,Yanping,Huang,,Google Brain,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,1,59,Youlong,Cheng,,Google,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,2,59,Ankur,Bapna,,Google,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,3,59,Orhan,Firat,,Google,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,4,59,Dehao,Chen,,Google,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,5,59,Mia,Chen,,Google Brain,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,6,59,HyoukJoong,Lee,,Google,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,7,59,Jiquan,Ngiam,,Google Brain,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,8,59,Quoc,Le,,Google,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,9,59,Yonghui,Wu,,Google,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,10,59,zhifeng,Chen,,Google Brain,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
neurips,2019,0,7667,Suhas,Jayaram Subramanya,cmu,Carnegie Mellon University,suhas@cmu.edu,DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node
neurips,2019,1,7667,Fnu,Devvrit,gmail,University of Texas at Austin,devvrit.03@gmail.com,DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node
neurips,2019,2,7667,Harsha Vardhan,Simhadri,texas,Microsoft Research,rak@cs.texas.edu,DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node
neurips,2019,3,7667,Ravishankar,Krishnawamy,microsoft,Microsoft Research India,rakri@microsoft.com,DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node
neurips,2019,4,7667,Rohan,Kadekodi,microsoft,The University of Texas at Austin,harshasi@microsoft.com,DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node
neurips,2019,0,4958,Sanae,Amani,ucsb,University of California Santa Barbara,samanigeshnigani@ucsb.edu,Linear Stochastic Bandits Under Safety Constraints
neurips,2019,1,4958,Mahnoosh,Alizadeh,ucsb,University of California Santa Barbara,alizadeh@ucsb.edu,Linear Stochastic Bandits Under Safety Constraints
neurips,2019,2,4958,Christos,Thrampoulidis,ucsb,UCSB,cthrampo@ucsb.edu,Linear Stochastic Bandits Under Safety Constraints
neurips,2019,0,8941,Jingbo,Liu,mit,MIT,jingbo@mit.edu,Power analysis of knockoff filters for correlated designs
neurips,2019,1,8941,Philippe,Rigollet,mit,MIT,rigollet@math.mit.edu,Power analysis of knockoff filters for correlated designs
neurips,2019,0,1863,Vaishak,Belle,wustl,University of Edinburgh & Alan Turing Institute,bjuba@wustl.edu,Implicitly learning to reason in first-order logic
neurips,2019,1,1863,Brendan,Juba,ed,Washington University in St. Louis,vaishak@ed.ac.uk,Implicitly learning to reason in first-order logic
neurips,2019,0,8962,Jonas,Mueller,mit,Amazon Web Services,jonasmueller@csail.mit.edu,Low-Rank Bandit Methods for High-Dimensional Dynamic Pricing
neurips,2019,1,8962,Vasilis,Syrgkanis,microsoft,Microsoft Research,vasy@microsoft.com,Low-Rank Bandit Methods for High-Dimensional Dynamic Pricing
neurips,2019,2,8962,Matt,Taddy,chicagobooth,Chicago Booth,taddy@chicagobooth.edu,Low-Rank Bandit Methods for High-Dimensional Dynamic Pricing
neurips,2019,0,5956,J. Zico,Kolter,cmu,Carnegie Mellon University / Bosch Center for AI,gmanek@cs.cmu.edu,Learning Stable Deep Dynamics Models
neurips,2019,1,5956,Gaurav,Manek,cmu,Carnegie Mellon University,zkolter@cs.cmu.edu,Learning Stable Deep Dynamics Models
neurips,2019,0,8646,Gagandeep,Singh,ethz,ETH Zurich,1gsingh@inf.ethz.ch,Beyond the Single Neuron Convex Barrier for Neural Network Certification
neurips,2019,1,8646,Rupanshu,Ganvir,ethz,ETH Zurich,pueschel@inf.ethz.ch,Beyond the Single Neuron Convex Barrier for Neural Network Certification
neurips,2019,2,8646,Markus,Püschel,ethz,ETH Zurich,martin.vechev@inf.ethz.ch,Beyond the Single Neuron Convex Barrier for Neural Network Certification
neurips,2019,3,8646,Martin,Vechev,ethz,"ETH Zurich, Switzerland",2rganvir@student.ethz.ch,Beyond the Single Neuron Convex Barrier for Neural Network Certification
neurips,2019,0,9192,Yuge,Shi,ox,University of Oxford,yshi@robots.ox.ac.uk,Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models
neurips,2019,1,9192,Siddharth,N,ox,Unversity of Oxford,nsid@robots.ox.ac.uk,Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models
neurips,2019,2,9192,Brooks,Paige,turing,Alan Turing Institute,bpaige@turing.ac.uk,Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models
neurips,2019,3,9192,Philip,Torr,ox,University of Oxford,philip.torr@eng.ox.ac.uk,Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models
neurips,2019,0,5017,YiDing,Jiang,,Google Research,,Language as an Abstraction for Hierarchical Deep Reinforcement Learning
neurips,2019,1,5017,Shixiang (Shane),Gu,,Google Brain,,Language as an Abstraction for Hierarchical Deep Reinforcement Learning
neurips,2019,2,5017,Kevin,Murphy,,Google,,Language as an Abstraction for Hierarchical Deep Reinforcement Learning
neurips,2019,3,5017,Chelsea,Finn,,Google Brain,,Language as an Abstraction for Hierarchical Deep Reinforcement Learning
neurips,2019,0,3697,David,Salinas,,Naverlabs,,High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes
neurips,2019,1,3697,Michael,Bohlke-Schneider,,Amazon,,High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes
neurips,2019,2,3697,Laurent,Callot,,Amazon,,High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes
neurips,2019,3,3697,Roberto,Medico,,Ghent University,,High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes
neurips,2019,4,3697,Jan,Gasthaus,,Amazon.com,,High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes
neurips,2019,0,4758,Farzane,Aminmansour,,University of Alberta,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,1,4758,Andrew,Patterson,,University of Alberta,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,2,4758,Lei,Le,,Amazon,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,3,4758,Yisu,Peng,,Northeastern University,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,4,4758,Daniel,Mitchell,,University of Alberta,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,5,4758,Franco,Pestilli,,Indiana University,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,6,4758,Cesar,Caiafa,,CONICET/RIKEN AIP,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,7,4758,Russell,Greiner,,University of Alberta,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,8,4758,Martha,White,,University of Alberta,,Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
neurips,2019,0,2652,Huaian,Diao,,Northeast Normal University,,Optimal Sketching for Kronecker Product Regression and Low Rank Approximation
neurips,2019,1,2652,Rajesh,Jayaram,,Carnegie Mellon University,,Optimal Sketching for Kronecker Product Regression and Low Rank Approximation
neurips,2019,2,2652,Zhao,Song,,UT-Austin,,Optimal Sketching for Kronecker Product Regression and Low Rank Approximation
neurips,2019,3,2652,Wen,Sun,,Microsoft Research NYC,,Optimal Sketching for Kronecker Product Regression and Low Rank Approximation
neurips,2019,4,2652,David,Woodruff,,Carnegie Mellon University,,Optimal Sketching for Kronecker Product Regression and Low Rank Approximation
neurips,2019,0,5661,Ziyin,Liu,u-tokyo,University of Tokyo,zliu@cat.phys.s.u-tokyo.ac.jp,Deep Gamblers: Learning to Abstain with Portfolio Theory
neurips,2019,1,5661,Zhikang,Wang,u-tokyo,University of Tokyo,wang@cat.phys.s.u-tokyo.ac.jp,Deep Gamblers: Learning to Abstain with Portfolio Theory
neurips,2019,2,5661,Paul Pu,Liang,u-tokyo,Carnegie Mellon University,ueda@phys.s.u-tokyo.ac.jp,Deep Gamblers: Learning to Abstain with Portfolio Theory
neurips,2019,3,5661,Russ,Salakhutdinov,cmu,Carnegie Mellon University,pliang@cs.cmu.edu,Deep Gamblers: Learning to Abstain with Portfolio Theory
neurips,2019,4,5661,Louis-Philippe,Morency,cmu,Carnegie Mellon University,rsalakhu@cs.cmu.edu,Deep Gamblers: Learning to Abstain with Portfolio Theory
neurips,2019,5,5661,Masahito,Ueda,cmu,University of Tokyo,morency@cs.cmu.edu,Deep Gamblers: Learning to Abstain with Portfolio Theory
neurips,2019,0,8836,Ali,Sadeghian,ufl,University of Florida,asadeghian@ufl.edu,DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs
neurips,2019,1,8836,Mohammadreza,Armandpour,ufl,Texas A&M University,daisyw@ufl.edu,DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs
neurips,2019,2,8836,Patrick,Ding,tamu,Texas A&M University,armand@stat.tamu.edu,DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs
neurips,2019,3,8836,Daisy Zhe,Wang,tamu,Univeresity of Florida,patrickding@stat.tamu.edu,DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs
neurips,2019,0,696,Paul Hongsuck,Seo,postech,POSTECH,hsseo@postech.ac.kr,Combinatorial Inference against Label Noise
neurips,2019,1,696,Geeho,Kim,snu,Seoul National University,snow1234@snu.ac.kr,Combinatorial Inference against Label Noise
neurips,2019,2,696,Bohyung,Han,snu,Seoul National University,bhhan@snu.ac.kr,Combinatorial Inference against Label Noise
neurips,2019,0,3968,Carlo,Ciliberto,imperial,Imperial College London,c.ciliberto@imperial.ac.uk,Localized Structured Prediction
neurips,2019,1,3968,Francis,Bach,inria,INRIA - Ecole Normale Superieure,francis.bach@inria.fr,Localized Structured Prediction
neurips,2019,2,3968,Alessandro,Rudi,inria,"INRIA, Ecole Normale Superieure",alessandro.rudi@inria.fr,Localized Structured Prediction
neurips,2019,0,408,Han,Liu,tsinghua,Tsinghua University,liuhan15@mails.tsinghua.edu.cn,Fast Low-rank Metric Learning for Large-scale and High-dimensional Data
neurips,2019,1,408,Zhizhong,Han,umd,"University of Maryland, College Park",h312h@umd.edu,Fast Low-rank Metric Learning for Large-scale and High-dimensional Data
neurips,2019,2,408,Yu-Shen,Liu,tsinghua,Tsinghua University,liuyushen@tsinghua.edu.cn,Fast Low-rank Metric Learning for Large-scale and High-dimensional Data
neurips,2019,3,408,Ming,Gu,tsinghua,Tsinghua University,guming@tsinghua.edu.cn,Fast Low-rank Metric Learning for Large-scale and High-dimensional Data
neurips,2019,0,4627,Jaehoon,Lee,google,Google Brain,jaehlee@google.com,Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
neurips,2019,1,4627,Lechao,Xiao,google,Google Brain,xlc@google.com,Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
neurips,2019,2,4627,Samuel,Schoenholz,google,Google Brain,schsam@google.com,Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
neurips,2019,3,4627,Yasaman,Bahri,google,Google Brain,yasamanb@google.com,Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
neurips,2019,4,4627,Roman,Novak,google,Google Brain,romann@google.com,Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
neurips,2019,5,4627,Jascha,Sohl-Dickstein,google,Google Brain,jaschasd@google.com,Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
neurips,2019,6,4627,Jeffrey,Pennington,google,Google Brain,jpennin@google.com,Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
neurips,2019,0,4761,Hanjun,Dai,,Georgia Tech,,Retrosynthesis Prediction with Conditional Graph Logic Network
neurips,2019,1,4761,Chengtao,Li,,MIT,,Retrosynthesis Prediction with Conditional Graph Logic Network
neurips,2019,2,4761,Connor,Coley,,MIT,,Retrosynthesis Prediction with Conditional Graph Logic Network
neurips,2019,3,4761,Bo,Dai,,Google Brain,,Retrosynthesis Prediction with Conditional Graph Logic Network
neurips,2019,4,4761,Le,Song,,Georgia Institute of Technology,,Retrosynthesis Prediction with Conditional Graph Logic Network
neurips,2019,0,3579,Tianyuan,Jin,ustc,University of Science and Technology of China,jty123@mail.ustc.edu.cn,Efficient Pure Exploration in Adaptive Round model
neurips,2019,1,3579,Jieming,SHI,ustc,NATIONAL UNIVERSITY OF SINGAPORE,cheneh@ustc.edu.cn,Efficient Pure Exploration in Adaptive Round model
neurips,2019,2,3579,Xiaokui,Xiao,nus,National University of Singapore,shijm@nus.edu.sg,Efficient Pure Exploration in Adaptive Round model
neurips,2019,3,3579,Enhong,Chen,nus,University of Science and Technology of China,xkxiao@nus.edu.sg,Efficient Pure Exploration in Adaptive Round model
neurips,2019,0,3880,Alban,Laflaquière,softbankrobotics,SoftBank Robotics Europe,alaflaquiere@softbankrobotics.com,Unsupervised Emergence of Egocentric Spatial Structure from Sensorimotor Prediction
neurips,2019,1,3880,Michael,Garcia Ortiz,softbankrobotics,SoftBank Robotics Europe,mgarciaortiz@softbankrobotics.com,Unsupervised Emergence of Egocentric Spatial Structure from Sensorimotor Prediction
neurips,2019,0,7741,Chongli,Qin,,DeepMind,,Adversarial Robustness through Local Linearization
neurips,2019,1,7741,James,Martens,,DeepMind,,Adversarial Robustness through Local Linearization
neurips,2019,2,7741,Sven,Gowal,,DeepMind,,Adversarial Robustness through Local Linearization
neurips,2019,3,7741,Dilip,Krishnan,,Google,,Adversarial Robustness through Local Linearization
neurips,2019,4,7741,Krishnamurthy,Dvijotham,,DeepMind,,Adversarial Robustness through Local Linearization
neurips,2019,5,7741,Alhussein,Fawzi,,DeepMind,,Adversarial Robustness through Local Linearization
neurips,2019,6,7741,Soham,De,,DeepMind,,Adversarial Robustness through Local Linearization
neurips,2019,7,7741,Robert,Stanforth,,DeepMind,,Adversarial Robustness through Local Linearization
neurips,2019,8,7741,Pushmeet,Kohli,,DeepMind,,Adversarial Robustness through Local Linearization
neurips,2019,0,1183,Shangtong,Zhang,ox,University of Oxford,shangtong.zhang@cs.ox.ac.uk,Generalized Off-Policy Actor-Critic
neurips,2019,1,1183,Wendelin,Boehmer,ox,University of Oxford,wendelin.boehmer@cs.ox.ac.uk,Generalized Off-Policy Actor-Critic
neurips,2019,2,1183,Shimon,Whiteson,ox,University of Oxford,shimon.whiteson@cs.ox.ac.uk,Generalized Off-Policy Actor-Critic
neurips,2019,0,4474,Saeed,Sharifi-Malvajerdi,upenn,University of Pennsylvania,mkearns@cis.upenn.edu,"Average Individual Fairness: Algorithms, Generalization and Experiments"
neurips,2019,1,4474,Michael,Kearns,upenn,University of Pennsylvania,aaroth@cis.upenn.edu,"Average Individual Fairness: Algorithms, Generalization and Experiments"
neurips,2019,2,4474,Aaron,Roth,upenn,University of Pennsylvania,saeedsh@wharton.upenn.edu,"Average Individual Fairness: Algorithms, Generalization and Experiments"
neurips,2019,0,6662,meyer,scetbon,,CREST-ENSAE,,Comparing distributions: $\ell_1$ geometry improves kernel two-sample testing
neurips,2019,1,6662,Gael,Varoquaux,,"Parietal Team, INRIA",,Comparing distributions: $\ell_1$ geometry improves kernel two-sample testing
neurips,2019,0,3526,Tobias Sommer,Thune,ku,University of Copenhagen,tobias.thune@di.ku.dk,Nonstochastic Multiarmed Bandits with Unrestricted Delays
neurips,2019,1,3526,Nicolò,Cesa-Bianchi,unimi,Università degli Studi di Milano,nicolo.cesa-bianchi@unimi.it,Nonstochastic Multiarmed Bandits with Unrestricted Delays
neurips,2019,2,3526,Yevgeny,Seldin,ku,University of Copenhagen,seldin@di.ku.dk,Nonstochastic Multiarmed Bandits with Unrestricted Delays
neurips,2019,0,3822,Cornelius,Schröder,uni-tuebingen,University of Tübingen,cornelius.schroeder@uni-tuebingen.de,Approximate Bayesian Inference for a Mechanistic Model of Vesicle Release at a Ribbon Synapse
neurips,2019,1,3822,Ben,James,gmail,University of Sussex,bmjame02@gmail.com,Approximate Bayesian Inference for a Mechanistic Model of Vesicle Release at a Ribbon Synapse
neurips,2019,2,3822,Leon,Lagnado,sussex,University of Sussex,l.lagnado@sussex.ac.uk,Approximate Bayesian Inference for a Mechanistic Model of Vesicle Release at a Ribbon Synapse
neurips,2019,3,3822,Philipp,Berens,uni-tuebingen,University of Tübingen,philipp.berens@uni-tuebingen.de,Approximate Bayesian Inference for a Mechanistic Model of Vesicle Release at a Ribbon Synapse
neurips,2019,0,5136,Colin,Wei,,Stanford University,,Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation
neurips,2019,1,5136,Tengyu,Ma,,Stanford University,,Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation
neurips,2019,0,3507,Zaiqiao,Meng,gmail,University of Glasgow,zaiqiao.meng@gmail.com,Semi-supervisedly Co-embedding Attributed Networks
neurips,2019,1,3507,Shangsong,Liang,gmail,Sun Yat-sen University,liangshangsong@gmail.com,Semi-supervisedly Co-embedding Attributed Networks
neurips,2019,2,3507,Jinyuan,Fang,gmail,Sun Yat-sen University,fangjy6@gmail.com,Semi-supervisedly Co-embedding Attributed Networks
neurips,2019,3,3507,Teng,Xiao,gmail,Sun Yat-sen University,tengxiao01@gmail.com,Semi-supervisedly Co-embedding Attributed Networks
neurips,2019,0,2672,Xingyu,Lin,cmu,Carnegie Mellon University,xlin3@andrew.cmu.edu,Adaptive Auxiliary Task Weighting for Reinforcement Learning
neurips,2019,1,2672,Harjatin,Baweja,cmu,CMU,harjatis@andrew.cmu.edu,Adaptive Auxiliary Task Weighting for Reinforcement Learning
neurips,2019,2,2672,George,Kantor,cmu,CMU,kantor@andrew.cmu.edu,Adaptive Auxiliary Task Weighting for Reinforcement Learning
neurips,2019,3,2672,David,Held,cmu,CMU,dheld@andrew.cmu.edu,Adaptive Auxiliary Task Weighting for Reinforcement Learning
neurips,2019,0,6826,Emile,Mathieu,ox,University of Oxford,emile.mathieu@stats.ox.ac.uk,Continuous Hierarchical Representations with Poincaré Variational Auto-Encoders
neurips,2019,1,6826,Charline,Le Lan,ox,University of Oxford,charline.lelan@stats.ox.ac.uk,Continuous Hierarchical Representations with Poincaré Variational Auto-Encoders
neurips,2019,2,6826,Chris,Maddison,ox,"Institute for Advanced Study, Princeton",cmaddis@stats.ox.ac.uk,Continuous Hierarchical Representations with Poincaré Variational Auto-Encoders
neurips,2019,3,6826,Ryota,Tomioka,microsoft,Microsoft Research Cambridge,ryoto@microsoft.com,Continuous Hierarchical Representations with Poincaré Variational Auto-Encoders
neurips,2019,4,6826,Yee Whye,Teh,ox,"University of Oxford, DeepMind",y.w.teh@stats.ox.ac.uk,Continuous Hierarchical Representations with Poincaré Variational Auto-Encoders
neurips,2019,0,1422,Zhihao,Xia,wustl,Washington University in St. Louis,zhihao.xia@wustl.edu,Training Image Estimators without Image Ground Truth
neurips,2019,1,1422,Ayan,Chakrabarti,wustl,Washington University in St. Louis,ayan@wustl.edu,Training Image Estimators without Image Ground Truth
neurips,2019,0,3626,Zeyuan,Allen-Zhu,mit,Microsoft Research,zeyuan@csail.mit.edu,On the Convergence Rate of Training Recurrent Neural Networks
neurips,2019,1,3626,Yuanzhi,Li,cmu,Princeton,yuanzhil@andrew.cmu.edu,On the Convergence Rate of Training Recurrent Neural Networks
neurips,2019,2,3626,Zhao,Song,utexas,University of Washington,zhaos@utexas.edu,On the Convergence Rate of Training Recurrent Neural Networks
neurips,2019,0,4074,Marco,Loog,,Delft University of Technology & University of Copenhagen,,Minimizers of the Empirical Risk and Risk Monotonicity
neurips,2019,1,4074,Tom,Viering,,"Delft University of Technology, Netherlands",,Minimizers of the Empirical Risk and Risk Monotonicity
neurips,2019,2,4074,Alexander,Mey,,TU Delft,,Minimizers of the Empirical Risk and Risk Monotonicity
neurips,2019,0,2800,Jicong,Fan,cornell,Cornell University,jf577@cornell.edu,Factor Group-Sparse Regularization for Efficient Low-Rank Matrix Recovery
neurips,2019,1,2800,Lijun,Ding,cornell,Cornell University,ld446@cornell.edu,Factor Group-Sparse Regularization for Efficient Low-Rank Matrix Recovery
neurips,2019,2,2800,Yudong,Chen,cornell,Cornell University,yudong.chen@cornell.edu,Factor Group-Sparse Regularization for Efficient Low-Rank Matrix Recovery
neurips,2019,3,2800,Madeleine,Udell,cornell,Cornell University,udell@cornell.edu,Factor Group-Sparse Regularization for Efficient Low-Rank Matrix Recovery
neurips,2019,0,4471,Zhixin,Zhou,gmail,City University of Hong Kong,zhixin0825@gmail.com,Möbius Transformation for Fast Inner Product Search on Graph
neurips,2019,1,4471,Shulong,Tan,gmail,Baidu Research,zhaozhuoxu@gmail.com,Möbius Transformation for Fast Inner Product Search on Graph
neurips,2019,2,4471,Zhaozhuo,Xu,baidu,Baidu Research,shulongtan@baidu.com,Möbius Transformation for Fast Inner Product Search on Graph
neurips,2019,3,4471,Ping,Li,baidu,Baidu Research USA,liping11@baidu.com,Möbius Transformation for Fast Inner Product Search on Graph
neurips,2019,0,1044,Songbai,Yan,ucsd,"University of California, San Diego",yansongbai@eng.ucsd.edu,The Label Complexity of Active Learning from Observational Data
neurips,2019,1,1044,Kamalika,Chaudhuri,ucsd,UCSD,kamalika@cs.ucsd.edu,The Label Complexity of Active Learning from Observational Data
neurips,2019,2,1044,Tara,Javidi,ucsd,University of California San Diego,tjavidi@eng.ucsd.edu,The Label Complexity of Active Learning from Observational Data
neurips,2019,0,4472,Qi,Liu,fb,University of Oxford,qiliu@fb.com,Hyperbolic Graph Neural Networks
neurips,2019,1,4472,Maximilian,Nickel,fb,Facebook AI Research,maxn@fb.com,Hyperbolic Graph Neural Networks
neurips,2019,2,4472,Douwe,Kiela,fb,Facebook AI Research,dkiela@fb.com,Hyperbolic Graph Neural Networks
neurips,2019,0,7755,Jiechuan,Jiang,pku,Peking University,jiechuan.jiang@pku.edu.cn,Learning Fairness in Multi-Agent Systems
neurips,2019,1,7755,Zongqing,Lu,pku,Peking University,zongqing.lu@pku.edu.cn,Learning Fairness in Multi-Agent Systems
neurips,2019,0,7666,Pranjal,Awasthi,rutgers,Rutgers University/Google,pranjal.awasthi@rutgers.edu,On Robustness to Adversarial Examples and Polynomial Optimization
neurips,2019,1,7666,Abhratanu,Dutta,northwestern,Northwestern University,abhratanudutta2020@u.northwestern.edu,On Robustness to Adversarial Examples and Polynomial Optimization
neurips,2019,2,7666,Aravindan,Vijayaraghavan,northwestern,Northwestern University,aravindv@northwestern.edu,On Robustness to Adversarial Examples and Polynomial Optimization
neurips,2019,0,3078,Hui,Guan,ncsu,North Carolina State University,hguan2@ncsu.edu,In-Place Zero-Space Memory Protection for CNN
neurips,2019,1,3078,Lin,Ning,ncsu,NCSU,lning@ncsu.edu,In-Place Zero-Space Memory Protection for CNN
neurips,2019,2,3078,Zhen,Lin,ncsu,NCSU,zlin4@ncsu.edu,In-Place Zero-Space Memory Protection for CNN
neurips,2019,3,3078,Xipeng,Shen,ncsu,North Carolina State University,xshen5@ncsu.edu,In-Place Zero-Space Memory Protection for CNN
neurips,2019,4,3078,Huiyang,Zhou,ncsu,NCSU,hzhou@ncsu.edu,In-Place Zero-Space Memory Protection for CNN
neurips,2019,5,3078,Seung-Hwan,Lim,ornl,Oak Ridge National Laboratory,lims1@ornl.gov,In-Place Zero-Space Memory Protection for CNN
neurips,2019,0,691,Max,Simchowitz,berkeley,Berkeley,msimchow@berkeley.edu,Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs
neurips,2019,1,691,Kevin,Jamieson,washington,U Washington,jamieson@cs.washington.edu,Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs
neurips,2019,0,4978,Vivek,Veeriah,,University of Michigan,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,1,4978,Matteo,Hessel,,Google DeepMind,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,2,4978,Zhongwen,Xu,,DeepMind,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,3,4978,Janarthanan,Rajendran,,University of Michigan,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,4,4978,Richard,Lewis,,University of Michigan,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,5,4978,Junhyuk,Oh,,DeepMind,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,6,4978,Hado,van Hasselt,,DeepMind,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,7,4978,David,Silver,,DeepMind,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,8,4978,Satinder,Singh,,University of Michigan,,Discovery of Useful Questions as Auxiliary Tasks
neurips,2019,0,5407,Gautam,Singh,rutgers,Rutgers University,singh.gautam@rutgers.edu,Sequential Neural Processes
neurips,2019,1,5407,Jaesik,Yoon,sap,SAP,jaesik.yoon01@sap.com,Sequential Neural Processes
neurips,2019,2,5407,Youngsung,Son,re,Electronics and Telecommunications Research Institute,ysson@etri.re.kr,Sequential Neural Processes
neurips,2019,3,5407,Sungjin,Ahn,rutgers,Rutgers University,sungjin.ahn@rutgers.edu,Sequential Neural Processes
neurips,2019,0,1952,Hattie,Zhou,uber,Uber,hattie@uber.com,"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask"
neurips,2019,1,1952,Janice,Lan,uber,Uber AI,janlan@uber.com,"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask"
neurips,2019,2,1952,Rosanne,Liu,uber,Uber AI Labs,rosanne@uber.com,"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask"
neurips,2019,3,1952,Jason,Yosinski,uber,Uber AI; Recursion,yosinski@uber.com,"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask"
neurips,2019,0,4367,James,Requeima,cam,University of Cambridge / Invenia Labs,jrr41@cam.ac.uk,Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes
neurips,2019,1,4367,Jonathan,Gordon,cam,University of Cambridge,jg801@cam.ac.uk,Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes
neurips,2019,2,4367,John,Bronskill,cam,University of Cambridge,jfb54@cam.ac.uk,Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes
neurips,2019,3,4367,Sebastian,Nowozin,google,Google Research Berlin,nowozin@google.com,Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes
neurips,2019,4,4367,Richard,Turner,cam,Cambridge,ret26@cam.ac.uk,Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes
neurips,2019,0,7203,Wesley,Maddox,,New York University,,A Simple Baseline for Bayesian Uncertainty in Deep Learning
neurips,2019,1,7203,Pavel,Izmailov,,New York University,,A Simple Baseline for Bayesian Uncertainty in Deep Learning
neurips,2019,2,7203,Timur,Garipov,,MIT,,A Simple Baseline for Bayesian Uncertainty in Deep Learning
neurips,2019,3,7203,Dmitry,Vetrov,,"Higher School of Economics, Samsung AI Center, Moscow",,A Simple Baseline for Bayesian Uncertainty in Deep Learning
neurips,2019,4,7203,Andrew Gordon,Wilson,,New York University,,A Simple Baseline for Bayesian Uncertainty in Deep Learning
neurips,2019,0,303,Changqing,Zhang,,Tianjin University,,CPM-Nets: Cross Partial Multi-View Networks
neurips,2019,1,303,Zongbo,Han,,Tianjin University,,CPM-Nets: Cross Partial Multi-View Networks
neurips,2019,2,303,yajie,cui,,tianjin university,,CPM-Nets: Cross Partial Multi-View Networks
neurips,2019,3,303,Huazhu,Fu,,Inception Institute of Artificial Intelligence,,CPM-Nets: Cross Partial Multi-View Networks
neurips,2019,4,303,Joey Tianyi,Zhou,,"IHPC, A*STAR",,CPM-Nets: Cross Partial Multi-View Networks
neurips,2019,5,303,Qinghua,Hu,,Tianjin University,,CPM-Nets: Cross Partial Multi-View Networks
neurips,2019,0,8254,Alix,LHERITIER,amadeus,Amadeus,alix.lheritier@amadeus.com,Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees
neurips,2019,1,8254,Frederic,Cazals,inria,Inria,frederic.cazals@inria.fr,Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees
neurips,2019,0,4983,Stéphane,d'Ascoli,ens,ENS / FAIR,stephane.dascoli@ens.fr,Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias
neurips,2019,1,4983,Levent,Sagun,fb,EPFL,leventsagun@fb.com,Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias
neurips,2019,2,4983,Giulio,Biroli,nyu,ENS,bruna@cims.nyu.edu,Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias
neurips,2019,3,4983,Joan,Bruna,ens,NYU,giulio.biroli@lps.ens.fr,Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias
neurips,2019,0,5317,Emmanouil-Vasileios,Vlatakis-Gkaragkounis,columbia,Columbia University,lamflokas@cs.columbia.edu,Efficiently avoiding saddle points with zero order methods: No gradients required
neurips,2019,1,5317,Lampros,Flokas,columbia,Columbia University,emvlatakis@cs.columbia.edu,Efficiently avoiding saddle points with zero order methods: No gradients required
neurips,2019,2,5317,Georgios,Piliouras,sutd,Singapore University of Technology and Design,georgios@sutd.edu.sg,Efficiently avoiding saddle points with zero order methods: No gradients required
neurips,2019,0,5218,Qi,Zhao,osu,The Ohio State University,zhao.2017@osu.edu,Learning metrics for persistence-based summaries and applications for graph classification
neurips,2019,1,5218,Yusu,Wang,ohio-state,Ohio State University,yusu@cse.ohio-state.edu,Learning metrics for persistence-based summaries and applications for graph classification
neurips,2019,0,2179,Yikang,LI,cuhk,The Chinese University of Hong Kong; Sensetime,ykli@ee.cuhk.edu.hk,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph
neurips,2019,1,2179,Tao,Ma,cuhk,Northwestern Polytechnical University,xgwang@ee.cuhk.edu.hk,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph
neurips,2019,2,2179,Yeqi,Bai,nwpu,Nanyang Technological University,taoma@mail.nwpu.edu.cn,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph
neurips,2019,3,2179,Nan,Duan,gmail,Microsoft Research Asia,baiyeqi@gmail.com,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph
neurips,2019,4,2179,Sining,Wei,microsoft,Microsoft Research,sinwei@microsoft.com,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph
neurips,2019,5,2179,Xiaogang,Wang,microsoft,The Chinese University of Hong Kong,nanduan@microsoft.com,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph
neurips,2019,0,4384,Emre,Yolcu,cmu,Carnegie Mellon University,eyolcu@cs.cmu.edu,Learning Local Search Heuristics for Boolean Satisfiability
neurips,2019,1,4384,Barnabas,Poczos,cmu,Carnegie Mellon University,bapoczos@cs.cmu.edu,Learning Local Search Heuristics for Boolean Satisfiability
neurips,2019,0,3385,Xinyun,Chen,,UC Berkeley,,Learning to Perform Local Rewriting for Combinatorial Optimization
neurips,2019,1,3385,Yuandong,Tian,,Facebook AI Research,,Learning to Perform Local Rewriting for Combinatorial Optimization
neurips,2019,0,4261,Felix,Leibfried,prowler,PROWLER.io,felix@prowler.io,A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment
neurips,2019,1,4261,Sergio,Pascual-Díaz,prowler,-,sergio.diaz@prowler.io,A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment
neurips,2019,2,4261,Jordi,Grau-Moya,prowler,PROWLER.io,jordi@prowler.io,A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment
neurips,2019,0,2058,Qianli,Ma,scut,South China University of Technology,qianlima@scut.edu.cn,Learning Representations for Time Series Clustering
neurips,2019,1,2058,Jiawei,Zheng,foxmail,South China University of Technology,awslee@foxmail.com,Learning Representations for Time Series Clustering
neurips,2019,2,2058,Sen,Li,foxmail,South China University of Technology,csjwzheng@foxmail.com,Learning Representations for Time Series Clustering
neurips,2019,3,2058,Gary,Cottrell,ucsd,UCSD,gary@ucsd.edu,Learning Representations for Time Series Clustering
neurips,2019,0,5502,Lingxiao,Wang,,Northwestern University,,Statistical-Computational Tradeoff in Single Index Models
neurips,2019,1,5502,Zhuoran,Yang,,Princeton University,,Statistical-Computational Tradeoff in Single Index Models
neurips,2019,2,5502,Zhaoran,Wang,,Northwestern University,,Statistical-Computational Tradeoff in Single Index Models
neurips,2019,0,4182,Meng,Qu,,Mila,,Probabilistic Logic Neural Networks for Reasoning
neurips,2019,1,4182,Jian,Tang,,Mila,,Probabilistic Logic Neural Networks for Reasoning
neurips,2019,0,159,Xueting,Li,,"University of California, Merced",,Joint-task Self-supervised Learning for Temporal Correspondence
neurips,2019,1,159,Sifei,Liu,,NVIDIA,,Joint-task Self-supervised Learning for Temporal Correspondence
neurips,2019,2,159,Shalini,De Mello,,NVIDIA,,Joint-task Self-supervised Learning for Temporal Correspondence
neurips,2019,3,159,Xiaolong,Wang,,CMU,,Joint-task Self-supervised Learning for Temporal Correspondence
neurips,2019,4,159,Jan,Kautz,,NVIDIA,,Joint-task Self-supervised Learning for Temporal Correspondence
neurips,2019,5,159,Ming-Hsuan,Yang,,Google / UC Merced,,Joint-task Self-supervised Learning for Temporal Correspondence
neurips,2019,0,3656,Jacky,Zhang,illinois,UIUC,yiboz@illinois.edu,Learning Sparse Distributions using Iterative Hard Thresholding
neurips,2019,1,3656,Rajiv,Khanna,berkeley,University of California at Berkeley,rajivak@berkeley.edu,Learning Sparse Distributions using Iterative Hard Thresholding
neurips,2019,2,3656,Anastasios,Kyrillidis,berkeley,Rice University,rajivak@berkeley.edu,Learning Sparse Distributions using Iterative Hard Thresholding
neurips,2019,3,3656,Oluwasanmi,Koyejo,illinois,UIUC,sanmi@illinois.edu,Learning Sparse Distributions using Iterative Hard Thresholding
neurips,2019,0,5905,Aditya,Bhaskara,utah,University of Utah,bhaskara@cs.utah.edu,On Distributed Averaging for Stochastic k-PCA
neurips,2019,1,5905,Pruthuvi Maheshakya,Wijewardena,gmail,University of Utah,pmaheshakya4@gmail.com,On Distributed Averaging for Stochastic k-PCA
neurips,2019,0,2323,Alhussein,Fawzi,google,DeepMind,afawzi@google.com,Learning dynamic polynomial proofs
neurips,2019,1,2323,Mateusz,Malinowski,google,DeepMind,mateuszm@google.com,Learning dynamic polynomial proofs
neurips,2019,2,2323,Hamza,Fawzi,cam,University of Cambridge,hf323@cam.ac.uk,Learning dynamic polynomial proofs
neurips,2019,3,2323,Omar,Fawzi,ens-lyon,ENS Lyon,omar.fawzi@ens-lyon.fr,Learning dynamic polynomial proofs
neurips,2019,0,1823,Sai Qian,Zhang,,Harvard University,,Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control
neurips,2019,1,1823,Qi,Zhang,,Amazon,,Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control
neurips,2019,2,1823,Jieyu,Lin,,University of Toronto,,Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control
neurips,2019,0,7344,Lei,Wu,princeton,Princeton University,leiwu@princeton.edu,Global Convergence of Gradient Descent  for Deep Linear Residual Networks
neurips,2019,1,7344,Qingcan,Wang,princeton,"Program in Applied and Computational Mathematics, Princeton University",qingcanw@princeton.edu,Global Convergence of Gradient Descent  for Deep Linear Residual Networks
neurips,2019,2,7344,Chao,Ma,princeton,Princeton University,chaom@princeton.edu,Global Convergence of Gradient Descent  for Deep Linear Residual Networks
neurips,2019,0,5285,Hamid,Shayestehmanesh,uvic,University of Victoria,hamidshayestehmanesh@uvic.ca,Dying Experts: Efficient Algorithms with Optimal Regret Bounds
neurips,2019,1,5285,Sajjad,Azami,uvic,University of Victoria,sajjadazami@uvic.ca,Dying Experts: Efficient Algorithms with Optimal Regret Bounds
neurips,2019,2,5285,Nishant,Mehta,uvic,University of Victoria,nmehta@uvic.ca,Dying Experts: Efficient Algorithms with Optimal Regret Bounds
neurips,2019,0,5129,Koosha,Khalvati,washington,University of Washington,koosha@cs.washington.edu,A Bayesian Theory of Conformity in Collective Decision Making
neurips,2019,1,5129,Saghar,Mirbagheri,nyu,New York University,sm7369@nyu.edu,A Bayesian Theory of Conformity in Collective Decision Making
neurips,2019,2,5129,Seongmin,Park,cnrs,"Cognitive Neuroscience Center, CNRS",park@isc.cnrs.fr,A Bayesian Theory of Conformity in Collective Decision Making
neurips,2019,3,5129,Jean-Claude,Dreher,cnrs,cnrs,dreher@isc.cnrs.fr,A Bayesian Theory of Conformity in Collective Decision Making
neurips,2019,4,5129,Rajesh,Rao,washington,University of Washington,rao@cs.washington.edu,A Bayesian Theory of Conformity in Collective Decision Making
neurips,2019,0,395,Aaron,Schein,,UMass Amherst,,Poisson-Randomized Gamma Dynamical Systems
neurips,2019,1,395,Scott,Linderman,,Columbia University,,Poisson-Randomized Gamma Dynamical Systems
neurips,2019,2,395,Mingyuan,Zhou,,University of Texas at Austin,,Poisson-Randomized Gamma Dynamical Systems
neurips,2019,3,395,David,Blei,,Columbia University,,Poisson-Randomized Gamma Dynamical Systems
neurips,2019,4,395,Hanna,Wallach,,MSR NYC,,Poisson-Randomized Gamma Dynamical Systems
neurips,2019,0,8286,Dustin,Tran,,Google Brain,,Bayesian Layers: A Module for Neural Network Uncertainty
neurips,2019,1,8286,Mike,Dusenberry,,Google Brain,,Bayesian Layers: A Module for Neural Network Uncertainty
neurips,2019,2,8286,Mark,van der Wilk,,PROWLER.io,,Bayesian Layers: A Module for Neural Network Uncertainty
neurips,2019,3,8286,Danijar,Hafner,,Google,,Bayesian Layers: A Module for Neural Network Uncertainty
neurips,2019,0,4178,Dmitrii,Emelianenko,yandex-team,Yandex; National Research University Higher School of Economics,dimdi-y@yandex-team.ru,Sequence Modeling with Unconstrained Generation Order
neurips,2019,1,4178,Elena,Voita,yandex-team,Yandex; University of Amsterdam,lena-voita@yandex-team.ru,Sequence Modeling with Unconstrained Generation Order
neurips,2019,2,4178,Pavel,Serdyukov,yandex-team,Yandex,pavser@yandex-team.ru,Sequence Modeling with Unconstrained Generation Order
neurips,2019,0,6371,Rahaf,Aljundi,gmail,"KU Leuven, Belgium",rahaf.aljundi@gmail.com,Online Continual Learning with Maximal Interfered Retrieval
neurips,2019,1,6371,Eugene,Belilovsky,mcgill,"Mila, University of Montreal",lucas.page-caccia@mail.mcgill.ca,Online Continual Learning with Maximal Interfered Retrieval
neurips,2019,2,6371,Tinne,Tuytelaars,umontreal,KU Leuven,eugene.belilovsky@umontreal.ca,Online Continual Learning with Maximal Interfered Retrieval
neurips,2019,3,6371,Laurent,Charlin,gmail,MILA / U.Montreal,massimo.p.caccia@gmail.com,Online Continual Learning with Maximal Interfered Retrieval
neurips,2019,4,6371,Massimo,Caccia,gmail,MILA,mavenlin@gmail.com,Online Continual Learning with Maximal Interfered Retrieval
neurips,2019,5,6371,Min,Lin,gmail,MILA,lcharlin@gmail.com,Online Continual Learning with Maximal Interfered Retrieval
neurips,2019,6,6371,Lucas,Page-Caccia,kuleuven,McGill University,tinne.tuytelaars@esat.kuleuven.be,Online Continual Learning with Maximal Interfered Retrieval
neurips,2019,0,4632,Emily,Reif,google,Google,andycoenen@google.com,Visualizing and Measuring the Geometry of BERT
neurips,2019,1,4632,Ann,Yuan,google,Google,ereif@google.com,Visualizing and Measuring the Geometry of BERT
neurips,2019,2,4632,Martin,Wattenberg,google,Google,annyuan@google.com,Visualizing and Measuring the Geometry of BERT
neurips,2019,3,4632,Fernanda,Viegas,google,Google,beenkim@google.com,Visualizing and Measuring the Geometry of BERT
neurips,2019,4,4632,Andy,Coenen,google,Google,adampearce@google.com,Visualizing and Measuring the Geometry of BERT
neurips,2019,5,4632,Adam,Pearce,google,Google,viegas@google.com,Visualizing and Measuring the Geometry of BERT
neurips,2019,6,4632,Been,Kim,google,Google,wattenberg@google.com,Visualizing and Measuring the Geometry of BERT
neurips,2019,0,2886,Daniel,Freeman,google,Google Brain,cdfreeman@google.com,Learning to Predict Without Looking Ahead: World Models Without Forward Prediction
neurips,2019,1,2886,David,Ha,google,Google Brain,lmetz@google.com,Learning to Predict Without Looking Ahead: World Models Without Forward Prediction
neurips,2019,2,2886,Luke,Metz,google,Google Brain,hadavid@google.com,Learning to Predict Without Looking Ahead: World Models Without Forward Prediction
neurips,2019,0,1931,Andrew,Bennett,cornell,Cornell University,awb222@cornell.edu,Deep Generalized Method of Moments for Instrumental Variable Analysis
neurips,2019,1,1931,Nathan,Kallus,cornell,Cornell University,kallus@cornell.edu,Deep Generalized Method of Moments for Instrumental Variable Analysis
neurips,2019,2,1931,Tobias,Schnabel,cornell,Microsoft Research,tbs49@cornell.edu,Deep Generalized Method of Moments for Instrumental Variable Analysis
neurips,2019,0,3510,Natasa,Tagasovska,unil,University of Lausanne,natasa.tagasovska@unil.ch,Copulas as High-Dimensional Generative Models: Vine Copula Autoencoders
neurips,2019,1,3510,Damien,Ackerer,swissquote,Swissquote,damien.ackerer@swissquote.ch,Copulas as High-Dimensional Generative Models: Vine Copula Autoencoders
neurips,2019,2,3510,Thibault,Vatter,columbia,Columbia University,thibault.vatter@columbia.edu,Copulas as High-Dimensional Generative Models: Vine Copula Autoencoders
neurips,2019,0,6875,Yulin,Wang,gmail,Tsinghua University,yulin.bh@gmail.com,Implicit Semantic Data Augmentation for Deep Networks
neurips,2019,1,6875,Xuran,Pan,gmail,Tsinghua University,fykalviny@gmail.com,Implicit Semantic Data Augmentation for Deep Networks
neurips,2019,2,6875,Shiji,Song,tsinghua,"Department of Automation, Tsinghua University",pxr18@mails.tsinghua.edu.cn,Implicit Semantic Data Augmentation for Deep Networks
neurips,2019,3,6875,Hong,Zhang,tsinghua,Baidu Inc.,shijis@tsinghua.edu.cn,Implicit Semantic Data Augmentation for Deep Networks
neurips,2019,4,6875,Gao,Huang,tsinghua,Tsinghua,wuc@tsinghua.edu.cn,Implicit Semantic Data Augmentation for Deep Networks
neurips,2019,5,6875,Cheng,Wu,tsinghua,Tsinghua,gaohuang@tsinghua.edu.cn,Implicit Semantic Data Augmentation for Deep Networks
neurips,2019,0,2295,Iordanis,Kerenidis,irif,Université Paris Diderot,jkeren@irif.fr,q-means: A quantum algorithm for unsupervised machine learning
neurips,2019,1,2295,Jonas,Landman,irif,Université Paris Diderot,landman@irif.fr,q-means: A quantum algorithm for unsupervised machine learning
neurips,2019,2,2295,Alessandro,Luongo,irif,Institut de Recherche en Informatique Fondamentale,aluongo@irif.fr,q-means: A quantum algorithm for unsupervised machine learning
neurips,2019,3,2295,Anupam,Prakash,irif,Université Paris Diderot,anupam.prakash@irif.fr,q-means: A quantum algorithm for unsupervised machine learning
neurips,2019,0,7508,Jose A.,Arjona-Medina,,"LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz, Austria",,RUDDER: Return Decomposition for Delayed Rewards
neurips,2019,1,7508,Michael,Gillhofer,,LIT AI Lab / University Linz,,RUDDER: Return Decomposition for Delayed Rewards
neurips,2019,2,7508,Michael,Widrich,,LIT AI Lab / University Linz,,RUDDER: Return Decomposition for Delayed Rewards
neurips,2019,3,7508,Thomas,Unterthiner,,Google Research,,RUDDER: Return Decomposition for Delayed Rewards
neurips,2019,4,7508,Johannes,Brandstetter,,LIT AI Lab / University Linz,,RUDDER: Return Decomposition for Delayed Rewards
neurips,2019,5,7508,Sepp,Hochreiter,,LIT AI Lab / University Linz / IARAI,,RUDDER: Return Decomposition for Delayed Rewards
neurips,2019,0,4025,Piotr,Indyk,mit,MIT,indyk@mit.edu,Learning-Based Low-Rank Approximations
neurips,2019,1,4025,Ali,Vakilian,wisc,University of Wisconsin-Madison,vakilian@wisc.edu,Learning-Based Low-Rank Approximations
neurips,2019,2,4025,Yang,Yuan,tsinghua,Cornell University,yuanyang@tsinghua.edu.cn,Learning-Based Low-Rank Approximations
neurips,2019,0,3368,Motonobu,Kanagawa,eurecom,EURECOM,motonobu.kanagawa@eurecom.fr,Convergence Guarantees for Adaptive Bayesian Quadrature Methods
neurips,2019,1,3368,Philipp,Hennig,uni-tuebingen,University of Tübingen and MPI for Intelligent Systems Tübingen,philipp.hennig@uni-tuebingen.de,Convergence Guarantees for Adaptive Bayesian Quadrature Methods
neurips,2019,0,2178,JIAJIN,LI,cuhk,The Chinese University of Hong Kong,jjli@se.cuhk.edu.hk,A First-Order Algorithmic Framework for Distributionally Robust Logistic Regression
neurips,2019,1,2178,SEN,HUANG,cuhk,The Chinese University of Hong Kong,hsen@se.cuhk.edu.hk,A First-Order Algorithmic Framework for Distributionally Robust Logistic Regression
neurips,2019,2,2178,Anthony Man-Cho,So,cuhk,CUHK,manchoso@se.cuhk.edu.hk,A First-Order Algorithmic Framework for Distributionally Robust Logistic Regression
neurips,2019,0,6644,Zhuozhuo,Tu,sydney,The University of Sydney,zhtu3055@uni.sydney.edu.au,Theoretical Analysis of Adversarial Learning: A Minimax Approach
neurips,2019,1,6644,Jingwei,Zhang,ust,HKUST,jzhangey@cse.ust.hk,Theoretical Analysis of Adversarial Learning: A Minimax Approach
neurips,2019,2,6644,Dacheng,Tao,sydney,University of Sydney,dacheng.tao@sydney.edu.au,Theoretical Analysis of Adversarial Learning: A Minimax Approach
neurips,2019,0,3306,Yi,Tay,,Nanyang Technological University,,Compositional De-Attention Networks
neurips,2019,1,3306,Anh Tuan,Luu,,MIT CSAIL,,Compositional De-Attention Networks
neurips,2019,2,3306,Aston,Zhang,,Amazon AI,,Compositional De-Attention Networks
neurips,2019,3,3306,Shuohang,Wang,,Singapore Management University,,Compositional De-Attention Networks
neurips,2019,4,3306,Siu Cheung,Hui,,Nanyang Technological University,,Compositional De-Attention Networks
neurips,2019,0,8088,Jiefeng,Chen,,University of Wisconsin-Madison,,Robust Attribution Regularization
neurips,2019,1,8088,Xi,Wu,,Google,,Robust Attribution Regularization
neurips,2019,2,8088,Vaibhav,Rastogi,,Google,,Robust Attribution Regularization
neurips,2019,3,8088,Yingyu,Liang,,University of Wisconsin Madison,,Robust Attribution Regularization
neurips,2019,4,8088,Somesh,Jha,,"University of Wisconsin, Madison",,Robust Attribution Regularization
neurips,2019,0,8507,Yizhe,Zhu,rutgers,Rutgers University,yizhe.zhu@rutgers.edu,Semantic-Guided Multi-Attention Localization for Zero-Shot Learning
neurips,2019,1,8507,Jianwen,Xie,ucla,Hikvision,jianwen@ucla.edu,Semantic-Guided Multi-Attention Localization for Zero-Shot Learning
neurips,2019,2,8507,Zhiqiang,Tang,rutgers,Rutgers,zhiqiang.tang@rutgers.edu,Semantic-Guided Multi-Attention Localization for Zero-Shot Learning
neurips,2019,3,8507,Xi,Peng,udel,University of Delaware,xipeng@udel.edu,Semantic-Guided Multi-Attention Localization for Zero-Shot Learning
neurips,2019,4,8507,Ahmed,Elgammal,rutgers,Rutgers University,elgammal@cs.rutgers.edu,Semantic-Guided Multi-Attention Localization for Zero-Shot Learning
neurips,2019,0,4898,Matthew,Staib,mit,MIT,mstaib@mit.edu,Distributionally Robust Optimization and Generalization in Kernel Methods
neurips,2019,1,4898,Stefanie,Jegelka,mit,MIT,stefje@csail.mit.edu,Distributionally Robust Optimization and Generalization in Kernel Methods
neurips,2019,0,2580,Rahul,Singh,mit,MIT,rahul.singh@mit.edu,Kernel Instrumental Variable Regression
neurips,2019,1,2580,Maneesh,Sahani,ucl,"Gatsby Unit, UCL",maneesh@gatsby.ucl.ac.uk,Kernel Instrumental Variable Regression
neurips,2019,2,2580,Arthur,Gretton,gmail,"Gatsby Unit, UCL",arthur.gretton@gmail.com,Kernel Instrumental Variable Regression
neurips,2019,0,7322,Tsendsuren,Munkhdalai,,Microsoft Research,,Metalearned Neural Memory
neurips,2019,1,7322,Alessandro,Sordoni,,Microsoft Research Montreal,,Metalearned Neural Memory
neurips,2019,2,7322,TONG,WANG,,Microsoft Research Montreal,,Metalearned Neural Memory
neurips,2019,3,7322,Adam,Trischler,,Microsoft,,Metalearned Neural Memory
neurips,2019,0,4813,Adarsh,Barik,purdue,Purdue University,abarik@purdue.edu,Learning Bayesian Networks with Low Rank Conditional Probability Tables
neurips,2019,1,4813,Jean,Honorio,purdue,Purdue University,jhonorio@purdue.edu,Learning Bayesian Networks with Low Rank Conditional Probability Tables
neurips,2019,0,5567,Jeff,Donahue,google,DeepMind,jeffdonahue@google.com,Large Scale Adversarial Representation Learning
neurips,2019,1,5567,Karen,Simonyan,google,DeepMind,simonyan@google.com,Large Scale Adversarial Representation Learning
neurips,2019,0,6771,Anna,Harutyunyan,,DeepMind,,Hindsight Credit Assignment
neurips,2019,1,6771,Will,Dabney,,DeepMind,,Hindsight Credit Assignment
neurips,2019,2,6771,Thomas,Mesnard,,DeepMind,,Hindsight Credit Assignment
neurips,2019,3,6771,Mohammad,Gheshlaghi Azar,,DeepMind,,Hindsight Credit Assignment
neurips,2019,4,6771,Bilal,Piot,,DeepMind,,Hindsight Credit Assignment
neurips,2019,5,6771,Nicolas,Heess,,Google DeepMind,,Hindsight Credit Assignment
neurips,2019,6,6771,Hado,van Hasselt,,DeepMind,,Hindsight Credit Assignment
neurips,2019,7,6771,Gregory,Wayne,,Google DeepMind,,Hindsight Credit Assignment
neurips,2019,8,6771,Satinder,Singh,,DeepMind,,Hindsight Credit Assignment
neurips,2019,9,6771,Doina,Precup,,DeepMind,,Hindsight Credit Assignment
neurips,2019,10,6771,Remi,Munos,,DeepMind,,Hindsight Credit Assignment
neurips,2019,0,36,Hyeonwoo,Yu,snu,Seoul National University,bgus2000@snu.ac.kr,Zero-shot Learning via Simultaneous Generating and Learning
neurips,2019,1,36,Beomhee,Lee,snu,Seoul National University,bhlee@snu.ac.kr,Zero-shot Learning via Simultaneous Generating and Learning
neurips,2019,0,3347,Guy,Lorberbom,,Technion,,Direct Optimization through $\arg \max$ for Discrete Variational Auto-Encoder
neurips,2019,1,3347,Andreea,Gane,,Google AI,,Direct Optimization through $\arg \max$ for Discrete Variational Auto-Encoder
neurips,2019,2,3347,Tommi,Jaakkola,,MIT,,Direct Optimization through $\arg \max$ for Discrete Variational Auto-Encoder
neurips,2019,3,3347,Tamir,Hazan,,Technion,,Direct Optimization through $\arg \max$ for Discrete Variational Auto-Encoder
neurips,2019,0,8674,Xiaoyun,Li,rutgers,Rutgers University,xiaoyun.li@rutgers.edu,Generalization Error Analysis of Quantized Compressive Learning
neurips,2019,1,8674,Ping,Li,baidu,Baidu Research USA,liping11@baidu.com,Generalization Error Analysis of Quantized Compressive Learning
neurips,2019,0,2543,David,Janz,,University of Cambridge,,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning
neurips,2019,1,2543,Jiri,Hron,,University of Cambridge,,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning
neurips,2019,2,2543,Przemysaw,Mazur,,Wayve,,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning
neurips,2019,3,2543,Katja,Hofmann,,Microsoft Research,,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning
neurips,2019,4,2543,José Miguel,Hernández-Lobato,,University of Cambridge,,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning
neurips,2019,5,2543,Sebastian,Tschiatschek,,Microsoft Research,,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning
neurips,2019,0,4908,Mario,Lezcano Casado,ox,Univeristy of Oxford,mario.lezcanocasado@maths.ox.ac.uk,Trivializations for Gradient-Based Optimization on Manifolds
neurips,2019,0,8270,Francesco,Locatello,,ETH Zürich - MPI Tübingen,,On the Fairness of Disentangled Representations
neurips,2019,1,8270,Gabriele,Abbati,,University of Oxford,,On the Fairness of Disentangled Representations
neurips,2019,2,8270,Thomas,Rainforth,,University of Oxford,,On the Fairness of Disentangled Representations
neurips,2019,3,8270,Stefan,Bauer,,MPI for Intelligent Systems,,On the Fairness of Disentangled Representations
neurips,2019,4,8270,Bernhard,Schölkopf,,MPI for Intelligent Systems,,On the Fairness of Disentangled Representations
neurips,2019,5,8270,Olivier,Bachem,,Google Brain,,On the Fairness of Disentangled Representations
neurips,2019,0,8106,Hado,van Hasselt,google,DeepMind,hado@google.com,When to use parametric models in reinforcement learning?
neurips,2019,1,8106,Matteo,Hessel,google,Google DeepMind,mtthss@google.com,When to use parametric models in reinforcement learning?
neurips,2019,2,8106,John,Aslanides,google,DeepMind,jaslanides@google.com,When to use parametric models in reinforcement learning?
neurips,2019,0,2952,Qian,Yang,,Duke University,,Ouroboros: On Accelerating Training of Transformer-Based Language Models
neurips,2019,1,2952,Zhouyuan,Huo,,University of Pittsburgh,,Ouroboros: On Accelerating Training of Transformer-Based Language Models
neurips,2019,2,2952,Wenlin,Wang,,Duke University,,Ouroboros: On Accelerating Training of Transformer-Based Language Models
neurips,2019,3,2952,Lawrence,Carin,,Duke University,,Ouroboros: On Accelerating Training of Transformer-Based Language Models
neurips,2019,0,7685,Igor,Kuralenok,yandex-team,Experts League Ltd.,solar@yandex-team.ru,MonoForest framework for tree ensemble analysis
neurips,2019,1,7685,Vasilii,Ershov,yandex-team,Yandex,noxoomo@yandex-team.ru,MonoForest framework for tree ensemble analysis
neurips,2019,2,7685,Igor,Labutin,gmail,Yandex,Labutin.IgorL@gmail.com,MonoForest framework for tree ensemble analysis
neurips,2019,0,7917,Bastian,Alt,tu-darmstadt,Technische Universität Darmstadt,bastian.alt@bcs.tu-darmstadt.de,Correlation Priors for Reinforcement Learning
neurips,2019,1,7917,Adrian,oi,tu-darmstadt,Merck KGaA,adrian.sosic@bcs.tu-darmstadt.de,Correlation Priors for Reinforcement Learning
neurips,2019,2,7917,Heinz,Koeppl,tu-darmstadt,Technische Universität Darmstadt,heinz.koeppl@bcs.tu-darmstadt.de,Correlation Priors for Reinforcement Learning
neurips,2019,0,3063,Xiao,Liu,pku,Peking University,xiaoliu23@pku.edu.cn,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently
neurips,2019,1,3063,Xiaolong,Zou,pku,Peking University,xiaolz@pku.edu.cn,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently
neurips,2019,2,3063,Zilong,Ji,pku,Beijing Normal University,tjhuang@pku.edu.cn,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently
neurips,2019,3,3063,Gengshuo,Tian,pku,Beijing Normal University,siwu@pku.edu.cn,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently
neurips,2019,4,3063,Yuanyuan,Mi,bnu,Weizmann Institute of Science,jizilong@mail.bnu.edu.cn,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently
neurips,2019,5,3063,Tiejun,Huang,163,Peking University,gengshuo_tian@163.com,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently
neurips,2019,6,3063,K. Y. Michael,Wong,cqu,"Department of Physics, Hong Kong University of Science and Technology",miyuanyuan0102@cqu.edu.cn,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently
neurips,2019,7,3063,Si,Wu,ust,Peking University,phkywong@ust.hk,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently
neurips,2019,0,6628,David,Widmann,uu,Uppsala University,david.widmann@it.uu.se,Calibration tests in multi-class classification: A unifying framework
neurips,2019,1,6628,Fredrik,Lindsten,liu,Linköping University,fredrik.lindsten@liu.se,Calibration tests in multi-class classification: A unifying framework
neurips,2019,2,6628,Dave,Zachariah,uu,Uppsala University,dave.zachariah@it.uu.se,Calibration tests in multi-class classification: A unifying framework
neurips,2019,0,2189,Han,Zhu,alibaba-inc,Alibaba Group,zhuhan.zh@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,1,2189,Daqing,Chang,alibaba-inc,Alibaba Group,daqing.cdq@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,2,2189,Ziru,Xu,alibaba-inc,Alibaba Group,ziru.xzr@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,3,2189,Pengye,Zhang,alibaba-inc,Alibaba Group,pengye.zpy@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,4,2189,Xiang,Li,alibaba-inc,Alibaba Group,yushi.lx@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,5,2189,Jie,He,alibaba-inc,Alibaba Group,jay.hj@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,6,2189,Han,Li,alibaba-inc,Alibaba Group,lihan.lh@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,7,2189,Jian,Xu,alibaba-inc,Alibaba Group,xiyu.xj@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,8,2189,Kun,Gai,alibaba-inc,Alibaba Group,jingshi.gk@alibaba-inc.com,Joint Optimization of Tree-based Index and Deep Model for Recommender Systems
neurips,2019,0,4806,Jeremiah,Liu,harvard,Google Research / Harvard,zhl112@mail.harvard.edu,Accurate Uncertainty Estimation and Decomposition in Ensemble Learning
neurips,2019,1,4806,John,Paisley,columbia,Columbia University,mk3961@cumc.columbia.edu,Accurate Uncertainty Estimation and Decomposition in Ensemble Learning
neurips,2019,2,4806,Marianthi-Anna,Kioumourtzoglou,columbia,Columbia University,jpaisley@columbia.edu,Accurate Uncertainty Estimation and Decomposition in Ensemble Learning
neurips,2019,3,4806,Brent,Coull,harvard,Harvard University,bcoull@hsph.harvard.edu,Accurate Uncertainty Estimation and Decomposition in Ensemble Learning
neurips,2019,0,7485,Yoav,Wald,huji,Hebrew University / Google,yoav.wald@mail.huji.ac.il,Globally Optimal Learning for Structured Elliptical Losses
neurips,2019,1,7485,Nofar,Noy,huji,Hebrew University,nofar.noy@mail.huji.ac.il,Globally Optimal Learning for Structured Elliptical Losses
neurips,2019,2,7485,Gal,Elidan,google,Google,awiesel@google.com,Globally Optimal Learning for Structured Elliptical Losses
neurips,2019,3,7485,Ami,Wiesel,google,"Google Research and The Hebrew University of Jerusalem, Israel",elidan@google.com,Globally Optimal Learning for Structured Elliptical Losses
neurips,2019,0,2774,David,Berthelot,google,Google Brain,dberth@google.com,MixMatch: A Holistic Approach to Semi-Supervised Learning
neurips,2019,1,2774,Nicholas,Carlini,google,Google,ncarlini@google.com,MixMatch: A Holistic Approach to Semi-Supervised Learning
neurips,2019,2,2774,Ian,Goodfellow,mailfence,Google Brain,ian-academic@mailfence.com,MixMatch: A Holistic Approach to Semi-Supervised Learning
neurips,2019,3,2774,Nicolas,Papernot,google,University of Toronto,avitalo@google.com,MixMatch: A Holistic Approach to Semi-Supervised Learning
neurips,2019,4,2774,Avital,Oliver,google,Google Brain,papernot@google.com,MixMatch: A Holistic Approach to Semi-Supervised Learning
neurips,2019,5,2774,Colin,Raffel,google,Google Brain,craffel@google.com,MixMatch: A Holistic Approach to Semi-Supervised Learning
neurips,2019,0,8869,Qiyang,Li,toronto,University of Toronto,jlucas@cs.toronto.edu,Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks
neurips,2019,1,8869,Saminul,Haque,toronto,University of Toronto,rgrosse@cs.toronto.edu,Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks
neurips,2019,2,8869,Cem,Anil,vectorinstitute,University of Toronto; Vector Institute,j.jacobsen@vectorinstitute.ai,Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks
neurips,2019,3,8869,James,Lucas,utoronto,University of Toronto,qiyang.li@mail.utoronto.ca,Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks
neurips,2019,4,8869,Roger,Grosse,utoronto,University of Toronto,saminul.haque@mail.utoronto.ca,Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks
neurips,2019,5,8869,Joern-Henrik,Jacobsen,utoronto,Vector Institute,cem.anil@mail.utoronto.ca,Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks
neurips,2019,0,6459,Ji,Feng,nju,Sinovation Ventures,fengj@lamda.nju.edu.cn,Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder
neurips,2019,1,6459,Qi-Zhi,Cai,nju,Sinovation Ventures,zhouzh@lamda.nju.edu.cn,Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder
neurips,2019,2,6459,Zhi-Hua,Zhou,chuangxin,Nanjing University,caiqizhi@chuangxin.com,Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder
neurips,2019,0,8392,Fanny,Yang,ethz,"Stanford University, ETH Zurich",fan.yang@stat.math.ethz.ch,Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness
neurips,2019,1,8392,Zuowen,Wang,ethz,ETH Zurich,wangzu@ethz.ch,Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness
neurips,2019,2,8392,Christina,Heinze-Deml,ethz,ETH Zurich,heinzedeml@stat.math.ethz.ch,Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness
neurips,2019,0,6051,Ahmed,Alaa,,UCLA,,Attentive State-Space Modeling of Disease Progression
neurips,2019,1,6051,Mihaela,van der Schaar,,"University of Cambridge, Alan Turing Institute and UCLA",,Attentive State-Space Modeling of Disease Progression
neurips,2019,0,4215,Guillaume,Gautier,inria,"CNRS, INRIA, Univ. Lille",g.gautier@inria.fr,On two ways to use determinantal point processes for Monte Carlo integration
neurips,2019,1,4215,Rémi,Bardenet,deepmind,University of Lille,valkom@deepmind.com,On two ways to use determinantal point processes for Monte Carlo integration
neurips,2019,2,4215,Michal,Valko,gmail,DeepMind Paris and Inria Lille - Nord Europe,remi.bardenet@gmail.com,On two ways to use determinantal point processes for Monte Carlo integration
neurips,2019,0,5005,Jinjin,Tian,cmu,Carnegie Mellon University,aramdas@cmu.edu,ADDIS: an adaptive discarding algorithm for online FDR control with conservative nulls
neurips,2019,1,5005,Aaditya,Ramdas,cmu,Carnegie Mellon University,jinjint@andrew.cmu.edu,ADDIS: an adaptive discarding algorithm for online FDR control with conservative nulls
neurips,2019,0,1221,Bowen,Li,ox,University of Oxford,bowen.li@cs.ox.ac.uk,Controllable Text-to-Image Generation
neurips,2019,1,1221,Xiaojuan,Qi,ox,University of Oxford,thomas.lukasiewicz@cs.ox.ac.uk,Controllable Text-to-Image Generation
neurips,2019,2,1221,Thomas,Lukasiewicz,ox,University of Oxford,xiaojuan.qi@eng.ox.ac.uk,Controllable Text-to-Image Generation
neurips,2019,3,1221,Philip,Torr,ox,University of Oxford,philip.torr@eng.ox.ac.uk,Controllable Text-to-Image Generation
neurips,2019,0,9226,Aida,Rahmattalabi,usc,University of Southern California,rahmatta@usc.edu,Exploring Algorithmic Fairness in Robust Graph Covering Problems
neurips,2019,1,9226,Phebe,Vayanos,usc,University of Southern California,phebe.vayanos@usc.edu,Exploring Algorithmic Fairness in Robust Graph Covering Problems
neurips,2019,2,9226,Anthony,Fulginiti,du,University of Denver,anthony.fulginiti@du.edu,Exploring Algorithmic Fairness in Robust Graph Covering Problems
neurips,2019,3,9226,Eric,Rice,usc,University of Southern California,ericr@usc.edu,Exploring Algorithmic Fairness in Robust Graph Covering Problems
neurips,2019,4,9226,Bryan,Wilder,harvard,,bwilder@g.harvard.edu,Exploring Algorithmic Fairness in Robust Graph Covering Problems
neurips,2019,5,9226,Amulya,Yadav,psu,Pennsylvania State University,amulya@psu.edu,Exploring Algorithmic Fairness in Robust Graph Covering Problems
neurips,2019,6,9226,Milind,Tambe,harvard,USC,milind_tambe@harvard.edu,Exploring Algorithmic Fairness in Robust Graph Covering Problems
neurips,2019,0,4416,Guodong,Zhang,toronto,University of Toronto,gdzhang@cs.toronto.edu,Fast Convergence of Natural Gradient Descent for Over-Parameterized Neural Networks
neurips,2019,1,4416,James,Martens,toronto,DeepMind,rgrosse@cs.toronto.edu,Fast Convergence of Natural Gradient Descent for Over-Parameterized Neural Networks
neurips,2019,2,4416,Roger,Grosse,google,University of Toronto,jamesmartens@google.com,Fast Convergence of Natural Gradient Descent for Over-Parameterized Neural Networks
neurips,2019,0,2887,Sébastien,Arnold,usc,University of Southern California,seb.arnold@usc.edu,Reducing the variance in online optimization by transporting past gradients
neurips,2019,1,2887,Pierre-Antoine,Manzagol,google,Google,manzagop@google.com,Reducing the variance in online optimization by transporting past gradients
neurips,2019,2,2887,Reza,Babanezhad Harikandeh,ubc,UBC,rezababa@cs.ubc.ca,Reducing the variance in online optimization by transporting past gradients
neurips,2019,3,2887,Ioannis,Mitliagkas,umontreal,Mila & University of Montreal,ioannis@iro.umontreal.ca,Reducing the variance in online optimization by transporting past gradients
neurips,2019,4,2887,Nicolas,Le Roux,google,Google Brain,nlr@google.com,Reducing the variance in online optimization by transporting past gradients
neurips,2019,0,8215,Benyamin,Allahgholizadeh Haghi,,California Institute of Technology,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,1,8215,Spencer,Kellis,,California Institute of Technology,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,2,8215,Sahil,Shah,,California Institute of Technology,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,3,8215,Maitreyi,Ashok,,California Institute of Technology,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,4,8215,Luke,Bashford,,California Institute of Technology,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,5,8215,Daniel,Kramer,,University of Southern California,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,6,8215,Brian,Lee,,University of Southern California,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,7,8215,Charles,Liu,,University of Southern California,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,8,8215,Richard,Andersen,,California Institute of Technology,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,9,8215,Azita,Emami,,California Institute of Technology,,Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
neurips,2019,0,7511,Jenny,Liu,toronto,"Vector Institute, University of Toronto",jyliu@cs.toronto.edu,Graph Normalizing Flows
neurips,2019,1,7511,Aviral,Kumar,berkeley,UC Berkeley,aviralk@berkeley.edu,Graph Normalizing Flows
neurips,2019,2,7511,Jimmy,Ba,toronto,University of Toronto / Vector Institute,jba@cs.toronto.edu,Graph Normalizing Flows
neurips,2019,3,7511,Jamie,Kiros,google,Google Inc.,kiros@google.com,Graph Normalizing Flows
neurips,2019,4,7511,Kevin,Swersky,google,Google,kswersky@google.com,Graph Normalizing Flows
neurips,2019,0,1010,Hao,Zheng,hotmail,East China Normal University,wsnbzh@hotmail.com,Cascaded Dilated Dense Network with Two-step Data Consistency for MRI Reconstruction
neurips,2019,1,1010,Faming,Fang,ecnu,East China Normal University,fmfang@cs.ecnu.edu.cn,Cascaded Dilated Dense Network with Two-step Data Consistency for MRI Reconstruction
neurips,2019,2,1010,Guixu,Zhang,ecnu,East China Normal University,gxzhang@cs.ecnu.edu.cn,Cascaded Dilated Dense Network with Two-step Data Consistency for MRI Reconstruction
neurips,2019,0,1100,Guruprasad,Raghavan,caltech,California Institute of Technology,graghava@caltech.edu,Neural networks grown and self-organized by noise
neurips,2019,1,1100,Matt,Thomson,caltech,California Institute of Technology,mthomson@caltech.edu,Neural networks grown and self-organized by noise
neurips,2019,0,8301,Jie,Ren,google,Google Brain,jjren@google.com,Likelihood Ratios for Out-of-Distribution Detection
neurips,2019,1,8301,Peter,Liu,google,Google Brain,peterjliu@google.com,Likelihood Ratios for Out-of-Distribution Detection
neurips,2019,2,8301,Emily,Fertig,google,Google Research,emilyaf@google.com,Likelihood Ratios for Out-of-Distribution Detection
neurips,2019,3,8301,Jasper,Snoek,google,Google Brain,jsnoek@google.com,Likelihood Ratios for Out-of-Distribution Detection
neurips,2019,4,8301,Ryan,Poplin,google,Google,rpoplin@google.com,Likelihood Ratios for Out-of-Distribution Detection
neurips,2019,5,8301,Mark,Depristo,google,Google,mdepristo@google.com,Likelihood Ratios for Out-of-Distribution Detection
neurips,2019,6,8301,Joshua,Dillon,google,Google,jvdillon@google.com,Likelihood Ratios for Out-of-Distribution Detection
neurips,2019,7,8301,Balaji,Lakshminarayanan,google,Google DeepMind,balajiln@google.com,Likelihood Ratios for Out-of-Distribution Detection
neurips,2019,0,6705,Biao,Zhang,ed,University of Edinburgh,B.Zhang@ed.ac.uk,Root Mean Square Layer Normalization
neurips,2019,1,6705,Rico,Sennrich,uzh,University of Edinburgh,sennrich@cl.uzh.ch,Root Mean Square Layer Normalization
neurips,2019,0,850,Naganand,Yadati,gmail,Indian Institute of Science,y.naganand@gmail.com,HyperGCN: A New Method For Training Graph Convolutional Networks on Hypergraphs
neurips,2019,1,850,Madhav,Nimishakavi,gmail,Indian Institute of Science,cse.madhav@gmail.com,HyperGCN: A New Method For Training Graph Convolutional Networks on Hypergraphs
neurips,2019,2,850,Prateek,Yadav,gmail,Indian Institute of Science,ugprateek@gmail.com,HyperGCN: A New Method For Training Graph Convolutional Networks on Hypergraphs
neurips,2019,3,850,Vikram,Nitin,gmail,Indian Institute of Science,vikramnitin9@gmail.com,HyperGCN: A New Method For Training Graph Convolutional Networks on Hypergraphs
neurips,2019,4,850,Anand,Louis,iisc,"Indian Institute of Science, Bengaluru",anandl@iisc.ac.in,HyperGCN: A New Method For Training Graph Convolutional Networks on Hypergraphs
neurips,2019,5,850,Partha,Talukdar,talukdar,"Indian Institute of Science, Bangalore",partha@talukdar.net,HyperGCN: A New Method For Training Graph Convolutional Networks on Hypergraphs
neurips,2019,0,1985,Edgar,Dobriban,upenn,University of Pennsylvania,dobriban@wharton.upenn.edu,Asymptotics for Sketching in Least Squares Regression
neurips,2019,1,1985,Sifan,Liu,stanford,Stanford University,sfliu@stanford.edu,Asymptotics for Sketching in Least Squares Regression
neurips,2019,0,4545,Francis,Williams,,New York University,,Gradient Dynamics of Shallow Univariate ReLU Networks
neurips,2019,1,4545,Matthew,Trager,,NYU,,Gradient Dynamics of Shallow Univariate ReLU Networks
neurips,2019,2,4545,Daniele,Panozzo,,NYU,,Gradient Dynamics of Shallow Univariate ReLU Networks
neurips,2019,3,4545,Claudio,Silva,,New York University,,Gradient Dynamics of Shallow Univariate ReLU Networks
neurips,2019,4,4545,Denis,Zorin,,New York University,,Gradient Dynamics of Shallow Univariate ReLU Networks
neurips,2019,5,4545,Joan,Bruna,,NYU,,Gradient Dynamics of Shallow Univariate ReLU Networks
neurips,2019,0,4452,Raymond,Yeh,illinois,University of Illinois at UrbanaChampaign,yeh17@illinois.edu,Chirality Nets for Human Pose Regression
neurips,2019,1,4452,Yuan-Ting,Hu,illinois,University of Illinois Urbana-Champaign,ythu2@illinois.edu,Chirality Nets for Human Pose Regression
neurips,2019,2,4452,Alexander,Schwing,illinois,University of Illinois at Urbana-Champaign,aschwing@illinois.edu,Chirality Nets for Human Pose Regression
neurips,2019,0,9052,Jingxiang,Lin,,University of Illinois at Urbana-Champaign,,TAB-VCR: Tags and Attributes based VCR Baselines
neurips,2019,1,9052,Unnat,Jain,,University of Illinois at Urbana Champaign,,TAB-VCR: Tags and Attributes based VCR Baselines
neurips,2019,2,9052,Alexander,Schwing,,University of Illinois at Urbana-Champaign,,TAB-VCR: Tags and Attributes based VCR Baselines
neurips,2019,0,4994,Gaurush,Hiranandani,illinois,University of Illinois at Urbana-Champaign,gaurush2@illinois.edu,Multiclass Performance Metric Elicitation
neurips,2019,1,4994,Shant,Boodaghians,illinois,UIUC,boodagh2@illinois.edu,Multiclass Performance Metric Elicitation
neurips,2019,2,4994,Ruta,Mehta,illinois,UIUC,rutameht@illinois.edu,Multiclass Performance Metric Elicitation
neurips,2019,3,4994,Oluwasanmi,Koyejo,illinois,UIUC,sanmi@illinois.edu,Multiclass Performance Metric Elicitation
neurips,2019,0,7252,Yi Chern,Tan,yale,Yale University,yichern.tan@yale.edu,Assessing Social and Intersectional Biases in Contextualized Word Representations
neurips,2019,1,7252,L. Elisa,Celis,yale,Yale University,elisa.celis@yale.edu,Assessing Social and Intersectional Biases in Contextualized Word Representations
neurips,2019,0,3739,Chenwei,DING,sydney,The University of Sydney,cdin2224@uni.sydney.edu.au,Likelihood-Free Overcomplete ICA and Applications In Causal Discovery
neurips,2019,1,3739,Mingming,Gong,unimelb,University of Melbourne,mingming.gong@unimelb.edu.au,Likelihood-Free Overcomplete ICA and Applications In Causal Discovery
neurips,2019,2,3739,Kun,Zhang,cmu,CMU,kunz1@cmu.edu,Likelihood-Free Overcomplete ICA and Applications In Causal Discovery
neurips,2019,3,3739,Dacheng,Tao,sydney,University of Sydney,dacheng.tao@uni.sydney.edu.au,Likelihood-Free Overcomplete ICA and Applications In Causal Discovery
neurips,2019,0,3167,Xuezhe,Ma,cmu,Carnegie Mellon University,xuezhem@cs.cmu.edu,MaCow: Masked Convolutional Generative Flow
neurips,2019,1,3167,Xiang,Kong,cmu,Carnegie Mellon University,xiangk@cs.cmu.edu,MaCow: Masked Convolutional Generative Flow
neurips,2019,2,3167,Shanghang,Zhang,cmu,Carnegie Mellon University,shanghaz@andrew.cmu.edu,MaCow: Masked Convolutional Generative Flow
neurips,2019,3,3167,Eduard,Hovy,cmu,Carnegie Mellon University,hovy@cmu.edu,MaCow: Masked Convolutional Generative Flow
neurips,2019,0,277,Zijun,Gao,stanford,Stanford University,zijungao@stanford.edu,Batched Multi-armed Bandits Problem
neurips,2019,1,277,Yanjun,Han,stanford,Stanford University,yjhan@stanford.edu,Batched Multi-armed Bandits Problem
neurips,2019,2,277,Zhimei,Ren,stanford,Stanford University,zren@stanford.edu,Batched Multi-armed Bandits Problem
neurips,2019,3,277,Zhengqing,Zhou,stanford,Stanford University,zqzhou@stanford.edu,Batched Multi-armed Bandits Problem
neurips,2019,0,3776,Samuli,Laine,,NVIDIA,,High-Quality Self-Supervised Deep Image Denoising
neurips,2019,1,3776,Tero,Karras,,NVIDIA,,High-Quality Self-Supervised Deep Image Denoising
neurips,2019,2,3776,Jaakko,Lehtinen,,Aalto University & NVIDIA,,High-Quality Self-Supervised Deep Image Denoising
neurips,2019,3,3776,Timo,Aila,,NVIDIA,,High-Quality Self-Supervised Deep Image Denoising
neurips,2019,0,9289,Anthony,Ndirango,intel,Intel AI Lab,tyler.p.lee@intel.com,Generalization in multitask deep neural classifiers: a statistical physics approach
neurips,2019,1,9289,Tyler,Lee,intel,Intel AI Lab,anthony.ndirango@intel.com,Generalization in multitask deep neural classifiers: a statistical physics approach
neurips,2019,0,6906,Dominik,Janzing,amazon,Amazon,janzind@amazon.com,Causal Regularization
neurips,2019,0,5303,Lin,Chen,yale,Yale University,lin.chen@yale.edu,Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond
neurips,2019,1,5303,Hossein,Esfandiari,google,Google Research,esfandiari@google.com,Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond
neurips,2019,2,5303,Gang,Fu,google,Google Research,thomasfu@google.com,Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond
neurips,2019,3,5303,Vahab,Mirrokni,google,Google Research NYC,mirrokni@google.com,Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond
neurips,2019,0,1771,Emilien,Dupont,ox,Oxford University,dupont@stats.ox.ac.uk,Augmented Neural ODEs
neurips,2019,1,1771,Arnaud,Doucet,ox,Oxford,doucet@stats.ox.ac.uk,Augmented Neural ODEs
neurips,2019,2,1771,Yee Whye,Teh,ox,"University of Oxford, DeepMind",y.w.teh@stats.ox.ac.uk,Augmented Neural ODEs
neurips,2019,0,3751,Wenqing,Hu,mst,Missouri S&T,huwen@mst.edu,Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent
neurips,2019,1,3751,Chris Junchi,Li,gmail,Tecent AI Lab,junchi.li.duke@gmail.com,Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent
neurips,2019,2,3751,Xiangru,Lian,xrlian,University of Rochester,admin@mail.xrlian.com,Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent
neurips,2019,3,3751,Ji,Liu,gmail,"University of Rochester, Tencent AI lab",ji.liu.uwisc@gmail.com,Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent
neurips,2019,4,3751,Huizhuo,Yuan,pku,Peking University,hzyuan@pku.edu.cn,Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent
neurips,2019,0,1633,Rinu,Boney,,Aalto University,,Regularizing Trajectory Optimization with Denoising Autoencoders
neurips,2019,1,1633,Norman,Di Palo,,-,,Regularizing Trajectory Optimization with Denoising Autoencoders
neurips,2019,2,1633,Mathias,Berglund,,Curious AI,,Regularizing Trajectory Optimization with Denoising Autoencoders
neurips,2019,3,1633,Alexander,Ilin,,Aalto University,,Regularizing Trajectory Optimization with Denoising Autoencoders
neurips,2019,4,1633,Juho,Kannala,,Aalto University,,Regularizing Trajectory Optimization with Denoising Autoencoders
neurips,2019,5,1633,Antti,Rasmus,,The Curious AI Company,,Regularizing Trajectory Optimization with Denoising Autoencoders
neurips,2019,6,1633,Harri,Valpola,,Curious AI,,Regularizing Trajectory Optimization with Denoising Autoencoders
neurips,2019,0,8678,Uthaipon,Tantipongpipat,,Georgia Tech,,Multi-Criteria Dimensionality Reduction with Applications to Fairness
neurips,2019,1,8678,Samira,Samadi,,Georgia Tech,,Multi-Criteria Dimensionality Reduction with Applications to Fairness
neurips,2019,2,8678,Mohit,Singh,,Georgia Tech,,Multi-Criteria Dimensionality Reduction with Applications to Fairness
neurips,2019,3,8678,Jamie,Morgenstern,,University of Washington,,Multi-Criteria Dimensionality Reduction with Applications to Fairness
neurips,2019,4,8678,Santosh,Vempala,,Georgia Tech,,Multi-Criteria Dimensionality Reduction with Applications to Fairness
neurips,2019,0,8871,Dina,Obeid,,Harvard University,dinaobeid@seas,Structured and Deep Similarity Matching via  Structured and Deep Hebbian Networks
neurips,2019,1,8871,Hugo,Ramambason,,Havard University,hugo_ramambason@g,Structured and Deep Similarity Matching via  Structured and Deep Hebbian Networks
neurips,2019,2,8871,Cengiz,Pehlevan,harvard,Harvard University,cpehlevan@seas.harvard.edu,Structured and Deep Similarity Matching via  Structured and Deep Hebbian Networks
neurips,2019,0,5585,Boyi,Liu,,Northwestern University,,Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy
neurips,2019,1,5585,Qi,Cai,,Northwestern University,,Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy
neurips,2019,2,5585,Zhuoran,Yang,,Princeton University,,Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy
neurips,2019,3,5585,Zhaoran,Wang,,Northwestern University,,Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy
neurips,2019,0,2811,Tianjun,Zhang,,"University of California, Berkeley",,ANODEV2: A Coupled Neural ODE Framework
neurips,2019,1,2811,Zhewei,Yao,,UC Berkeley,,ANODEV2: A Coupled Neural ODE Framework
neurips,2019,2,2811,Amir,Gholami,,"University of California, Berkeley",,ANODEV2: A Coupled Neural ODE Framework
neurips,2019,3,2811,Joseph,Gonzalez,,UC Berkeley,,ANODEV2: A Coupled Neural ODE Framework
neurips,2019,4,2811,Kurt,Keutzer,,"EECS, UC Berkeley",,ANODEV2: A Coupled Neural ODE Framework
neurips,2019,5,2811,Michael,Mahoney,,UC Berkeley,,ANODEV2: A Coupled Neural ODE Framework
neurips,2019,6,2811,George,Biros,,University of Texas at Austin,,ANODEV2: A Coupled Neural ODE Framework
neurips,2019,0,6074,Han,Zhao,cmu,Carnegie Mellon University,han.zhao@cs.cmu.edu,Learning Neural Networks with Adaptive Regularization
neurips,2019,1,6074,Yao-Hung Hubert,Tsai,cmu,Carnegie Mellon University,yaohungt@cs.cmu.edu,Learning Neural Networks with Adaptive Regularization
neurips,2019,2,6074,Russ,Salakhutdinov,cmu,Carnegie Mellon University,rsalakhu@cs.cmu.edu,Learning Neural Networks with Adaptive Regularization
neurips,2019,3,6074,Geoffrey,Gordon,microsoft,MSR Montréal & CMU,geoff.gordon@microsoft.com,Learning Neural Networks with Adaptive Regularization
neurips,2019,0,1585,Yihan,Jiang,,University of Washington Seattle,,Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels
neurips,2019,1,1585,Hyeji,Kim,,Samsung AI Center Cambridge,,Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels
neurips,2019,2,1585,Himanshu,Asnani,,"University of Washington, Seattle",,Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels
neurips,2019,3,1585,Sreeram,Kannan,,University of Washington,,Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels
neurips,2019,4,1585,Sewoong,Oh,,University of Washington,,Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels
neurips,2019,5,1585,Pramod,Viswanath,,UIUC,,Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels
neurips,2019,0,3602,Yukang,Chen,ia,"Institute of Automation, Chinese Academy of Sciences",yukang.chen@nlpr.ia.ac.cn,DetNAS: Backbone Search for Object Detection
neurips,2019,1,3602,Tong,Yang,ia,Megvii Inc.,gfmeng@nlpr.ia.ac.cn,DetNAS: Backbone Search for Object Detection
neurips,2019,2,3602,Xiangyu,Zhang,ia,MEGVII Technology,xinyu.xiao@nlpr.ia.ac.cn,DetNAS: Backbone Search for Object Detection
neurips,2019,3,3602,GAOFENG,MENG,megvii,"Institute of Automation, Chinese Academy of Sciences",yangtong@megvii.com,DetNAS: Backbone Search for Object Detection
neurips,2019,4,3602,Xinyu,Xiao,megvii,"National Laboratory of Pattern recognition (NLPR),  Institute of Automation of Chinese Academy of Sciences (CASIA)",zhangxiangyu@megvii.com,DetNAS: Backbone Search for Object Detection
neurips,2019,5,3602,Jian,Sun,megvii,"Megvii, Face++",sunjian@megvii.com,DetNAS: Backbone Search for Object Detection
neurips,2019,0,4107,Laura Rose,Edmondson,sheffield,University of Sheffield,lredmondson1@sheffield.ac.uk,Nonlinear scaling of resource allocation in sensory bottlenecks
neurips,2019,1,4107,Alejandro,Jimenez Rodriguez,sheffield,University of Sheffield,a.jimenez-rodriguez@sheffield.ac.uk,Nonlinear scaling of resource allocation in sensory bottlenecks
neurips,2019,2,4107,Hannes P.,Saal,sheffield,University of Sheffield,h.saal@sheffield.ac.uk,Nonlinear scaling of resource allocation in sensory bottlenecks
neurips,2019,0,4068,Carl,Allen,ed,University of Edinburgh,carl.allen@ed.ac.uk,What the Vec? Towards Probabilistically Grounded Embeddings
neurips,2019,1,4068,Ivana,Balazevic,ed,University of Edinburgh,ivana.balazevic@ed.ac.uk,What the Vec? Towards Probabilistically Grounded Embeddings
neurips,2019,2,4068,Timothy,Hospedales,ed,University of Edinburgh,t.hospedales@ed.ac.uk,What the Vec? Towards Probabilistically Grounded Embeddings
neurips,2019,0,7338,Johannes,Klicpera,tum,Technical University of Munich,klicpera@in.tum.de,Diffusion Improves Graph Learning
neurips,2019,1,7338,Stefan,Weißenberger,tum,Technical University of Munich,stefan.weissenberger@in.tum.de,Diffusion Improves Graph Learning
neurips,2019,2,7338,Stephan,Günnemann,tum,Technical University of Munich,guennemann@in.tum.de,Diffusion Improves Graph Learning
neurips,2019,0,7778,Qi,Lei,,University of Texas at Austin,,"Inverting Deep Generative models, One layer at a time"
neurips,2019,1,7778,Ajil,Jalal,,University of Texas at Austin,,"Inverting Deep Generative models, One layer at a time"
neurips,2019,2,7778,Inderjit,Dhillon,,UT Austin & Amazon,,"Inverting Deep Generative models, One layer at a time"
neurips,2019,3,7778,Alexandros,Dimakis,,"University of Texas, Austin",,"Inverting Deep Generative models, One layer at a time"
neurips,2019,0,5546,Akshay,Krishnamurthy,umass,Microsoft,akshay@cs.umass.edu,Sample Complexity of Learning Mixture of Sparse Linear Regressions
neurips,2019,1,5546,Arya,Mazumdar,umass,University of Massachusetts Amherst,mcgregor@cs.umass.edu,Sample Complexity of Learning Mixture of Sparse Linear Regressions
neurips,2019,2,5546,Andrew,McGregor,umass,University of Massachusetts Amherst,arya@cs.umass.edu,Sample Complexity of Learning Mixture of Sparse Linear Regressions
neurips,2019,3,5546,Soumyabrata,Pal,umass,University of Massachusetts Amherst,spal@cs.umass.edu,Sample Complexity of Learning Mixture of Sparse Linear Regressions
neurips,2019,0,5207,Hadi,Salman,microsoft,Microsoft Research AI,hadi.salman@microsoft.com,A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks
neurips,2019,1,5207,Greg,Yang,microsoft,Microsoft Research,gregyang@microsoft.com,A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks
neurips,2019,2,5207,Huan,Zhang,huan-zhang,UCLA,huan@huan-zhang.com,A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks
neurips,2019,3,5207,Cho-Jui,Hsieh,ucla,UCLA,chohsieh@cs.ucla.edu,A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks
neurips,2019,4,5207,Pengchuan,Zhang,microsoft,Microsoft Research,penzhan@microsoft.com,A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks
neurips,2019,0,3017,Philippe,Casgrain,utoronto,Citadel / University of Toronto,p.casgrain@mail.utoronto.ca,A Latent Variational Framework for Stochastic Optimization
neurips,2019,0,3961,Yue,Sun,uw,University of Washington,yuesun@uw.edu,Escaping from saddle points on Riemannian manifolds
neurips,2019,1,3961,Nicolas,Flammarion,epfl,EPFL,nicolas.flammarion@epfl.ch,Escaping from saddle points on Riemannian manifolds
neurips,2019,2,3961,Maryam,Fazel,uw,University of Washington,mfazel@uw.edu,Escaping from saddle points on Riemannian manifolds
neurips,2019,0,8501,Maher,Nouiehed,usc,American University of Beirut,nouiehed@usc.edu,Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods
neurips,2019,1,8501,Maziar,Sanjabi,usc,USC,sanjabi@usc.edu,Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods
neurips,2019,2,8501,Tianjian,Huang,usc,University of Southern California,tianjian@usc.edu,Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods
neurips,2019,3,8501,Jason,Lee,princeton,Princeton University,jasonlee@princeton.edu,Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods
neurips,2019,4,8501,Meisam,Razaviyayn,usc,University of Southern California,razaviya@usc.edu,Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods
neurips,2019,0,7148,Andre,Barreto,google,DeepMind,andrebarreto@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,1,7148,Diana,Borsa,google,DeepMind,borsa@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,2,7148,Shaobo,Hou,google,DeepMind,shaobohou@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,3,7148,Gheorghe,Comanici,google,DeepMind,gcomanici@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,4,7148,Eser,Aygün,google,DeepMind,eser@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,5,7148,Philippe,Hamel,google,Deepmind,hamelphi@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,6,7148,Daniel,Toyama,google,DeepMind,kenjitoyama@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,7,7148,Jonathan,hunt,google,DeepMind,jjhunt@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,8,7148,Shibl,Mourad,google,Google,shibl@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,9,7148,David,Silver,google,DeepMind,davidsilver@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,10,7148,Doina,Precup,google,DeepMind,doinap@google.com,The Option Keyboard: Combining Skills in Reinforcement Learning
neurips,2019,0,1521,Lili,Su,mit,MIT,lilisu@mit.edu,On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective
neurips,2019,1,1521,Pengkun,Yang,princeton,Princeton University,pengkuny@princeton.edu,On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective
neurips,2019,0,4002,Lei,Xu,mit,MIT,leix@mit.edu,Modeling Tabular data using Conditional GAN
neurips,2019,1,4002,Maria,Skoularidou,cam,University of Cambridge,ms2407@cam.ac.uk,Modeling Tabular data using Conditional GAN
neurips,2019,2,4002,Alfredo,Cuesta-Infante,urjc,Universidad Rey Juan Carlos,alfredo.cuesta@urjc.es,Modeling Tabular data using Conditional GAN
neurips,2019,3,4002,Kalyan,Veeramachaneni,mit,Massachusetts Institute of Technology,kalyanv@mit.edu,Modeling Tabular data using Conditional GAN
neurips,2019,0,2038,Sharan,Vaswani,,"Mila, Université de Montréal",,"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates"
neurips,2019,1,2038,Aaron,Mishkin,,University of British Columbia,,"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates"
neurips,2019,2,2038,Issam,Laradji,,University of British Columbia,,"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates"
neurips,2019,3,2038,Mark,Schmidt,,University of British Columbia,,"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates"
neurips,2019,4,2038,Gauthier,Gidel,,Mila,,"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates"
neurips,2019,5,2038,Simon,Lacoste-Julien,,"Mila, Université de Montréal",,"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates"
neurips,2019,0,6614,Yonathan,Efroni,,Technion,,Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies
neurips,2019,1,6614,Nadav,Merlis,,Technion,,Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies
neurips,2019,2,6614,Mohammad,Ghavamzadeh,,Facebook AI Research,,Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies
neurips,2019,3,6614,Shie,Mannor,,Technion,,Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies
neurips,2019,0,6485,Yoan,Russac,ens,Ecole Normale Supérieure,yoan.russac@ens.fr,Weighted Linear Bandits for Non-Stationary Environments
neurips,2019,1,6485,Claire,Vernade,google,Google DeepMind,vernade@google.com,Weighted Linear Bandits for Non-Stationary Environments
neurips,2019,2,6485,Olivier,Cappé,cnrs,CNRS,olivier.cappe@cnrs.fr,Weighted Linear Bandits for Non-Stationary Environments
neurips,2019,0,1824,Ya-Chien,Chang,ucsd,"University of California, San Diego",yac021@eng.ucsd.edu,Neural Lyapunov Control
neurips,2019,1,1824,Nima,Roohi,ucsd,University of California San Diego,nroohi@eng.ucsd.edu,Neural Lyapunov Control
neurips,2019,2,1824,Sicun,Gao,ucsd,"University of California, San Diego",sicung@eng.ucsd.edu,Neural Lyapunov Control
neurips,2019,0,5233,Adithya M,Devraj,,University of Florida,,Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization
neurips,2019,1,5233,Jianshu,Chen,,Tencent AI Lab,,Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization
neurips,2019,0,8850,Samuel,Greydanus,google,Oregon State University,sgrey@google.com,Hamiltonian Neural Networks
neurips,2019,1,8850,Misko,Dzamba,gmail,Freenome,mouse9911@gmail.com,Hamiltonian Neural Networks
neurips,2019,2,8850,Jason,Yosinski,uber,Uber AI; Recursion,yosinski@uber.com,Hamiltonian Neural Networks
neurips,2019,0,4837,Tamas,Madarasz,ox,University of Oxford,tamas.madarasz@ndcn.ox.ac.uk,Better Transfer Learning with Inferred Successor Maps
neurips,2019,1,4837,Tim,Behrens,ox,University of Oxford,behrens@fmrib.ox.ac.uk,Better Transfer Learning with Inferred Successor Maps
neurips,2019,0,6848,Arindam,Banerjee,,Voleon,,Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond
neurips,2019,1,6848,Qilong,Gu,,University of Minnesota Twin Cities,,Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond
neurips,2019,2,6848,Vidyashankar,Sivakumar,,University of Minnesota,,Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond
neurips,2019,3,6848,Steven,Wu,,University of Minnesota,,Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond
neurips,2019,0,4594,John,Lawson,stanford,Stanford University,jdlawson@stanford.edu,Energy-Inspired Models: Learning with Sampler-Induced Distributions
neurips,2019,1,4594,George,Tucker,google,Google Brain,gjt@google.com,Energy-Inspired Models: Learning with Sampler-Induced Distributions
neurips,2019,2,4594,Bo,Dai,google,Google Brain,bodai@google.com,Energy-Inspired Models: Learning with Sampler-Induced Distributions
neurips,2019,3,4594,Rajesh,Ranganath,nyu,New York University,rajeshr@cims.nyu.edu,Energy-Inspired Models: Learning with Sampler-Induced Distributions
neurips,2019,0,981,Yuki,Yoshida,,The University of Tokyo,yoshida@mns,Data-Dependence of Plateau Phenomenon in Learning with Neural Network --- Statistical Mechanical Analysis
neurips,2019,1,981,Masato,Okada,u-tokyo,The University of Tokyo,okada@edu.k.u-tokyo.ac.jp,Data-Dependence of Plateau Phenomenon in Learning with Neural Network --- Statistical Mechanical Analysis
neurips,2019,0,394,Junbang,Liang,,"University of Maryland, College Park",,Differentiable Cloth Simulation for Inverse Problems
neurips,2019,1,394,Ming,Lin,,University of Maryland - College Park,,Differentiable Cloth Simulation for Inverse Problems
neurips,2019,2,394,Vladlen,Koltun,,Intel Labs,,Differentiable Cloth Simulation for Inverse Problems
neurips,2019,0,4241,Roman,Werpachowski,google,DeepMind,romanw@google.com,Detecting Overfitting via Adversarial Examples
neurips,2019,1,4241,András,György,google,DeepMind,agyorgy@google.com,Detecting Overfitting via Adversarial Examples
neurips,2019,2,4241,Csaba,Szepesvari,google,DeepMind / University of Alberta,szepi@google.com,Detecting Overfitting via Adversarial Examples
neurips,2019,0,642,Zhengyang,Shen,unc,University of North Carolina at Chapel Hill,zyshen@cs.unc.edu,Region-specific Diffeomorphic Metric Mapping
neurips,2019,1,642,Francois-Xavier,Vialard,u-pem,University Paris-Est,francois-xavier.vialard@u-pem.fr,Region-specific Diffeomorphic Metric Mapping
neurips,2019,2,642,Marc,Niethammer,unc,UNC Chapel Hill,mn@cs.unc.edu,Region-specific Diffeomorphic Metric Mapping
neurips,2019,0,2238,Anette,Hunziker,,ETH Zurich,,Teaching Multiple Concepts to a Forgetful Learner
neurips,2019,1,2238,Yuxin,Chen,,UChicago,,Teaching Multiple Concepts to a Forgetful Learner
neurips,2019,2,2238,Oisin,Mac Aodha,,California Institute of Technology,,Teaching Multiple Concepts to a Forgetful Learner
neurips,2019,3,2238,Manuel,Gomez Rodriguez,,Max Planck Institute for Software Systems,,Teaching Multiple Concepts to a Forgetful Learner
neurips,2019,4,2238,Andreas,Krause,,ETH Zurich,,Teaching Multiple Concepts to a Forgetful Learner
neurips,2019,5,2238,Pietro,Perona,,California Institute of Technology,,Teaching Multiple Concepts to a Forgetful Learner
neurips,2019,6,2238,Yisong,Yue,,Caltech,,Teaching Multiple Concepts to a Forgetful Learner
neurips,2019,7,2238,Adish,Singla,,MPI-SWS,,Teaching Multiple Concepts to a Forgetful Learner
neurips,2019,0,3472,Qi,Dou,imperial,Imperial College London,qi.dou@imperial.ac.uk,Domain Generalization via Model-Agnostic Learning of Semantic Features
neurips,2019,1,3472,Daniel,Coelho de Castro,imperial,Imperial College London,dc315@imperial.ac.uk,Domain Generalization via Model-Agnostic Learning of Semantic Features
neurips,2019,2,3472,Konstantinos,Kamnitsas,imperial,Imperial College London,kk2412@imperial.ac.uk,Domain Generalization via Model-Agnostic Learning of Semantic Features
neurips,2019,3,3472,Ben,Glocker,imperial,Imperial College London,b.glocker@imperial.ac.uk,Domain Generalization via Model-Agnostic Learning of Semantic Features
neurips,2019,0,859,Antoine,Wehenkel,,ULiège,,Unconstrained Monotonic Neural Networks
neurips,2019,1,859,Gilles,Louppe,,University of Liège,,Unconstrained Monotonic Neural Networks
neurips,2019,0,6781,Daniel,Kumor,purdue,Purdue University,dkumor@purdue.edu,Efficient Identification in Linear Structural Causal Models with Instrumental Cutsets
neurips,2019,1,6781,Bryant,Chen,brex,Brex,bryant@brex.com,Efficient Identification in Linear Structural Causal Models with Instrumental Cutsets
neurips,2019,2,6781,Elias,Bareinboim,columbia,Purdue,eb@cs.columbia.edu,Efficient Identification in Linear Structural Causal Models with Instrumental Cutsets
neurips,2019,0,5434,Sawyer,Birnbaum,stanford,Stanford University,sawyerb@cs.stanford.edu,Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulations.
neurips,2019,1,5434,Volodymyr,Kuleshov,stanford,Stanford University,kuleshov@cs.stanford.edu,Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulations.
neurips,2019,2,5434,Zayd,Enam,stanford,Stanford,zayd@cs.stanford.edu,Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulations.
neurips,2019,3,5434,Pang Wei,Koh,stanford,Stanford University,pangwei@cs.stanford.edu,Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulations.
neurips,2019,4,5434,Stefano,Ermon,stanford,Stanford,ermon@cs.stanford.edu,Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulations.
neurips,2019,0,719,Shuang,Wu,,Tsinghua University,,Convolution with even-sized kernels and symmetric padding
neurips,2019,1,719,Guanrui,Wang,,Tsinghua University,,Convolution with even-sized kernels and symmetric padding
neurips,2019,2,719,Pei,Tang,,Tsinghua University,,Convolution with even-sized kernels and symmetric padding
neurips,2019,3,719,Feng,Chen,,Tsinghua University,,Convolution with even-sized kernels and symmetric padding
neurips,2019,4,719,Luping,Shi,,Tsinghua University,,Convolution with even-sized kernels and symmetric padding
neurips,2019,0,7868,Dan,Schwartz,cmu,Carnegie Mellon University,drschwar@cs.cmu.edu,Inducing brain-relevant bias in natural language processing models
neurips,2019,1,7868,Mariya,Toneva,cmu,Carnegie Mellon University,mariya@cmu.edu,Inducing brain-relevant bias in natural language processing models
neurips,2019,2,7868,Leila,Wehbe,cmu,Carnegie Mellon University,lwehbe@cmu.edu,Inducing brain-relevant bias in natural language processing models
neurips,2019,0,4289,Seyed Kamyar,Seyed Ghasemipour,toronto,"University of Toronto, Vector Institute",kamyar@cs.toronto.edu,SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies
neurips,2019,1,4289,Shixiang (Shane),Gu,toronto,Google Brain,zemel@cs.toronto.edu,SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies
neurips,2019,2,4289,Richard,Zemel,google,Vector Institute/University of Toronto,shanegu@google.com,SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies
neurips,2019,0,2832,Erik,Nijkamp,ucla,UCLA,enijkamp@ucla.edu,Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model
neurips,2019,1,2832,Mitch,Hill,ucla,UCLA Department of Statistics,mkhill@ucla.edu,Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model
neurips,2019,2,2832,Song-Chun,Zhu,ucla,UCLA,sczhu@stat.ucla.edu,Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model
neurips,2019,3,2832,Ying Nian,Wu,ucla,"University of California, Los Angeles",ywu@stat.ucla.edu,Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model
neurips,2019,0,2977,Kohei,Hayashi,preferred,Preferred Networks,hayasick@preferred.jp,Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks
neurips,2019,1,2977,Taiki,Yamaguchi,u-tokyo,The University of Tokyo,yamaguchi@hep-th.phys.s.u-tokyo.ac.jp,Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks
neurips,2019,2,2977,Yohei,Sugawara,preferred,"Preferred Networks, Inc.",suga@preferred.jp,Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks
neurips,2019,3,2977,Shin-ichi,Maeda,preferred,Preferred Networks,ichi@preferred.jp,Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks
neurips,2019,0,3627,Ben,Deverett,google,Princeton University,bendeverett@google.com,Interval timing in deep reinforcement learning agents
neurips,2019,1,3627,Ryan,Faulkner,google,Deepmind,rfaulk@google.com,Interval timing in deep reinforcement learning agents
neurips,2019,2,3627,Meire,Fortunato,google,DeepMind,meirefortunato@google.com,Interval timing in deep reinforcement learning agents
neurips,2019,3,3627,Gregory,Wayne,google,Google DeepMind,gregwayne@google.com,Interval timing in deep reinforcement learning agents
neurips,2019,4,3627,Joel,Leibo,google,DeepMind,jzl@google.com,Interval timing in deep reinforcement learning agents
neurips,2019,0,7484,Karol,Gregor,google,DeepMind,karolg@google.com,Shaping Belief States with Generative Environment Models for RL
neurips,2019,1,7484,Danilo,Jimenez Rezende,google,Google DeepMind,danilor@google.com,Shaping Belief States with Generative Environment Models for RL
neurips,2019,2,7484,Frederic,Besse,google,DeepMind,fbesse@google.com,Shaping Belief States with Generative Environment Models for RL
neurips,2019,3,7484,Yan,Wu,google,DeepMind,yanwu@google.com,Shaping Belief States with Generative Environment Models for RL
neurips,2019,4,7484,Hamza,Merzic,google,DeepMind,hamzamerzic@google.com,Shaping Belief States with Generative Environment Models for RL
neurips,2019,5,7484,Aaron,van den Oord,google,Google Deepmind,avdnoord@google.com,Shaping Belief States with Generative Environment Models for RL
neurips,2019,0,2447,Hongjoon,Ahn,skku,Sunkyunkwan University,hong0805@skku.edu,Uncertainty-based Continual Learning with Adaptive Regularization
neurips,2019,1,2447,Sungmin,Cha,skku,Sungkyunkwan University,csm9493@skku.edu,Uncertainty-based Continual Learning with Adaptive Regularization
neurips,2019,2,2447,Donggyu,Lee,skku,Sungkyunkwan university,ldk308@skku.edu,Uncertainty-based Continual Learning with Adaptive Regularization
neurips,2019,3,2447,Taesup,Moon,skku,Sungkyunkwan University (SKKU),tsmoon@skku.edu,Uncertainty-based Continual Learning with Adaptive Regularization
neurips,2019,0,8210,Haibin,YU,nus,National University of Singapore,haibin@comp.nus.edu.sg,Implicit Posterior Variational Inference for Deep Gaussian Processes
neurips,2019,1,8210,Yizhou,Chen,nus,National University of Singapore,ychen041@comp.nus.edu.sg,Implicit Posterior Variational Inference for Deep Gaussian Processes
neurips,2019,2,8210,Bryan Kian Hsiang,Low,nus,National University of Singapore,daiz@comp.nus.edu.sg,Implicit Posterior Variational Inference for Deep Gaussian Processes
neurips,2019,3,8210,Patrick,Jaillet,nus,MIT,lowkh@comp.nus.edu.sg,Implicit Posterior Variational Inference for Deep Gaussian Processes
neurips,2019,4,8210,Zhongxiang,Dai,mit,National University of Singapore,jaillet@mit.edu,Implicit Posterior Variational Inference for Deep Gaussian Processes
neurips,2019,0,7830,Paul,Michel,cmu,"Carnegie Mellon University, Language Technologies Institute",pmichel1@cs.cmu.edu,Are Sixteen Heads Really Better than One?
neurips,2019,1,7830,Omer,Levy,fb,Facebook AI Research,omerlevy@fb.com,Are Sixteen Heads Really Better than One?
neurips,2019,2,7830,Graham,Neubig,cmu,Carnegie Mellon University,gneubig@cs.cmu.edu,Are Sixteen Heads Really Better than One?
neurips,2019,0,760,Shupeng,Gui,tamu,University of Rochester,htwang@tamu.edu,Model Compression with Adversarial Robustness: A Unified Optimization Framework
neurips,2019,1,760,Haotao,Wang,tamu,Texas A&M University,atlaswang@tamu.edu,Model Compression with Adversarial Robustness: A Unified Optimization Framework
neurips,2019,2,760,Haichuan,Yang,rochester,University of Rochester,sgui2@ur.rochester.edu,Model Compression with Adversarial Robustness: A Unified Optimization Framework
neurips,2019,3,760,Chen,Yu,rochester,University of Rochester,hyang36@ur.rochester.edu,Model Compression with Adversarial Robustness: A Unified Optimization Framework
neurips,2019,4,760,Zhangyang,Wang,rochester,TAMU,cyu28@ur.rochester.edu,Model Compression with Adversarial Robustness: A Unified Optimization Framework
neurips,2019,5,760,Ji,Liu,gmail,"University of Rochester, Tencent AI lab",ji.liu.uwisc@gmail.com,Model Compression with Adversarial Robustness: A Unified Optimization Framework
neurips,2019,0,2100,Yiwen,Guo,tsinghua,Bytedance AI Lab,yza18@mails.tsinghua.edu.cn,Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks
neurips,2019,1,2100,Ziang,Yan,bytedance,Tsinghua University,guoyiwen.ai@bytedance.com,Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks
neurips,2019,2,2100,Changshui,Zhang,tsinghua,Tsinghua University,zcs@mail.tsinghua.edu.cn,Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks
neurips,2019,0,1678,Changyong,Oh,uva,University of Amsterdam,C.Oh@uva.nl,Combinatorial Bayesian Optimization using the Graph Cartesian Product
neurips,2019,1,1678,Jakub,Tomczak,qualcomm,Qualcomm AI Research,jtomczak@qti.qualcomm.com,Combinatorial Bayesian Optimization using the Graph Cartesian Product
neurips,2019,2,1678,Efstratios,Gavves,uva,University of Amsterdam,egavves@uva.nl,Combinatorial Bayesian Optimization using the Graph Cartesian Product
neurips,2019,3,1678,Max,Welling,uva,University of Amsterdam / Qualcomm AI Research,m.welling@uva.nl,Combinatorial Bayesian Optimization using the Graph Cartesian Product
neurips,2019,0,4863,Michael,Zhu,stanford,Stanford University,mhzhu@cs.stanford.edu,Sample Adaptive MCMC
neurips,2019,0,6654,Tam,Le,riken,RIKEN AIP,tam.le@riken.jp,Tree-Sliced Variants of Wasserstein Distances
neurips,2019,1,6654,Makoto,Yamada,ism,Kyoto University / RIKEN AIP,fukumizu@ism.ac.jp,Tree-Sliced Variants of Wasserstein Distances
neurips,2019,2,6654,Kenji,Fukumizu,riken,Institute of Statistical Mathematics / Preferred Networks / RIKEN AIP,makoto.yamada@riken.jp,Tree-Sliced Variants of Wasserstein Distances
neurips,2019,3,6654,Marco,Cuturi,google,Google Brain  &  CREST - ENSAE,cuturi@google.com,Tree-Sliced Variants of Wasserstein Distances
neurips,2019,0,7995,Robert,Ness,gamalon,Gamalon,robert.ness@gamalon.com,Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems
neurips,2019,1,7995,Kaushal,Paneri,gmail,Microsoft,kaushalpaneri@gmail.com,Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems
neurips,2019,2,7995,Olga,Vitek,northeastern,Northeastern University,o.vitek@northeastern.edu,Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems
neurips,2019,0,2979,Wei,Deng,purdue,Purdue University,deng106@purdue.edu,An Adaptive Empirical  Bayesian Method for Sparse Deep Learning
neurips,2019,1,2979,Xiao,Zhang,purdue,Purdue University,zhang923@purdue.edu,An Adaptive Empirical  Bayesian Method for Sparse Deep Learning
neurips,2019,2,2979,Faming,Liang,purdue,Purdue University,fmliang@purdue.edu,An Adaptive Empirical  Bayesian Method for Sparse Deep Learning
neurips,2019,3,2979,Guang,Lin,purdue,Purdue University,guanglin@purdue.edu,An Adaptive Empirical  Bayesian Method for Sparse Deep Learning
neurips,2019,0,3030,Xiaoling,Hu,,Stony Brook University,,Topology-Preserving Deep Image Segmentation
neurips,2019,1,3030,Fuxin,Li,,Oregon State University,,Topology-Preserving Deep Image Segmentation
neurips,2019,2,3030,Dimitris,Samaras,,Stony Brook University,,Topology-Preserving Deep Image Segmentation
neurips,2019,3,3030,Chao,Chen,,Stony Brook University,,Topology-Preserving Deep Image Segmentation
neurips,2019,0,8996,Adam,Kosiorek,,University of Oxford,,Stacked Capsule Autoencoders
neurips,2019,1,8996,Sara,Sabour,,Google,,Stacked Capsule Autoencoders
neurips,2019,2,8996,Yee Whye,Teh,,"University of Oxford, DeepMind",,Stacked Capsule Autoencoders
neurips,2019,3,8996,Geoffrey,Hinton,,Google & University of Toronto,,Stacked Capsule Autoencoders
neurips,2019,0,3378,Dan,Zhang,bosch,Bosch Center for Artificial Intelligence,dan.zhang2@bosch.com,Progressive Augmentation of GANs
neurips,2019,1,3378,Anna,Khoreva,bosch,Bosch Center for Artificial Intelligence,anna.khoreva@bosch.com,Progressive Augmentation of GANs
neurips,2019,0,741,Holden,Lee,,Princeton University,,Online sampling from log-concave distributions
neurips,2019,1,741,Oren,Mangoubi,,Worcester Polytechnic Institute,,Online sampling from log-concave distributions
neurips,2019,2,741,Nisheeth,Vishnoi,,Yale University,,Online sampling from log-concave distributions
neurips,2019,0,5204,Jian,Wu,gmail,Cornell University,wujian046@gmail.com,Practical Two-Step Lookahead Bayesian Optimization
neurips,2019,1,5204,Peter,Frazier,cornell,Cornell / Uber,pf98@cornell.edu,Practical Two-Step Lookahead Bayesian Optimization
neurips,2019,0,3136,Zhiyong,Yang,iie,"SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences; SCS, University of Chinese Academy of Sciences",yangzhiyong@iie.ac.cn,Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer
neurips,2019,1,3136,Qianqian,Xu,ict,"Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences",xuqianqian@ict.ac.cn,Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer
neurips,2019,2,3136,Yangbangyan,Jiang,iie,"Institute of Information Engineering, Chinese Academy of Sciences",jiangyangbangyan@iie.ac.cn,Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer
neurips,2019,3,3136,Xiaochun,Cao,iie,"Institute of Information Engineering, Chinese Academy of Sciences",caoxiaochun@iie.ac.cn,Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer
neurips,2019,4,3136,Qingming,Huang,ucas,University of Chinese Academy of Sciences,qmhuang@ucas.ac.cn,Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer
neurips,2019,0,4824,Young Hun,Jung,umich,University of Michigan,yhjung@umich.edu,Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems
neurips,2019,1,4824,Ambuj,Tewari,umich,University of Michigan,tewaria@umich.edu,Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems
neurips,2019,0,2882,Marko,Mitrovic,yale,Yale University,marko.mitrovic@yale.edu,Adaptive Sequence Submodularity
neurips,2019,1,2882,Ehsan,Kazemi,yale,Yale,ehsan.kazemi@yale.edu,Adaptive Sequence Submodularity
neurips,2019,2,2882,Moran,Feldman,openu,Open University of Israel,moranfe@openu.ac.il,Adaptive Sequence Submodularity
neurips,2019,3,2882,Andreas,Krause,ethz,ETH Zurich,krausea@ethz.ch,Adaptive Sequence Submodularity
neurips,2019,4,2882,Amin,Karbasi,yale,Yale,amin.karbasi@yale.edu,Adaptive Sequence Submodularity
neurips,2019,0,4579,Shengchao,Liu,wisc,UW-Madison,shengchao@cs.wisc.edu,"N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules"
neurips,2019,1,4579,Mehmet,Demirel,wisc,University of Wisconsin-Madison,demirel@cs.wisc.edu,"N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules"
neurips,2019,2,4579,Yingyu,Liang,wisc,University of Wisconsin Madison,yliang@cs.wisc.edu,"N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules"
neurips,2019,0,4541,Benjamin,Aubin,,Ipht Saclay,,The spiked matrix model with generative priors
neurips,2019,1,4541,Bruno,Loureiro,,IPhT Saclay,,The spiked matrix model with generative priors
neurips,2019,2,4541,Antoine,Maillard,,Ecole Normale Supérieure,,The spiked matrix model with generative priors
neurips,2019,3,4541,Florent,Krzakala,,ENS Paris & Sorbonnes Université,,The spiked matrix model with generative priors
neurips,2019,4,4541,Lenka,Zdeborová,,CEA Saclay,,The spiked matrix model with generative priors
neurips,2019,0,8546,Rong,Ge,duke,Duke University,rongge@cs.duke.edu,"The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares"
neurips,2019,1,8546,Sham,Kakade,washington,University of Washington,sham@cs.washington.edu,"The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares"
neurips,2019,2,8546,Rahul,Kidambi,cornell,Cornell University,rkidambi@cornell.edu,"The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares"
neurips,2019,3,8546,Praneeth,Netrapalli,microsoft,Microsoft Research,praneeth@microsoft.com,"The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares"
neurips,2019,0,2446,Jingjing,Xu,pku,Peking University,jingjingxu@pku.edu.cn,Understanding and Improving Layer Normalization
neurips,2019,1,2446,Xu,Sun,pku,Peking University,xusun@pku.edu.cn,Understanding and Improving Layer Normalization
neurips,2019,2,2446,Zhiyuan,Zhang,pku,Peking University,zzy1210@pku.edu.cn,Understanding and Improving Layer Normalization
neurips,2019,3,2446,Guangxiang,Zhao,pku,Peking University,zhaoguangxiang@pku.edu.cn,Understanding and Improving Layer Normalization
neurips,2019,4,2446,Junyang,Lin,pku,Alibaba Group,linjunyang@pku.edu.cn,Understanding and Improving Layer Normalization
neurips,2019,0,6392,Yang,Song,stanford,Stanford University,yangsong@cs.stanford.edu,Generative Modeling by Estimating Gradients of the Data Distribution
neurips,2019,1,6392,Stefano,Ermon,stanford,Stanford,ermon@cs.stanford.edu,Generative Modeling by Estimating Gradients of the Data Distribution
neurips,2019,0,3642,Dylan,Foster,mit,MIT,dylanf@mit.edu,Hypothesis Set Stability and Generalization
neurips,2019,1,3642,Spencer,Greenberg,sparkwave,Spark Wave,admin@sparkwave.tech,Hypothesis Set Stability and Generalization
neurips,2019,2,3642,Satyen,Kale,satyenkale,Google,satyen@satyenkale.com,Hypothesis Set Stability and Generalization
neurips,2019,3,3642,Haipeng,Luo,usc,University of Southern California,haipengl@usc.edu,Hypothesis Set Stability and Generalization
neurips,2019,4,3642,Mehryar,Mohri,google,Courant Inst. of Math. Sciences & Google Research,mohri@google.com,Hypothesis Set Stability and Generalization
neurips,2019,5,3642,Karthik,Sridharan,cornell,Cornell University,sridharan@cs.cornell.edu,Hypothesis Set Stability and Generalization
neurips,2019,0,2854,Nixie,Lesmana,ntu,Nanyang Technological University,nixiesap001@e.ntu.edu.sg,Balancing Efficiency and Fairness in On-Demand Ridesourcing
neurips,2019,1,2854,Xuan,Zhang,illinois,University of Illinois at Urbana-Champaign,xuan6@illinois.edu,Balancing Efficiency and Fairness in On-Demand Ridesourcing
neurips,2019,2,2854,Xiaohui,Bei,ntu,Nanyang Technological University,xhbei@ntu.edu.sg,Balancing Efficiency and Fairness in On-Demand Ridesourcing
neurips,2019,0,1419,Ayan,Chakrabarti,wustl,Washington University in St. Louis,ayan@wustl.edu,Backprop with Approximate Activations for Memory-efficient Network Training
neurips,2019,1,1419,Benjamin,Moseley,cmu,Carnegie Mellon University,moseleyb@andrew.cmu.edu,Backprop with Approximate Activations for Memory-efficient Network Training
neurips,2019,0,4644,Alon,Cohen,,Google,,Learning to Screen
neurips,2019,1,4644,Avinatan,Hassidim,,Google,,Learning to Screen
neurips,2019,2,4644,Haim,Kaplan,,"TAU, GOOGLE",,Learning to Screen
neurips,2019,3,4644,Yishay,Mansour,,Tel Aviv University / Google,,Learning to Screen
neurips,2019,4,4644,Shay,Moran,,Google AI Princeton,,Learning to Screen
neurips,2019,0,4966,Rohan,Gala,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,1,4966,Nathan,Gouwens,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,2,4966,Zizhen,Yao,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,3,4966,Agata,Budzillo,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,4,4966,Osnat,Penn,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,5,4966,Bosiljka,Tasic,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,6,4966,Gabe,Murphy,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,7,4966,Hongkui,Zeng,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,8,4966,Uygar,Sümbül,,Allen Institute,,A coupled autoencoder approach for multi-modal analysis of cell types
neurips,2019,0,6278,Lantao,Yu,stanford,Stanford University,lantaoyu@cs.stanford.edu,Meta-Inverse Reinforcement Learning with Probabilistic Context Variables
neurips,2019,1,6278,Tianhe,Yu,stanford,Stanford University,tianheyu@cs.stanford.edu,Meta-Inverse Reinforcement Learning with Probabilistic Context Variables
neurips,2019,2,6278,Chelsea,Finn,stanford,Stanford University,cbfinn@cs.stanford.edu,Meta-Inverse Reinforcement Learning with Probabilistic Context Variables
neurips,2019,3,6278,Stefano,Ermon,stanford,Stanford,ermon@cs.stanford.edu,Meta-Inverse Reinforcement Learning with Probabilistic Context Variables
neurips,2019,0,3655,Seppo,Virtanen,cam,University of Cambridge,sjv35@cam.ac.uk,Precision-Recall Balanced Topic Modelling
neurips,2019,1,3655,Mark,Girolami,cam,Imperial College London,mag92@cam.ac.uk,Precision-Recall Balanced Topic Modelling
neurips,2019,0,2025,Kevin,Bello,purdue,Purdue University,kbellome@purdue.edu,Exact inference in structured prediction
neurips,2019,1,2025,Jean,Honorio,purdue,Purdue University,jhonorio@purdue.edu,Exact inference in structured prediction
neurips,2019,0,2244,Paul,Rubenstein,mpg,MPI for IS,paul.rubenstein@tuebingen.mpg.de,Practical and Consistent Estimation of f-Divergences
neurips,2019,1,2244,Olivier,Bousquet,google,Google Brain (Zurich),obousquet@google.com,Practical and Consistent Estimation of f-Divergences
neurips,2019,2,2244,Josip,Djolonga,google,"Google Research, Brain Team",josipd@google.com,Practical and Consistent Estimation of f-Divergences
neurips,2019,3,2244,Carlos,Riquelme,google,Google Brain,rikel@google.com,Practical and Consistent Estimation of f-Divergences
neurips,2019,4,2244,Ilya,Tolstikhin,google,MPI for Intelligent Systems,tolstikhin@google.com,Practical and Consistent Estimation of f-Divergences
neurips,2019,0,8247,Yuzhe,Ma,wisc,University of Wisconsin-Madison,yzm234@cs.wisc.edu,Policy Poisoning in Batch Reinforcement Learning and Control
neurips,2019,1,8247,Xuezhou,Zhang,wisc,UW-Madison,zhangxz1123@cs.wisc.edu,Policy Poisoning in Batch Reinforcement Learning and Control
neurips,2019,2,8247,Wen,Sun,microsoft,Microsoft Research NYC,Sun.Wen@microsoft.com,Policy Poisoning in Batch Reinforcement Learning and Control
neurips,2019,3,8247,Jerry,Zhu,wisc,University of Wisconsin-Madison,jerryzhu@cs.wisc.edu,Policy Poisoning in Batch Reinforcement Learning and Control
neurips,2019,0,6717,Jerome,Revaud,,Naver Labs Europe,,R2D2: Reliable and Repeatable Detector and Descriptor
neurips,2019,1,6717,Cesar,De Souza,,NAVER LABS Europe,,R2D2: Reliable and Repeatable Detector and Descriptor
neurips,2019,2,6717,Martin,Humenberger,,Naver Labs Europe,,R2D2: Reliable and Repeatable Detector and Descriptor
neurips,2019,3,6717,Philippe,Weinzaepfel,,NAVER LABS Europe,,R2D2: Reliable and Repeatable Detector and Descriptor
neurips,2019,0,3854,Aliaksandr,Siarohin,unitn,University of Trento,aliaksandr.siarohin@unitn.it,First Order Motion Model for Image Animation
neurips,2019,1,3854,Stéphane,Lathuilière,telecom-paris,University of Trento,stephane.lathuilire@telecom-paris.fr,First Order Motion Model for Image Animation
neurips,2019,2,3854,Sergey,Tulyakov,snap,Snap Inc,stulyakov@snap.com,First Order Motion Model for Image Animation
neurips,2019,3,3854,Elisa,Ricci,unitn,FBK - Technologies of Vision,e.ricci@unitn.it,First Order Motion Model for Image Animation
neurips,2019,4,3854,Nicu,Sebe,unitn,University of Trento,niculae.sebe@unitn.it,First Order Motion Model for Image Animation
neurips,2019,0,3187,Mikhail,Yurochkin,ibm,"IBM Research, MIT-IBM Watson AI Lab",mikhail.yurochkin@ibm.com,Scalable inference of topic evolution via models for latent geometric structures
neurips,2019,1,3187,Zhiwei,Fan,wisc,University of Wisconsin-Madison,zhiwei@cs.wisc.edu,Scalable inference of topic evolution via models for latent geometric structures
neurips,2019,2,3187,Aritra,Guha,umich,University of Michigan,aritra@umich.edu,Scalable inference of topic evolution via models for latent geometric structures
neurips,2019,3,3187,Paraschos,Koutris,wisc,University of Wisconsin-Madison,paris@cs.wisc.edu,Scalable inference of topic evolution via models for latent geometric structures
neurips,2019,4,3187,XuanLong,Nguyen,umich,University of Michigan,xuanlong@umich.edu,Scalable inference of topic evolution via models for latent geometric structures
neurips,2019,0,3386,Rahma,Chaabouni,fb,FAIR/ENS,rchaabouni@fb.com,Anti-efficient encoding in emergent communication
neurips,2019,1,3386,Eugene,Kharitonov,fb,Facebook AI,kharitonov@fb.com,Anti-efficient encoding in emergent communication
neurips,2019,2,3386,Emmanuel,Dupoux,fb,Ecole des Hautes Etudes en Sciences Sociales,dpx@fb.com,Anti-efficient encoding in emergent communication
neurips,2019,3,3386,Marco,Baroni,fb,University of Trento,mbaroni@fb.com,Anti-efficient encoding in emergent communication
neurips,2019,0,5860,Shuyu,Cheng,tsinghua,Tsinghua University,chengsy18@mails.tsinghua.edu.cn,Improving Black-box Adversarial Attacks with a Transfer-based Prior
neurips,2019,1,5860,Yinpeng,Dong,tsinghua,Tsinghua University,dyp17@mails.tsinghua.edu.cn,Improving Black-box Adversarial Attacks with a Transfer-based Prior
neurips,2019,2,5860,Tianyu,Pang,tsinghua,Tsinghua University,pty17@mails.tsinghua.edu.cn,Improving Black-box Adversarial Attacks with a Transfer-based Prior
neurips,2019,3,5860,Hang,Su,tsinghua,Tsinghua Univiersity,suhangss@mail.tsinghua.edu.cn,Improving Black-box Adversarial Attacks with a Transfer-based Prior
neurips,2019,4,5860,Jun,Zhu,tsinghua,Tsinghua University,dcszj@mail.tsinghua.edu.cn,Improving Black-box Adversarial Attacks with a Transfer-based Prior
neurips,2019,0,7091,Yiwei,Liu,bit,Beijing institute of technology,yiweiliu@bit.edu.cn,REM: From Structural Entropy to Community Structure Deception
neurips,2019,1,7091,Jiamou,Liu,bit,University of Auckland,zhangzijian@bit.edu.cn,REM: From Structural Entropy to Community Structure Deception
neurips,2019,2,7091,Zijian,Zhang,bit,Beijing Institute of Technology,liehuangz@bit.edu.cn,REM: From Structural Entropy to Community Structure Deception
neurips,2019,3,7091,Liehuang,Zhu,auckland,Beijing Institute of Technology,jiamou.liu@auckland.ac.nz,REM: From Structural Entropy to Community Structure Deception
neurips,2019,4,7091,Angsheng,Li,buaa,Beihang University,angsheng@buaa.edu.cn,REM: From Structural Entropy to Community Structure Deception
neurips,2019,0,6913,Mickaël,Chen,lip6,Sorbonne Université,mickael.chen@lip6.fr,Unsupervised Object Segmentation by Redrawing
neurips,2019,1,6913,Thierry,Artières,centrale-marseille,Aix-Marseille Université,thierry.artieres@centrale-marseille.fr,Unsupervised Object Segmentation by Redrawing
neurips,2019,2,6913,Ludovic,Denoyer,fb,Facebook - FAIR,denoyer@fb.com,Unsupervised Object Segmentation by Redrawing
neurips,2019,0,5993,Yair,Carmon,stanford,Stanford University,yairc@stanford.edu,Unlabeled Data Improves Adversarial Robustness
neurips,2019,1,5993,Aditi,Raghunathan,stanford,Stanford University,aditir@stanford.edu,Unlabeled Data Improves Adversarial Robustness
neurips,2019,2,5993,Ludwig,Schmidt,berkeley,UC Berkeley,ludwig@berkeley.edu,Unlabeled Data Improves Adversarial Robustness
neurips,2019,3,5993,John,Duchi,stanford,Stanford,pliang@cs.stanford.edu,Unlabeled Data Improves Adversarial Robustness
neurips,2019,4,5993,Percy,Liang,stanford,Stanford University,jduchi@stanford.edu,Unlabeled Data Improves Adversarial Robustness
neurips,2019,0,2899,Yunwen,Lei,sustech,Technical University of Kaiserslautern,leiyw@sustech.edu.cn,Optimal Stochastic and Online Learning with Individual Iterates
neurips,2019,1,2899,Peng,Yang,sustech,Southern University of Science and Technology,yangp@sustech.edu.cn,Optimal Stochastic and Online Learning with Individual Iterates
neurips,2019,2,2899,Ke,Tang,sustech,Southern University of Science and Technology,tangk3@sustech.edu.cn,Optimal Stochastic and Online Learning with Individual Iterates
neurips,2019,3,2899,Ding-Xuan,Zhou,cityu,City University of Hong Kong,mazhou@cityu.edu.hk,Optimal Stochastic and Online Learning with Individual Iterates
neurips,2019,0,4204,Qian,Qian,osu,Ohio State University,qian.216@osu.edu,The Implicit Bias of AdaGrad on Separable Data
neurips,2019,1,4204,Xiaoyuan,Qian,dlut,Dalian University of Technology,xyqian@dlut.edu.cn,The Implicit Bias of AdaGrad on Separable Data
neurips,2019,0,2161,Qianqian,Xu,ict,"Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences",xuqianqian@ict.ac.cn,iSplit LBI: Individualized Partial Ranking with Ties via Split LBI
neurips,2019,1,2161,Xinwei,Sun,microsoft,MSRA,xinsun@microsoft.com,iSplit LBI: Individualized Partial Ranking with Ties via Split LBI
neurips,2019,2,2161,Zhiyong,Yang,iie,"SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences; SCS, University of Chinese Academy of Sciences",yangzhiyong@iie.ac.cn,iSplit LBI: Individualized Partial Ranking with Ties via Split LBI
neurips,2019,3,2161,Xiaochun,Cao,iie,"Institute of Information Engineering, Chinese Academy of Sciences",caoxiaochun@iie.ac.cn,iSplit LBI: Individualized Partial Ranking with Ties via Split LBI
neurips,2019,4,2161,Qingming,Huang,ucas,University of Chinese Academy of Sciences,qmhuang@ucas.ac.cn,iSplit LBI: Individualized Partial Ranking with Ties via Split LBI
neurips,2019,5,2161,Yuan,Yao,ust,Hong Kong Univ. of Science & Technology,yuany@ust.hk,iSplit LBI: Individualized Partial Ranking with Ties via Split LBI
neurips,2019,0,3907,Can,Qin,neu,Northeastern University,qin.ca@husky.neu.edu,PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation
neurips,2019,1,3907,Haoxuan,You,columbia,Columbia University,haoxuan.you@columbia.edu,PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation
neurips,2019,2,3907,Lichen,Wang,gmail,Northeastern University,wanglichenxj@gmail.com,PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation
neurips,2019,3,3907,C.-C. Jay,Kuo,usc,University of Southern California,cckuo@sipi.usc.edu,PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation
neurips,2019,4,3907,Yun,Fu,neu,Northeastern University,yunfu@ece.neu.edu,PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation
neurips,2019,0,5038,Bai,Li,duke,Duke University,bai.li@duke.edu,Certified Adversarial Robustness with Additive Noise
neurips,2019,1,5038,Changyou,Chen,gmail,University at Buffalo,cchangyou@gmail.com,Certified Adversarial Robustness with Additive Noise
neurips,2019,2,5038,Wenlin,Wang,duke,Duke University,wenlin.wang@duke.edu,Certified Adversarial Robustness with Additive Noise
neurips,2019,3,5038,Lawrence,Carin,duke,Duke University,lcarin@duke.edu,Certified Adversarial Robustness with Additive Noise
neurips,2019,0,4639,Jialin,Wu,utexas,UT Austin,jialinwu@utexas.edu,Self-Critical Reasoning for Robust Visual Question Answering
neurips,2019,1,4639,Raymond,Mooney,utexas,University of Texas at Austin,mooney@cs.utexas.edu,Self-Critical Reasoning for Robust Visual Question Answering
neurips,2019,0,535,Arsenii,Vanunts,yandex,Yandex,avanunts@yandex.ru,Optimal Pricing in Repeated Posted-Price Auctions with Different Patience of the Seller and the Buyer
neurips,2019,1,535,Alexey,Drutsa,yandex,Yandex,adrutsa@yandex.ru,Optimal Pricing in Repeated Posted-Price Auctions with Different Patience of the Seller and the Buyer
neurips,2019,0,41,Prajit,Ramachandran,,Google Brain,,Stand-Alone Self-Attention in Vision Models
neurips,2019,1,41,Niki,Parmar,,Google,,Stand-Alone Self-Attention in Vision Models
neurips,2019,2,41,Ashish,Vaswani,,Google Brain,,Stand-Alone Self-Attention in Vision Models
neurips,2019,3,41,Irwan,Bello,,Google Brain,,Stand-Alone Self-Attention in Vision Models
neurips,2019,4,41,Anselm,Levskaya,,Google,,Stand-Alone Self-Attention in Vision Models
neurips,2019,5,41,Jon,Shlens,,Google Research,,Stand-Alone Self-Attention in Vision Models
neurips,2019,0,6430,Kolyan,Ray,kcl,King's College London,kolyan.ray@kcl.ac.uk,Debiased Bayesian inference for average treatment effects
neurips,2019,1,6430,Botond,Szabo,leidenuniv,Leiden University,b.t.szabo@math.leidenuniv.nl,Debiased Bayesian inference for average treatment effects
neurips,2019,0,2489,Bryon,Aragam,chicagobooth,University of Chicago,bryon@chicagobooth.edu,Globally optimal score-based learning of directed acyclic graphs in high-dimensions
neurips,2019,1,2489,Arash,Amini,ucla,UCLA,aaamini@stat.ucla.edu,Globally optimal score-based learning of directed acyclic graphs in high-dimensions
neurips,2019,2,2489,Qing,Zhou,ucla,UCLA,zhou@stat.ucla.edu,Globally optimal score-based learning of directed acyclic graphs in high-dimensions
neurips,2019,0,3786,Yuan,Liu,,Zhejiang University,,GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs
neurips,2019,1,3786,Zehong,Shen,,Zhejiang University,,GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs
neurips,2019,2,3786,Zhixuan,Lin,,Zhejiang University,,GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs
neurips,2019,3,3786,Sida,Peng,,Zhejiang University,,GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs
neurips,2019,4,3786,Hujun,Bao,,Zhejiang University,,GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs
neurips,2019,5,3786,Xiaowei,Zhou,,"Zhejiang University, China",,GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs
neurips,2019,0,7140,Ruiqi,Gao,,Peking University,,Convergence of Adversarial Training in Overparametrized Neural Networks
neurips,2019,1,7140,Tianle,Cai,,Peking University,,Convergence of Adversarial Training in Overparametrized Neural Networks
neurips,2019,2,7140,Haochuan,Li,,MIT,,Convergence of Adversarial Training in Overparametrized Neural Networks
neurips,2019,3,7140,Cho-Jui,Hsieh,,UCLA,,Convergence of Adversarial Training in Overparametrized Neural Networks
neurips,2019,4,7140,Liwei,Wang,,Peking University,,Convergence of Adversarial Training in Overparametrized Neural Networks
neurips,2019,5,7140,Jason,Lee,,Princeton University,,Convergence of Adversarial Training in Overparametrized Neural Networks
neurips,2019,0,591,Nicki,Skafte,dtu,Technical University of Denmark,nsde@dtu.dk,Explicit Disentanglement of Appearance and Perspective in Generative Models
neurips,2019,1,591,Søren,Hauberg,dtu,Technical University of Denmark,sohau@dtu.dk,Explicit Disentanglement of Appearance and Perspective in Generative Models
neurips,2019,0,7113,James,Bailey,tamu,Texas A&M University,jamespbailey@tamu.edu,Fast and Furious Learning in Zero-Sum Games: Vanishing Regret with Non-Vanishing Step Sizes
neurips,2019,1,7113,Georgios,Piliouras,sutd,Singapore University of Technology and Design,georgios@sutd.edu.sg,Fast and Furious Learning in Zero-Sum Games: Vanishing Regret with Non-Vanishing Step Sizes
neurips,2019,0,5007,Vincent,Chen,stanford,Stanford University,vincentsc@cs.stanford.edu,Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices
neurips,2019,1,5007,Sen,Wu,stanford,Stanford University,senwu@stanford.edu,Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices
neurips,2019,2,5007,Alexander,Ratner,stanford,Stanford,zzweng@stanford.edu,Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices
neurips,2019,3,5007,Jen,Weng,stanford,Stanford University,ajratner@stanford.edu,Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices
neurips,2019,4,5007,Christopher,Ré,stanford,Stanford,chrismre@cs.stanford.edu,Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices
neurips,2019,0,5520,Ilias,Diakonikolas,wisc,UW Madison,ilias@cs.wisc.edu,Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin
neurips,2019,1,5520,Daniel,Kane,ucsd,UCSD,dakane@cs.ucsd.edu,Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin
neurips,2019,2,5520,Pasin,Manurangsi,berkeley,Google,pasin@berkeley.edu,Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin
neurips,2019,0,2659,Ilias,Diakonikolas,wisc,UW Madison,ilias@cs.wisc.edu,Distribution-Independent PAC Learning of Halfspaces with Massart Noise
neurips,2019,1,2659,Themis,Gouleakis,mpg,Max Planck Institute for Informatics,tgouleak@mpi-inf.mpg.de,Distribution-Independent PAC Learning of Halfspaces with Massart Noise
neurips,2019,2,2659,Christos,Tzamos,wisc,UW Madison,tzamos@wisc.edu,Distribution-Independent PAC Learning of Halfspaces with Massart Noise
neurips,2019,0,2732,Ruqi,Zhang,cornell,Cornell University,rz297@cornell.edu,Poisson-Minibatching for Gibbs Sampling with Convergence Rate Guarantees
neurips,2019,1,2732,Christopher,De Sa,cornell,Cornell,cdesa@cs.cornell.edu,Poisson-Minibatching for Gibbs Sampling with Convergence Rate Guarantees
neurips,2019,0,1386,Virag,Shah,stanford,Stanford University,virag@stanford.edu,Semi-Parametric Dynamic Contextual Pricing
neurips,2019,1,1386,Ramesh,Johari,stanford,Stanford University,jblanche@stanford.edu,Semi-Parametric Dynamic Contextual Pricing
neurips,2019,2,1386,Jose,Blanchet,stanford,Stanford University,rjohari@stanford.edu,Semi-Parametric Dynamic Contextual Pricing
neurips,2019,0,6368,Rafael,Pinot,,Dauphine University - CEA LIST Institute,,Theoretical evidence for adversarial robustness through randomization
neurips,2019,1,6368,Laurent,Meunier,,Dauphine University - FAIR Paris,,Theoretical evidence for adversarial robustness through randomization
neurips,2019,2,6368,Alexandre,Araujo,,Université Paris-Dauphine,,Theoretical evidence for adversarial robustness through randomization
neurips,2019,3,6368,Hisashi,Kashima,,Kyoto University/RIKEN Center for AIP,,Theoretical evidence for adversarial robustness through randomization
neurips,2019,4,6368,Florian,Yger,,Université Paris-Dauphine,,Theoretical evidence for adversarial robustness through randomization
neurips,2019,5,6368,Cedric,Gouy-Pailler,,CEA,,Theoretical evidence for adversarial robustness through randomization
neurips,2019,6,6368,Jamal,Atif,,Université Paris-Dauphine,,Theoretical evidence for adversarial robustness through randomization
neurips,2019,0,7763,Sunil,Thulasidasan,,Los Alamos National Laboratory & University of Washington,,On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks
neurips,2019,1,7763,Gopinath,Chennupati,,Los Alamos National Laboratory,,On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks
neurips,2019,2,7763,Jeff,Bilmes,,"University of Washington, Seattle",,On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks
neurips,2019,3,7763,Tanmoy,Bhattacharya,,Los Alamos National Laboratory,,On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks
neurips,2019,4,7763,Sarah,Michalak,,Los Alamos National Laboratory,,On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks
neurips,2019,0,1775,Min-hwan,Oh,columbia,Columbia University,m.oh@columbia.edu,Thompson Sampling for Multinomial Logit Contextual Bandits
neurips,2019,1,1775,Garud,Iyengar,columbia,Columbia,garud@ieor.columbia.edu,Thompson Sampling for Multinomial Logit Contextual Bandits
neurips,2019,0,2585,Hugo,Caselles-Dupré,ensta,Flowers Laboratory (ENSTA ParisTech & INRIA) & Softbank Robotics Europe,caselles@ensta.fr,Symmetry-Based Disentangled Representation Learning requires Interaction with Environments
neurips,2019,1,2585,Michael,Garcia Ortiz,softbankrobotics,SoftBank Robotics Europe,mgarciaortiz@softbankrobotics.com,Symmetry-Based Disentangled Representation Learning requires Interaction with Environments
neurips,2019,2,2585,David,Filliat,ensta,ENSTA,david.lliat@ensta.fr,Symmetry-Based Disentangled Representation Learning requires Interaction with Environments
neurips,2019,0,3320,Sangwoo,Mo,kaist,KAIST,swmo@kaist.ac.kr,Mining GOLD Samples for Conditional GANs
neurips,2019,1,3320,Chiheon,Kim,kakaobrain,Kakao Brain,chiheon.kim@kakaobrain.com,Mining GOLD Samples for Conditional GANs
neurips,2019,2,3320,Sungwoong,Kim,kakaobrain,Kakao Brain,swkim@kakaobrain.com,Mining GOLD Samples for Conditional GANs
neurips,2019,3,3320,Minsu,Cho,postech,POSTECH,mscho@postech.ac.kr,Mining GOLD Samples for Conditional GANs
neurips,2019,4,3320,Jinwoo,Shin,kaist,KAIST; AITRICS,jinwoos@kaist.ac.kr,Mining GOLD Samples for Conditional GANs
neurips,2019,0,2765,Ting-Chun,Wang,nvidia,NVIDIA,tingchunw@nvidia.com,Few-shot Video-to-Video Synthesis
neurips,2019,1,2765,Ming-Yu,Liu,nvidia,Nvidia Research,mingyul@nvidia.com,Few-shot Video-to-Video Synthesis
neurips,2019,2,2765,Andrew,Tao,nvidia,Nvidia Corporation,atao@nvidia.com,Few-shot Video-to-Video Synthesis
neurips,2019,3,2765,Guilin,Liu,nvidia,NVIDIA,guilinl@nvidia.com,Few-shot Video-to-Video Synthesis
neurips,2019,4,2765,Bryan,Catanzaro,nvidia,NVIDIA,jkautz@nvidia.com,Few-shot Video-to-Video Synthesis
neurips,2019,5,2765,Jan,Kautz,nvidia,NVIDIA,bcatanzaro@nvidia.com,Few-shot Video-to-Video Synthesis
neurips,2019,0,4734,Michael,Wick,oracle,Oracle Labs,michael.wick@oracle.com,Unlocking Fairness: a Trade-off Revisited
neurips,2019,1,4734,swetasudha,panda,oracle,Oracle Labs,swetasudha.panda@oracle.com,Unlocking Fairness: a Trade-off Revisited
neurips,2019,2,4734,Jean-Baptiste,Tristan,oracle,Oracle Labs,jean.baptiste.tristan@oracle.com,Unlocking Fairness: a Trade-off Revisited
neurips,2019,0,23,Liwei,Wu,ucdavis,"University of California, Davis",liwu@ucdavis.edu,Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers
neurips,2019,1,23,Shuqing,Li,ucdavis,"University of California, Davis",qshli@ucdavis.edu,Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers
neurips,2019,2,23,Cho-Jui,Hsieh,ucla,UCLA,chohsieh@cs.ucla.edu,Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers
neurips,2019,3,23,James,Sharpnack,ucdavis,UC Davis,jsharpna@ucdavis.edu,Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers
neurips,2019,0,7579,Joshua,Allen,,Microsoft,,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors
neurips,2019,1,7579,Bolin,Ding,,Alibaba Group,,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors
neurips,2019,2,7579,Janardhan,Kulkarni,,"MSR, Redmond",,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors
neurips,2019,3,7579,Harsha,Nori,,Microsoft,,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors
neurips,2019,4,7579,Olga,Ohrimenko,,Microsoft,,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors
neurips,2019,5,7579,Sergey,Yekhanin,,Microsoft,,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors
neurips,2019,0,1955,Yilun,Du,,MIT,,Implicit Generation and Modeling with Energy Based Models
neurips,2019,1,1955,Igor,Mordatch,,OpenAI,,Implicit Generation and Modeling with Energy Based Models
neurips,2019,0,5125,Roshan,Rao,berkeley,UC Berkeley,roshan_rao@berkeley.edu,Evaluating Protein Transfer Learning with TAPE
neurips,2019,1,5125,Nicholas,Bhattacharya,berkeley,UC Berkeley,nick_bhat@berkeley.edu,Evaluating Protein Transfer Learning with TAPE
neurips,2019,2,5125,Neil,Thomas,berkeley,UC Berkeley,nthomas@berkeley.edu,Evaluating Protein Transfer Learning with TAPE
neurips,2019,3,5125,Yan,Duan,covariant,COVARIANT.AI,rocky@covariant.ai,Evaluating Protein Transfer Learning with TAPE
neurips,2019,4,5125,Peter,Chen,covariant,COVARIANT.AI,peter@covariant.ai,Evaluating Protein Transfer Learning with TAPE
neurips,2019,5,5125,John,Canny,berkeley,UC Berkeley,canny@berkeley.edu,Evaluating Protein Transfer Learning with TAPE
neurips,2019,6,5125,Pieter,Abbeel,berkeley,UC Berkeley & covariant.ai,pabbeel@berkeley.edu,Evaluating Protein Transfer Learning with TAPE
neurips,2019,7,5125,Yun,Song,berkeley,UC Berkeley,yss@berkeley.edu,Evaluating Protein Transfer Learning with TAPE
neurips,2019,0,6993,Andrei,Nicolicioiu,bitdefender,Bitdefender,anicolicioiu@bitdefender.com,Recurrent Space-time Graph Neural Networks
neurips,2019,1,6993,Iulia,Duta,bitdefender,Bitdefender,iduta@bitdefender.com,Recurrent Space-time Graph Neural Networks
neurips,2019,2,6993,Marius,Leordeanu,imar,Institute of Mathematics of the Romanian Academy,marius.leordeanu@imar.ro,Recurrent Space-time Graph Neural Networks
neurips,2019,0,3398,Abraham,Traore,univ-rouen,University of Rouen,abraham.traore@etu.univ-rouen.fr,Singleshot : a scalable Tucker tensor decomposition
neurips,2019,1,3398,Maxime,Berar,univ-rouen,Université de Rouen,maxime.berar@univ-rouen.fr,Singleshot : a scalable Tucker tensor decomposition
neurips,2019,2,3398,Alain,Rakotomamonjy,insa-rouen,Université de Rouen Normandie   Criteo AI Lab,alain.rakoto@insa-rouen.fr,Singleshot : a scalable Tucker tensor decomposition
neurips,2019,0,622,Sepehr,Assadi,rutgers,Princeton University,sepehr.assadi@rutgers.edu,Secretary Ranking with Minimal Inversions
neurips,2019,1,622,Eric,Balkanski,gmail,Harvard University,ebalkans@gmail.com,Secretary Ranking with Minimal Inversions
neurips,2019,2,622,Renato,Leme,google,Google Research,renatoppl@google.com,Secretary Ranking with Minimal Inversions
neurips,2019,0,5415,Hao,Sun,,CUHK,,Policy Continuation with Hindsight Inverse Dynamics
neurips,2019,1,5415,Zhizhong,Li,,The Chinese University of Hong Kong,,Policy Continuation with Hindsight Inverse Dynamics
neurips,2019,2,5415,Xiaotong,Liu,,Peking Uinversity,,Policy Continuation with Hindsight Inverse Dynamics
neurips,2019,3,5415,Bolei,Zhou,,CUHK,,Policy Continuation with Hindsight Inverse Dynamics
neurips,2019,4,5415,Dahua,Lin,,The Chinese University of Hong Kong,,Policy Continuation with Hindsight Inverse Dynamics
neurips,2019,0,3809,Yaniv,Blumenfeld,gmail,Technion,yanivblm6@gmail.com,A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off
neurips,2019,1,3809,Dar,Gilboa,gmail,Columbia University,dargilboa@gmail.com,A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off
neurips,2019,2,3809,Daniel,Soudry,gmail,Technion,daniel.soudry@gmail.com,A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off
neurips,2019,0,6715,Cheng,Tang,amazon,Amazon,tcheng@amazon.com,Exponentially convergent stochastic k-PCA without variance reduction
neurips,2019,0,270,Qiangeng,Xu,usc,USC,weiyuewa@usc.edu,DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction
neurips,2019,1,270,Weiyue,Wang,usc,Waymo,qiangenx@usc.edu,DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction
neurips,2019,2,270,Duygu,Ceylan,usc,Adobe Research,uneumann@usc.edu,DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction
neurips,2019,3,270,Radomir,Mech,adobe,Adobe Systems Incorporated,ceylan@adobe.com,DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction
neurips,2019,4,270,Ulrich,Neumann,adobe,USC,rmech@adobe.com,DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction
neurips,2019,0,6131,Nima,Hamidi,,Stanford University,,Personalizing Many Decisions with High-Dimensional Covariates
neurips,2019,1,6131,Mohsen,Bayati,,Stanford University,,Personalizing Many Decisions with High-Dimensional Covariates
neurips,2019,2,6131,Kapil,Gupta,,Airbnb,,Personalizing Many Decisions with High-Dimensional Covariates
neurips,2019,0,7848,Joshua,Hanson,illinois,University of Illinois at Urbana-Champaign,jmh4@illinois.edu,Universal Approximation of Input-Output Maps by Temporal Convolutional Nets
neurips,2019,1,7848,Maxim,Raginsky,illinois,University of Illinois at Urbana-Champaign,maxim@illinois.edu,Universal Approximation of Input-Output Maps by Temporal Convolutional Nets
neurips,2019,0,3185,Kai,Zheng,pku,Peking University,zhengk92@pku.edu.cn,Equipping Experts/Bandits with Long-term Memory
neurips,2019,1,3185,Haipeng,Luo,usc,University of Southern California,haipengl@usc.edu,Equipping Experts/Bandits with Long-term Memory
neurips,2019,2,3185,Ilias,Diakonikolas,gmail,UW Madison,ilias.diakonikolas@gmail.com,Equipping Experts/Bandits with Long-term Memory
neurips,2019,3,3185,Liwei,Wang,pku,Peking University,wanglw@cis.pku.edu.cn,Equipping Experts/Bandits with Long-term Memory
neurips,2019,0,8540,Gregory,Benton,,New York University,,Function-Space Distributions over Kernels
neurips,2019,1,8540,Wesley,Maddox,,New York University,,Function-Space Distributions over Kernels
neurips,2019,2,8540,Jayson,Salkey,,New York University,,Function-Space Distributions over Kernels
neurips,2019,3,8540,Julio,Albinati,,Microsoft,,Function-Space Distributions over Kernels
neurips,2019,4,8540,Andrew Gordon,Wilson,,New York University,,Function-Space Distributions over Kernels
neurips,2019,0,1248,Takahiro,Omi,gmail,The University of Tokyo & RIKEN AIP,takahiro.omi.em@gmail.com,Fully Neural Network based Model for General Temporal Point Processes
neurips,2019,1,1248,naonori,ueda,ntt,RIKEN AIP,naonori.ueda.fr@hco.ntt.co.jp,Fully Neural Network based Model for General Temporal Point Processes
neurips,2019,2,1248,Kazuyuki,Aihara,u-tokyo,The University of Tokyo,aihara@sat.t.u-tokyo.ac.jp,Fully Neural Network based Model for General Temporal Point Processes
neurips,2019,0,5676,Lemeng,Wu,utexas,UT Austin,lqiang@cs.utexas.edu,Splitting Steepest Descent for Growing Neural Architectures
neurips,2019,1,5676,Dilin,Wang,utexas,UT Austin,lmwu@cs.utexas.edu,Splitting Steepest Descent for Growing Neural Architectures
neurips,2019,2,5676,Qiang,Liu,utexas,UT Austin,dilin@cs.utexas.edu,Splitting Steepest Descent for Growing Neural Architectures
neurips,2019,0,1222,Wenlin,Wang,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,1,1222,Chenyang,Tao,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,2,1222,Zhe,Gan,,Microsoft,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,3,1222,Guoyin,Wang,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,4,1222,Liqun,Chen,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,5,1222,Xinyuan,Zhang,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,6,1222,Ruiyi,Zhang,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,7,1222,Qian,Yang,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,8,1222,Ricardo,Henao,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,9,1222,Lawrence,Carin,,Duke University,,Improving Textual Network Learning with Variational Homophilic Embeddings
neurips,2019,0,661,Chengguang,Xu,neu,Northeastern University,xu.cheng@husky.neu.edu,Deep Supervised Summarization: Algorithm and Application to Learning Instructions
neurips,2019,1,661,Ehsan,Elhamifar,neu,Northeastern University,eelhami@ccs.neu.edu,Deep Supervised Summarization: Algorithm and Application to Learning Instructions
neurips,2019,0,2686,Rajat,Sen,,Amazon,,"Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting"
neurips,2019,1,2686,Hsiang-Fu,Yu,,Amazon,,"Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting"
neurips,2019,2,2686,Inderjit,Dhillon,,UT Austin & Amazon,,"Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting"
neurips,2019,0,2180,Quentin,Bertrand,inria,INRIA,quentin.bertrand@inria.fr,Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso
neurips,2019,1,2180,Mathurin,Massias,inria,Inria,mathurin.massias@inria.fr,Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso
neurips,2019,2,2180,Alexandre,Gramfort,inria,INRIA,alexandre.gramfort@inria.fr,Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso
neurips,2019,3,2180,Joseph,Salmon,umontpellier,Université de Montpellier,joseph.salmon@umontpellier.fr,Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso
neurips,2019,0,1560,Matthew,Holland,osaka-u,Osaka University,matthew-h@ar.sanken.osaka-u.ac.jp,PAC-Bayes under potentially heavy tails
neurips,2019,0,6030,Hadi,Salman,,Microsoft Research AI,,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers
neurips,2019,1,6030,Jerry,Li,,Microsoft,,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers
neurips,2019,2,6030,Ilya,Razenshteyn,,Microsoft Research,,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers
neurips,2019,3,6030,Pengchuan,Zhang,,Microsoft Research,,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers
neurips,2019,4,6030,Huan,Zhang,,Microsoft Research AI,,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers
neurips,2019,5,6030,Sebastien,Bubeck,,Microsoft Research,,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers
neurips,2019,6,6030,Greg,Yang,,Microsoft Research,,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers
neurips,2019,0,768,Danfei,Xu,,Stanford University,,Regression Planning Networks
neurips,2019,1,768,Roberto,Martín-Martín,,Stanford University,,Regression Planning Networks
neurips,2019,2,768,De-An,Huang,,Stanford University,,Regression Planning Networks
neurips,2019,3,768,Yuke,Zhu,,Stanford University,,Regression Planning Networks
neurips,2019,4,768,Silvio,Savarese,,Stanford University,,Regression Planning Networks
neurips,2019,5,768,Li,Fei-Fei,,Stanford University,,Regression Planning Networks
neurips,2019,0,8079,Junran,Peng,,CASIA,,Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection
neurips,2019,1,8079,Ming,Sun,,sensetime.com,,Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection
neurips,2019,2,8079,ZHAO-XIANG,ZHANG,,"Chinese Academy of Sciences, China",,Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection
neurips,2019,3,8079,Tieniu,Tan,,Chinese Academy of Sciences,,Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection
neurips,2019,4,8079,Junjie,Yan,,Sensetime Group Limited,,Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection
neurips,2019,0,5401,Patrick,Schwab,,ETH Zurich / Roche,,CXPlain: Causal Explanations for Model Interpretation under Uncertainty
neurips,2019,1,5401,Walter,Karlen,,ETH Zurich,,CXPlain: Causal Explanations for Model Interpretation under Uncertainty
neurips,2019,0,7589,Ching-Yi,Hung,gmail,Academia Sinica,brent12052003@gmail.com,"Compacting, Picking and Growing for Unforgetting Continual Learning"
neurips,2019,1,7589,Cheng-Hao,Tu,gmail,Academia Sinica,andytu455176@gmail.com,"Compacting, Picking and Growing for Unforgetting Continual Learning"
neurips,2019,2,7589,Cheng-En,Wu,sinica,Academia Sinica,chengen@iis.sinica.edu.tw,"Compacting, Picking and Growing for Unforgetting Continual Learning"
neurips,2019,3,7589,Chien-Hung,Chen,sinica,Academia Sinica,redsword26@iis.sinica.edu.tw,"Compacting, Picking and Growing for Unforgetting Continual Learning"
neurips,2019,4,7589,Yi-Ming,Chan,sinica,Academia Sinica,yiming@iis.sinica.edu.tw,"Compacting, Picking and Growing for Unforgetting Continual Learning"
neurips,2019,5,7589,Chu-Song,Chen,sinica,Academia Sinica,song@iis.sinica.edu.tw,"Compacting, Picking and Growing for Unforgetting Continual Learning"
neurips,2019,0,8728,Vasilis,Syrgkanis,microsoft,Microsoft Research,vasy@microsoft.com,Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments
neurips,2019,1,8728,Victor,Lei,tripadvisor,TripAdvisor,vlei@tripadvisor.com,Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments
neurips,2019,2,8728,Miruna,Oprescu,microsoft,Microsoft Research,moprescu@microsoft.com,Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments
neurips,2019,3,8728,Maggie,Hei,microsoft,Microsoft,Maggie.Hei@microsoft.com,Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments
neurips,2019,4,8728,Keith,Battocchi,microsoft,Microsoft,glewis@microsoft.com,Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments
neurips,2019,5,8728,Greg,Lewis,microsoft,Microsoft Research,kebatt@microsoft.com,Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments
neurips,2019,0,1122,Zhiao,Huang,,University of California San Diego,,Mapping State Space using Landmarks for Universal Goal Reaching
neurips,2019,1,1122,Fangchen,Liu,,"University of California, San Diego",,Mapping State Space using Landmarks for Universal Goal Reaching
neurips,2019,2,1122,Hao,Su,,University of California San Diego,,Mapping State Space using Landmarks for Universal Goal Reaching
neurips,2019,0,5181,Miguel,Vaquero,ucsd,UCSD,cortes@ucsd.edu,Convergence-Rate-Matching Discretization of Accelerated Optimization Flows Through Opportunistic State-Triggered Control
neurips,2019,1,5181,Jorge,Cortes,ucsd,UCSD,mivaquerovallina@ucsd.edu,Convergence-Rate-Matching Discretization of Accelerated Optimization Flows Through Opportunistic State-Triggered Control
neurips,2019,0,2126,Yujia,Jin,stanford,Stanford University,yujiajin@stanford.edu,Principal Component Projection and Regression in Nearly Linear Time through Asymmetric SVRG
neurips,2019,1,2126,Aaron,Sidford,stanford,Stanford,sidford@stanford.edu,Principal Component Projection and Regression in Nearly Linear Time through Asymmetric SVRG
neurips,2019,0,6024,Raef,Bassily,,The Ohio State University,,Private Stochastic Convex Optimization with Optimal Rates
neurips,2019,1,6024,Vitaly,Feldman,,Google Brain,,Private Stochastic Convex Optimization with Optimal Rates
neurips,2019,2,6024,Kunal,Talwar,,Google,,Private Stochastic Convex Optimization with Optimal Rates
neurips,2019,3,6024,Abhradeep,Guha Thakurta,,University of California Santa Cruz,,Private Stochastic Convex Optimization with Optimal Rates
neurips,2019,0,7769,Sebastien,Bubeck,microsoft,Microsoft Research,sebubeck@microsoft.com,Complexity of Highly Parallel Non-Smooth Convex Optimization
neurips,2019,1,7769,Qijia,Jiang,stanford,Stanford University,qjiang2@stanford.edu,Complexity of Highly Parallel Non-Smooth Convex Optimization
neurips,2019,2,7769,Yin-Tat,Lee,uw,,yintat@uw.edu,Complexity of Highly Parallel Non-Smooth Convex Optimization
neurips,2019,3,7769,Yuanzhi,Li,stanford,Princeton,yuanzhil@stanford.edu,Complexity of Highly Parallel Non-Smooth Convex Optimization
neurips,2019,4,7769,Aaron,Sidford,stanford,Stanford,sidford@stanford.edu,Complexity of Highly Parallel Non-Smooth Convex Optimization
neurips,2019,0,4443,Nicolas,Carion,fb,Facebook AI Research Paris,alcinos@fb.com,A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning
neurips,2019,1,4443,Nicolas,Usunier,fb,Facebook AI Research,gab@fb.com,A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning
neurips,2019,2,4443,Gabriel,Synnaeve,fb,Facebook,lazaric@fb.com,A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning
neurips,2019,3,4443,Alessandro,Lazaric,fb,Facebook Artificial Intelligence Research,usunier@fb.com,A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning
neurips,2019,0,4344,Shuo,Yang,utexas,UT Austin,yangshuo_ut@utexas.edu,Interaction Hard Thresholding: Consistent Sparse Quadratic Regression in Sub-quadratic Time and Space
neurips,2019,1,4344,Yanyao,Shen,utexas,UT Austin,shenyanyao@utexas.edu,Interaction Hard Thresholding: Consistent Sparse Quadratic Regression in Sub-quadratic Time and Space
neurips,2019,2,4344,Sujay,Sanghavi,utexas,UT-Austin,sanghavi@mail.utexas.edu,Interaction Hard Thresholding: Consistent Sparse Quadratic Regression in Sub-quadratic Time and Space
neurips,2019,0,8195,Kanthi,Sarpatwar,ibm,IBM T. J. Watson Research Center,sarpatwa@us.ibm.com,Differentially Private Distributed Data Summarization under Covariate Shift
neurips,2019,1,8195,Karthikeyan,Shanmugam,ibm,"IBM Research, NY",karthikeyan.shanmugam2@ibm.com,Differentially Private Distributed Data Summarization under Covariate Shift
neurips,2019,2,8195,Venkata Sitaramagiridharganesh,Ganapavarapu,ibm,IBM Research,giridhar.ganapavarapu@ibm.com,Differentially Private Distributed Data Summarization under Covariate Shift
neurips,2019,3,8195,Ashish,Jagmohan,ibm,IBM Research,ashishja@us.ibm.com,Differentially Private Distributed Data Summarization under Covariate Shift
neurips,2019,4,8195,Roman,Vaculin,ibm,IBM Research,vaculin@us.ibm.com,Differentially Private Distributed Data Summarization under Covariate Shift
neurips,2019,0,5511,Chenyang,Tao,,Duke University,,On Fenchel Mini-Max Learning
neurips,2019,1,5511,Liqun,Chen,,Duke University,,On Fenchel Mini-Max Learning
neurips,2019,2,5511,Shuyang,Dai,,Duke University,,On Fenchel Mini-Max Learning
neurips,2019,3,5511,Junya,Chen,,Duke U,,On Fenchel Mini-Max Learning
neurips,2019,4,5511,Ke,Bai,,Duke University,,On Fenchel Mini-Max Learning
neurips,2019,5,5511,Dong,Wang,,Duke University,,On Fenchel Mini-Max Learning
neurips,2019,6,5511,Jianfeng,Feng,,Fudan University,,On Fenchel Mini-Max Learning
neurips,2019,7,5511,Wenlian,Lu,,Fudan University,,On Fenchel Mini-Max Learning
neurips,2019,8,5511,Georgiy,Bobashev,,RTI International,,On Fenchel Mini-Max Learning
neurips,2019,9,5511,Lawrence,Carin,,Duke University,,On Fenchel Mini-Max Learning
neurips,2019,0,5730,Harikrishna,Narasimhan,google,Google Research,hnarasimhan@google.com,Optimizing Generalized Rate Metrics with Three Players
neurips,2019,1,5730,Andrew,Cotter,google,Google,acotter@google.com,Optimizing Generalized Rate Metrics with Three Players
neurips,2019,2,5730,Maya,Gupta,google,Google,mayagupta@google.com,Optimizing Generalized Rate Metrics with Three Players
neurips,2019,0,4400,Fernando,Gama,nyu,University of Pennsylvania,bruna@cims.nyu.edu,Stability of Graph Scattering Transforms
neurips,2019,1,4400,Alejandro,Ribeiro,upenn,University of Pennsylvania,fgama@seas.upenn.edu,Stability of Graph Scattering Transforms
neurips,2019,2,4400,Joan,Bruna,upenn,NYU,aribeiro@seas.upenn.edu,Stability of Graph Scattering Transforms
neurips,2019,0,2438,Marc,Bellemare,,Google Brain,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,1,2438,Will,Dabney,,DeepMind,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,2,2438,Robert,Dadashi,,Google Brain,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,3,2438,Adrien,Ali Taiga,,MILA,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,4,2438,Pablo Samuel,Castro,,Google,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,5,2438,Nicolas,Le Roux,,Google Brain,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,6,2438,Dale,Schuurmans,,Google Inc.,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,7,2438,Tor,Lattimore,,DeepMind,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,8,2438,Clare,Lyle,,University of Oxford,,A Geometric Perspective on Optimal Representations for Reinforcement Learning
neurips,2019,0,1336,Quanfu,Fan,ibm,MIT-IBM Watson AI Lab,qfan@us.ibm.com,More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation
neurips,2019,1,1336,Chun-Fu (Richard),Chen,ibm,IBM Research,chenrich@us.ibm.com,More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation
neurips,2019,2,1336,Hilde,Kuehne,ibm,University of Bonn,pistoia@us.ibm.com,More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation
neurips,2019,3,1336,Marco,Pistoia,ibm,IBM Research,kuehne@ibm.com,More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation
neurips,2019,4,1336,David,Cox,ibm,MIT-IBM Watson AI Lab,david.d.cox@ibm.com,More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation
neurips,2019,0,4404,Simon,Du,ias,Institute for Advanced Study,ssdu@ias.edu,Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle
neurips,2019,1,4404,Yuping,Luo,princeton,Princeton University,yupingl@cs.princeton.edu,Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle
neurips,2019,2,4404,Ruosong,Wang,cmu,Carnegie Mellon University,ruosongw@andrew.cmu.edu,Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle
neurips,2019,3,4404,Hanrui,Zhang,duke,Duke University,hrzhang@cs.duke.edu,Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle
neurips,2019,0,2298,Sebastian,Tschiatschek,microsoft,Microsoft Research,setschia@microsoft.com,Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints
neurips,2019,1,2298,Ahana,Ghosh,mpi-sws,MPI-SWS,gahana@mpi-sws.org,Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints
neurips,2019,2,2298,Luis,Haug,ethz,ETH Zurich,lhaug@inf.ethz.ch,Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints
neurips,2019,3,2298,Rati,Devidze,mpi-sws,MPI-SWS,rdevidze@mpi-sws.org,Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints
neurips,2019,4,2298,Adish,Singla,mpi-sws,MPI-SWS,adishs@mpi-sws.org,Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints
neurips,2019,0,6604,Zakaria,Mhammedi,anu,The Australian National University,zak.mhammedi@anu.edu.au,PAC-Bayes Un-Expected Bernstein Inequality
neurips,2019,1,6604,Peter,Grünwald,cwi,CWI and Leiden University,pdg@cwi.nl,PAC-Bayes Un-Expected Bernstein Inequality
neurips,2019,2,6604,Benjamin,Guedj,inria,Inria & University College London,benjamin.guedj@inria.fr,PAC-Bayes Un-Expected Bernstein Inequality
neurips,2019,0,2232,Lorenzo,Dall'Amico,,GIPSA lab,,Revisiting the Bethe-Hessian: Improved Community Detection in Sparse Heterogeneous Graphs
neurips,2019,1,2232,Romain,Couillet,,CentralSupélec,,Revisiting the Bethe-Hessian: Improved Community Detection in Sparse Heterogeneous Graphs
neurips,2019,2,2232,Nicolas,Tremblay,,CNRS,,Revisiting the Bethe-Hessian: Improved Community Detection in Sparse Heterogeneous Graphs
neurips,2019,0,7901,Yingxiang,Yang,illinois,University of Illinois at Urbana-Champaign,yyang172@illinois.edu,Learning Positive Functions with Pseudo Mirror Descent
neurips,2019,1,7901,Haoxiang,Wang,illinois,"University of Illinois, Urbana-Champaign",hwang264@illinois.edu,Learning Positive Functions with Pseudo Mirror Descent
neurips,2019,2,7901,Negar,Kiyavash,epfl,EPFL,negar.kiyavash@epfl.ch,Learning Positive Functions with Pseudo Mirror Descent
neurips,2019,3,7901,Niao,He,illinois,UIUC,niaohe@illinois.edu,Learning Positive Functions with Pseudo Mirror Descent
neurips,2019,0,8223,Arun,Verma,iitb,Indian Institute of Technology Bombay,v.arun@iitb.ac.in,Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback
neurips,2019,1,8223,Manjesh,Hanawal,iitm,Indian Institute of Technology Bombay,arunr@cse.iitm.ac.in,Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback
neurips,2019,2,8223,Arun,Rajkumar,iitb,Indian Institute of Technology Madras,mhanwal@iitb.ac.in,Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback
neurips,2019,3,8223,Raman,Sankaran,linkedin,LinkedIn,rsankara@linkedin.com,Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback
neurips,2019,0,4848,Rowan,Zellers,,University of Washington,,Defending Against Neural Fake News
neurips,2019,1,4848,Ari,Holtzman,,University of Washington,,Defending Against Neural Fake News
neurips,2019,2,4848,Hannah,Rashkin,,University of Washington,,Defending Against Neural Fake News
neurips,2019,3,4848,Yonatan,Bisk,,Carnegie Mellon University,,Defending Against Neural Fake News
neurips,2019,4,4848,Ali,Farhadi,,"University of Washington, Allen Institute for Artificial Intelligence",,Defending Against Neural Fake News
neurips,2019,5,4848,Franziska,Roesner,,University of Washington,,Defending Against Neural Fake News
neurips,2019,6,4848,Yejin,Choi,,University of Washington,,Defending Against Neural Fake News
neurips,2019,0,4549,Amirhossein,Reisizadeh,ucsb,UC Santa Barbara,reisizadeh@ucsb.edu,Robust and Communication-Efficient Collaborative Learning
neurips,2019,1,4549,Hossein,Taheri,ucsb,UCSB,hossein@ucsb.edu,Robust and Communication-Efficient Collaborative Learning
neurips,2019,2,4549,Aryan,Mokhtari,utexas,UT Austin,mokhtari@austin.utexas.edu,Robust and Communication-Efficient Collaborative Learning
neurips,2019,3,4549,Hamed,Hassani,upenn,UPenn,hassani@seas.upenn.edu,Robust and Communication-Efficient Collaborative Learning
neurips,2019,4,4549,Ramtin,Pedarsani,ucsb,UC Santa Barbara,ramtin@ece.ucsb.edu,Robust and Communication-Efficient Collaborative Learning
neurips,2019,0,8329,Zehua,Zhang,indiana,Indiana University Bloomington,zehzhang@indiana.edu,A Self Validation Network for Object-Level Human Attention Estimation
neurips,2019,1,8329,Chen,Yu,indiana,Indiana University,chenyu@indiana.edu,A Self Validation Network for Object-Level Human Attention Estimation
neurips,2019,2,8329,David,Crandall,indiana,Indiana University,djcran@indiana.edu,A Self Validation Network for Object-Level Human Attention Estimation
neurips,2019,0,5526,Haohan,Wang,cmu,Carnegie Mellon University,haohanw@cs.cmu.edu,Learning Robust Global Representations by Penalizing Local Predictive Power
neurips,2019,1,5526,Songwei,Ge,cmu,Carnegie Mellon University,songweig@cs.cmu.edu,Learning Robust Global Representations by Penalizing Local Predictive Power
neurips,2019,2,5526,Zachary,Lipton,cmu,Carnegie Mellon University,epxing@cs.cmu.edu,Learning Robust Global Representations by Penalizing Local Predictive Power
neurips,2019,3,5526,Eric,Xing,cmu,Petuum Inc. /  Carnegie Mellon University,zlipton@cs.cmu.edu,Learning Robust Global Representations by Penalizing Local Predictive Power
neurips,2019,0,85,Mark,Bun,bu,Boston University,mbun@bu.edu,Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation
neurips,2019,1,85,Thomas,Steinke,thomas-steinke,IBM -- Almaden,smooth@thomas-steinke.net,Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation
neurips,2019,0,3186,Wenhao,Yang,pku,Peking University,lx10077@pku.edu.cn,A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning
neurips,2019,1,3186,Xiang,Li,pku,Peking University,yangwenhaosms@pku.edu.cn,A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning
neurips,2019,2,3186,Zhihua,Zhang,pku,Peking University,zhzhang@math.pku.edu.cn,A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning
neurips,2019,0,3270,Yu,Qi,zju,Zhejiang University,qiyu@zju.edu.cn,Dynamic Ensemble Modeling Approach to Nonstationary Neural Decoding in Brain-Computer Interfaces
neurips,2019,1,3270,Bin,Liu,ieee,Nanjing University of Posts and Telecommunications,bins@ieee.org,Dynamic Ensemble Modeling Approach to Nonstationary Neural Decoding in Brain-Computer Interfaces
neurips,2019,2,3270,Yueming,Wang,zju,Zhejiang University,ymingwang@zju.edu.cn,Dynamic Ensemble Modeling Approach to Nonstationary Neural Decoding in Brain-Computer Interfaces
neurips,2019,3,3270,Gang,Pan,zju,Zhejiang University,gpan@zju.edu.cn,Dynamic Ensemble Modeling Approach to Nonstationary Neural Decoding in Brain-Computer Interfaces
neurips,2019,0,3481,Ioannis,Panageas,sutd,SUTD,ioannis@sutd.edu.sg,First-order methods almost always avoid saddle points: The case of vanishing step-sizes
neurips,2019,1,3481,Georgios,Piliouras,sutd,Singapore University of Technology and Design,georgios@sutd.edu.sg,First-order methods almost always avoid saddle points: The case of vanishing step-sizes
neurips,2019,2,3481,Xiao,Wang,sutd,Singapore university of technology and design,xiao_wang@sutd.edu.sg,First-order methods almost always avoid saddle points: The case of vanishing step-sizes
neurips,2019,0,3042,Vikas,Garg,mit,MIT,vgarg@csail.mit.edu,Online Markov Decoding: Lower Bounds and Near-Optimal Approximation Algorithms
neurips,2019,1,3042,Tamar,Pichkhadze,mit,MIT,tamarp@alum.mit.edu,Online Markov Decoding: Lower Bounds and Near-Optimal Approximation Algorithms
neurips,2019,0,6072,Julaiti,Alafate,,University of California San Diego,,Faster Boosting with Smaller Memory
neurips,2019,1,6072,Yoav,Freund,,"University of California, San Diego",,Faster Boosting with Smaller Memory
neurips,2019,0,6526,Shuyue,Hu,cuhk,The Chinese University of Hong Kong,syhu@cse.cuhk.edu.hk,Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach
neurips,2019,1,6526,Chin-wing,Leung,cuhk,The Chinese University of Hong Kong,cwleung@cse.cuhk.edu.hk,Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach
neurips,2019,2,6526,Ho-fung,Leung,cuhk,The Chinese University of Hong Kong,lhf@cse.cuhk.edu.hk,Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach
neurips,2019,0,1430,Xiuyuan,Lu,stanford,Stanford University,lxy@stanford.edu,Information-Theoretic Confidence Bounds for Reinforcement Learning
neurips,2019,1,1430,Benjamin,Van Roy,stanford,Stanford University,bvr@stanford.edu,Information-Theoretic Confidence Bounds for Reinforcement Learning
neurips,2019,0,6576,Botao,Hao,gmail,Purdue University,haobotao000@gmail.com,Bootstrapping Upper Confidence Bound
neurips,2019,1,6576,Yasin,Abbasi Yadkori,gmail,"VinAI Research/ VinTech JSC.,",yasin.abbasi@gmail.com,Bootstrapping Upper Confidence Bound
neurips,2019,2,6576,Zheng,Wen,google,DeepMind,zhengwen@google.com,Bootstrapping Upper Confidence Bound
neurips,2019,3,6576,Guang,Cheng,purdue,Purdue University,chengg@purdue.edu,Bootstrapping Upper Confidence Bound
neurips,2019,0,5443,Shashank,Rajput,wisc,University of Wisconsin - Madison,rajput3@wisc.edu,DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation
neurips,2019,1,5443,Hongyi,Wang,wisc,University of Wisconsin-Madison,hongyiwang@cs.wisc.edu,DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation
neurips,2019,2,5443,Zachary,Charles,wisc,University of Wisconsin - Madison,zcharles@math.wisc.edu,DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation
neurips,2019,3,5443,Dimitris,Papailiopoulos,papail,University of Wisconsin-Madison,dimitris@papail.io,DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation
neurips,2019,0,7986,Kareem,Amin,google,Google Research,kamin@google.com,Differentially Private Covariance Estimation
neurips,2019,1,7986,Travis,Dick,cmu,TTIC,tdick@cs.cmu.edu,Differentially Private Covariance Estimation
neurips,2019,2,7986,Alex,Kulesza,google,Google,kulesza@google.com,Differentially Private Covariance Estimation
neurips,2019,3,7986,Andres,Munoz,google,Google,ammedina@google.com,Differentially Private Covariance Estimation
neurips,2019,4,7986,Sergei,Vassilvitskii,google,Google,sergeiv@google.com,Differentially Private Covariance Estimation
neurips,2019,0,1744,Satoshi,Tsutsui,indiana,Indiana University,stsutsui@indiana.edu,Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition
neurips,2019,1,1744,Yanwei,Fu,fudan,"Fudan University, Shanghai;",yanweifu@fudan.edu.cn,Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition
neurips,2019,2,1744,David,Crandall,indiana,Indiana University,djcran@indiana.edu,Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition
neurips,2019,0,2793,Anton,Bakhtin,fb,Facebook AI Research,yolo@fb.com,PHYRE: A New Benchmark for Physical Reasoning
neurips,2019,1,2793,Laurens,van der Maaten,fb,Facebook,lvdmaaten@fb.com,PHYRE: A New Benchmark for Physical Reasoning
neurips,2019,2,2793,Justin,Johnson,fb,University of Michigan / FAIR,jcjohns@fb.com,PHYRE: A New Benchmark for Physical Reasoning
neurips,2019,3,2793,Laura,Gustafson,fb,Facebook AI Research,lgustafson@fb.com,PHYRE: A New Benchmark for Physical Reasoning
neurips,2019,4,2793,Ross,Girshick,fb,FAIR,rbg@fb.com,PHYRE: A New Benchmark for Physical Reasoning
neurips,2019,0,4587,Yunus,Esencayi,buffalo,State University of New York at Buffalo,yunusese@buffalo.edu,Facility Location Problem in Differential Privacy Model Revisited
neurips,2019,1,4587,Marco,Gaboardi,bu,Univeristy at Buffalo,gaboardi@bu.edu,Facility Location Problem in Differential Privacy Model Revisited
neurips,2019,2,4587,Shi,Li,buffalo,University at Buffalo,shil@buffalo.edu,Facility Location Problem in Differential Privacy Model Revisited
neurips,2019,3,4587,Di,Wang,buffalo,State University of New York at Buffalo,dwang45@buffalo.edu,Facility Location Problem in Differential Privacy Model Revisited
neurips,2019,0,7134,Maksym,Andriushchenko,uni-tuebingen,University of Tübingen / EPFL,maksym.andriushchenko@uni-tuebingen.de,Provably robust boosted decision stumps and trees against adversarial attacks
neurips,2019,1,7134,Matthias,Hein,uni-tuebingen,University of Tübingen,matthias.hein@uni-tuebingen.de,Provably robust boosted decision stumps and trees against adversarial attacks
neurips,2019,0,3795,Fan,Zhou,,Shanghai University of Finance and Economics,,Graph-Based Semi-Supervised Learning with Non-ignorable Non-response
neurips,2019,1,3795,Tengfei,Li,,UNC Chapel Hill,,Graph-Based Semi-Supervised Learning with Non-ignorable Non-response
neurips,2019,2,3795,Haibo,Zhou,,University of North Carolina at Chapel Hill,,Graph-Based Semi-Supervised Learning with Non-ignorable Non-response
neurips,2019,3,3795,Hongtu,Zhu,,UNC Chapel Hill,,Graph-Based Semi-Supervised Learning with Non-ignorable Non-response
neurips,2019,4,3795,Ye,Jieping,,DiDi Chuxing,,Graph-Based Semi-Supervised Learning with Non-ignorable Non-response
neurips,2019,0,2858,Yulia,Rubanova,toronto,University of Toronto,rubanova@cs.toronto.edu,Latent Ordinary Differential Equations for Irregularly-Sampled Time Series
neurips,2019,1,2858,Ricky T. Q.,Chen,toronto,U of Toronto,rtqichen@cs.toronto.edu,Latent Ordinary Differential Equations for Irregularly-Sampled Time Series
neurips,2019,2,2858,David,Duvenaud,toronto,University of Toronto,duvenaud@cs.toronto.edu,Latent Ordinary Differential Equations for Irregularly-Sampled Time Series
neurips,2019,0,3848,Abi,Komanduru,,Purdue University,,On the Correctness and Sample Complexity of Inverse Reinforcement Learning
neurips,2019,1,3848,Jean,Honorio,,Purdue University,,On the Correctness and Sample Complexity of Inverse Reinforcement Learning
neurips,2019,0,7601,Andrew,Stirn,columbia,Columbia University,andrew.stirn@cs.columbia.edu,A New Distribution on the Simplex with Auto-Encoding Applications
neurips,2019,1,7601,Tony,Jebara,columbia,Spotify,jebara@cs.columbia.edu,A New Distribution on the Simplex with Auto-Encoding Applications
neurips,2019,2,7601,David,Knowles,columbia,Columbia University,daknowles@cs.columbia.edu,A New Distribution on the Simplex with Auto-Encoding Applications
neurips,2019,0,8339,Dylan,Foster,mit,MIT,dylanf@mit.edu,Model Selection for Contextual Bandits
neurips,2019,1,8339,Akshay,Krishnamurthy,usc,Microsoft,haipengl@usc.edu,Model Selection for Contextual Bandits
neurips,2019,2,8339,Haipeng,Luo,umass,University of Southern California,akshay@cs.umass.edu,Model Selection for Contextual Bandits
neurips,2019,0,4493,Andrew,Spielberg,mit,Massachusetts Institute of Technology,aespielberg@csail.mit.edu,Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations
neurips,2019,1,4493,Allan,Zhao,mit,Massachusetts Institute of Technology,azhao@mit.edu,Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations
neurips,2019,2,4493,Yuanming,Hu,mit,Massachusetts Institute of Technology,taodu@csail.mit.edu,Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations
neurips,2019,3,4493,Tao,Du,mit,MIT,yuanming@mit.edu,Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations
neurips,2019,4,4493,Wojciech,Matusik,mit,MIT,rus@csail.mit.edu,Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations
neurips,2019,5,4493,Daniela,Rus,mit,Massachusetts Institute of Technology,wojciech@csail.mit.edu,Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations
neurips,2019,0,81,Xiaosong,Zhang,,University of Chinese Academy of Sciences,,FreeAnchor: Learning to Match Anchors for Visual Object Detection
neurips,2019,1,81,Fang,Wan,,University of Chinese Academy of Sciences,,FreeAnchor: Learning to Match Anchors for Visual Object Detection
neurips,2019,2,81,Chang,Liu,,University of Chinese Academy of Sciences,,FreeAnchor: Learning to Match Anchors for Visual Object Detection
neurips,2019,3,81,Rongrong,Ji,,"Xiamen University, China",,FreeAnchor: Learning to Match Anchors for Visual Object Detection
neurips,2019,4,81,Qixiang,Ye,,"University of Chinese Academy of Sciences, China",,FreeAnchor: Learning to Match Anchors for Visual Object Detection
neurips,2019,0,8672,Rachel,Carrington,nottingham,University of Nottingham,rachel.carrington@nottingham.ac.uk,Invariance and identifiability issues for word embeddings
neurips,2019,1,8672,Karthik,Bharath,nottingham,University of Nottingham,karthik.bharath@nottingham.ac.uk,Invariance and identifiability issues for word embeddings
neurips,2019,2,8672,Simon,Preston,nottingham,University of Nottingham,simon.preston@nottingham.ac.uk,Invariance and identifiability issues for word embeddings
neurips,2019,0,1828,Alex,Wang,,New York University,,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems
neurips,2019,1,1828,Yada,Pruksachatkun,,New York University,,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems
neurips,2019,2,1828,Nikita,Nangia,,NYU,,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems
neurips,2019,3,1828,Amanpreet,Singh,,Facebook,,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems
neurips,2019,4,1828,Julian,Michael,,University of Washington,,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems
neurips,2019,5,1828,Felix,Hill,,Google Deepmind,,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems
neurips,2019,6,1828,Omer,Levy,,Facebook AI Research,,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems
neurips,2019,7,1828,Samuel,Bowman,,New York University,,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems
neurips,2019,0,1879,Yongkai,Wu,uark,University of Arkansas,yw009@uark.edu,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness
neurips,2019,1,1879,Lu,Zhang,uark,University of Arkansas,xintaowu@uark.edu,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness
neurips,2019,2,1879,Xintao,Wu,uark,University of Arkansas,lz006@uark.edu,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness
neurips,2019,3,1879,Hanghang,Tong,illinois,Arizona State University,htong@illinois.edu,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness
neurips,2019,0,8184,Daniel,Russo,columbia,Columbia University,djr2174@gsb.columbia.edu,Worst-Case Regret Bounds for Exploration via Randomized Value Functions
neurips,2019,0,1572,Yuxian,Meng,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,1,1572,Wei,Wu,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,2,1572,Fei,Wang,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,3,1572,Xiaoya,Li,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,4,1572,Ping,Nie,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,5,1572,Fan,Yin,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,6,1572,Muyu,Li,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,7,1572,Qinghong,Han,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,8,1572,Xiaofei,Sun,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,9,1572,Jiwei,Li,,Shannon.AI,,Glyce: Glyph-vectors for Chinese Character Representations
neurips,2019,0,6483,Fabian,Latorre,,EPFL,,Fast and Provable ADMM for Learning with Generative Priors
neurips,2019,1,6483,Armin,eftekhari,,EPFL,,Fast and Provable ADMM for Learning with Generative Priors
neurips,2019,2,6483,Volkan,Cevher,,EPFL,,Fast and Provable ADMM for Learning with Generative Priors
neurips,2019,0,4018,Edward,De Brouwer,kuleuven,KU Leuven,edward.debrouwer@esat.kuleuven.be,GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series
neurips,2019,1,4018,Jaak,Simm,kuleuven,KU Leuven,adam.arany@esat.kuleuven.be,GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series
neurips,2019,2,4018,Adam,Arany,kuleuven,University of Leuven,jaak.simm@esat.kuleuven.be,GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series
neurips,2019,3,4018,Yves,Moreau,kuleuven,KU Leuven,moreau@esat.kuleuven.be,GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series
neurips,2019,0,7167,Amin,Karbasi,upenn,Yale,hassani@seas.upenn.edu,Stochastic Continuous Greedy ++:  When Upper and Lower Bounds Match
neurips,2019,1,7167,Hamed,Hassani,yale,UPenn,amin.karbasi@yale.edu,Stochastic Continuous Greedy ++:  When Upper and Lower Bounds Match
neurips,2019,2,7167,Aryan,Mokhtari,utexas,UT Austin,mokhtari@austin.utexas.edu,Stochastic Continuous Greedy ++:  When Upper and Lower Bounds Match
neurips,2019,3,7167,Zebang,Shen,upenn,University of Pennsylvania,zebang@seas.upenn.edu,Stochastic Continuous Greedy ++:  When Upper and Lower Bounds Match
neurips,2019,0,8112,Maurice,Weiler,uva,University of Amsterdam,m.weiler@uva.nl,General E(2)-Equivariant Steerable CNNs
neurips,2019,1,8112,Gabriele,Cesa,gmail,University of Amsterdam,cesa.gabriele@gmail.com,General E(2)-Equivariant Steerable CNNs
neurips,2019,0,3769,Yu-Guan,Hsieh,ens,Université Grenoble Alpes / École Normale Supérieure Paris,yu-guan.hsieh@ens.fr,On the convergence of single-call stochastic extra-gradient methods
neurips,2019,1,3769,Franck,Iutzeler,univ-grenoble-alpes,Univ. Grenoble Alpes,franck.iutzeler@univ-grenoble-alpes.fr,On the convergence of single-call stochastic extra-gradient methods
neurips,2019,2,3769,Jérôme,Malick,univ-grenoble-alpes,CNRS and LJK,jerome.malick@univ-grenoble-alpes.fr,On the convergence of single-call stochastic extra-gradient methods
neurips,2019,3,3769,Panayotis,Mertikopoulos,imag,CNRS (French National Center for Scientific Research),panayotis.mertikopoulos@imag.fr,On the convergence of single-call stochastic extra-gradient methods
neurips,2019,0,7244,Guannan,Zhang,ornl,Oak Ridge National Laboratory,zhangg@ornl.gov,Learning nonlinear level sets for dimensionality reduction in function approximation
neurips,2019,1,7244,Jiaxin,Zhang,ornl,Oak Ridge National Laboratory,zhangj@ornl.gov,Learning nonlinear level sets for dimensionality reduction in function approximation
neurips,2019,2,7244,Jacob,Hinkle,ornl,Oak Ridge National Lab,hinklejd@ornl.gov,Learning nonlinear level sets for dimensionality reduction in function approximation
neurips,2019,0,2920,Corinna,Cortes,google,Google Research,corinna@google.com,Regularized Gradient Boosting
neurips,2019,1,2920,Mehryar,Mohri,google,Courant Inst. of Math. Sciences & Google Research,mohri@google.com,Regularized Gradient Boosting
neurips,2019,2,2920,Dmitry,Storcheus,google,Google Research,dstorcheus@google.com,Regularized Gradient Boosting
neurips,2019,0,2368,Vincent,LE GUEN,edf,"CNAM, Paris, France",vincent.le-guen@edf.fr,Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models
neurips,2019,1,2368,Nicolas,THOME,cnam,Cnam (Conservatoire national des arts et métiers),nicolas.thome@cnam.fr,Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models
neurips,2019,0,582,Tao,Sun,163,National university of defense technology,nudtsuntao@163.com,General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme
neurips,2019,1,582,Yuejiao,Sun,ucla,"University of California, Los Angeles",sunyj@math.ucla.edu,General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme
neurips,2019,2,582,Dongsheng,Li,nudt,"School of Computer Science, National University of Defense Technology",dsli@nudt.edu.cn,General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme
neurips,2019,3,582,Qing,Liao,hit,Harbin Institute of Technology (Shenzhen),liaoqing@hit.edu.cn,General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme
neurips,2019,0,8268,Rohith,Kuditipudi,duke,Duke University,rohith.kuditipudi@duke.edu,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets
neurips,2019,1,8268,Xiang,Wang,duke,Duke University,xwang@cs.duke.edu,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets
neurips,2019,2,8268,Holden,Lee,princeton,Princeton,holdenl@princeton.edu,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets
neurips,2019,3,8268,Yi,Zhang,princeton,Princeton,y.zhang@cs.princeton.edu,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets
neurips,2019,4,8268,Zhiyuan,Li,princeton,Princeton University,zhiyuanli@cs.princeton.edu,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets
neurips,2019,5,8268,Wei,Hu,princeton,Princeton University,huwei@cs.princeton.edu,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets
neurips,2019,6,8268,Rong,Ge,princeton,Duke University,arora@cs.princeton.edu,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets
neurips,2019,7,8268,Sanjeev,Arora,duke,Princeton University,rongge@cs.duke.edu,Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets
neurips,2019,0,2300,Frederik,Kunstner,ubc,EPFL,kunstner@cs.ubc.ca,Limitations of the empirical Fisher approximation for natural gradient descent
neurips,2019,1,2300,Philipp,Hennig,mpg,University of Tübingen and MPI for Intelligent Systems Tübingen,lballes@tue.mpg.de,Limitations of the empirical Fisher approximation for natural gradient descent
neurips,2019,2,2300,Lukas,Balles,mpg,University of Tuebingen,ph@tue.mpg.de,Limitations of the empirical Fisher approximation for natural gradient descent
neurips,2019,0,7941,Deeksha,Adil,toronto,University of Toronto,deeksha@cs.toronto.edu,"Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression"
neurips,2019,1,7941,Richard,Peng,gatech,Georgia Tech,rpeng@cc.gatech.edu,"Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression"
neurips,2019,2,7941,Sushant,Sachdeva,toronto,University of Toronto,sachdeva@cs.toronto.edu,"Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression"
neurips,2019,0,4354,John,Bradshaw,cam,University of Cambridge/MPI IS Tübingen,jab255@cam.ac.uk,A Model to Search for Synthesizable Molecules
neurips,2019,1,4354,Brooks,Paige,turing,Alan Turing Institute,bpaige@turing.ac.uk,A Model to Search for Synthesizable Molecules
neurips,2019,2,4354,Matt,Kusner,ucl,University College London,m.kusner@ucl.ac.uk,A Model to Search for Synthesizable Molecules
neurips,2019,3,4354,Marwin,Segler,benevolent,BenevolentAI,marwin.segler@benevolent.ai,A Model to Search for Synthesizable Molecules
neurips,2019,4,4354,José Miguel,Hernández-Lobato,cam,University of Cambridge,jmh233@cam.ac.uk,A Model to Search for Synthesizable Molecules
neurips,2019,0,2823,Saeed,Mahloujifar,,University of Virginia,,Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness
neurips,2019,1,2823,Xiao,Zhang,,University of Virginia,,Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness
neurips,2019,2,2823,Mohammad,Mahmoody,,University of Virginia,,Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness
neurips,2019,3,2823,David,Evans,,University of Virginia,,Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness
neurips,2019,0,1524,Fuwen,Tan,virginia,University of Virginia,fuwen.tan@virginia.edu,Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries
neurips,2019,1,1524,Paola,Cascante-Bonilla,virginia,University of Virginia,pc9za@virginia.com,Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries
neurips,2019,2,1524,Xiaoxiao,Guo,ibm,IBM Research,xiaoxiao.guo@ibm.com,Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries
neurips,2019,3,1524,Hui,Wu,ibm,IBM Research,wuhu@us.ibm.com,Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries
neurips,2019,4,1524,Song,Feng,ibm,IBM Research,sfeng@us.ibm.com,Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries
neurips,2019,5,1524,Vicente,Ordonez,virginia,University of Virginia,vicente@virginia.edu,Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries
neurips,2019,0,4388,Yu,Bai,stanford,Stanford University,yub@stanford.edu,Provably Efficient Q-Learning with Low Switching Cost
neurips,2019,1,4388,Tengyang,Xie,illinois,University of Illinois at Urbana-Champaign,tx10@illinois.edu,Provably Efficient Q-Learning with Low Switching Cost
neurips,2019,2,4388,Nan,Jiang,illinois,University of Illinois at Urbana-Champaign,nanjiang@illinois.edu,Provably Efficient Q-Learning with Low Switching Cost
neurips,2019,3,4388,Yu-Xiang,Wang,ucsb,UC Santa Barbara,yuxiangw@cs.ucsb.edu,Provably Efficient Q-Learning with Low Switching Cost
neurips,2019,0,4511,Alaa,Maalouf,gmail,The University of Haifa,Alaamalouf12@gmail.com,Fast and Accurate Least-Mean-Squares Solvers
neurips,2019,1,4511,Ibrahim,Jubran,gmail,The University of Haifa,ibrahim.jub@gmail.com,Fast and Accurate Least-Mean-Squares Solvers
neurips,2019,2,4511,Dan,Feldman,gmail,University of Haifa,dannyf.post@gmail.com,Fast and Accurate Least-Mean-Squares Solvers
neurips,2019,0,4699,Otilia,Stretcu,cmu,Carnegie Mellon University,ostretcu@cs.cmu.edu,Graph Agreement Models for Semi-Supervised Learning
neurips,2019,1,4699,Krishnamurthy,Viswanathan,google,Google Research,kvis@google.com,Graph Agreement Models for Semi-Supervised Learning
neurips,2019,2,4699,Dana,Movshovitz-Attias,google,Google,danama@google.com,Graph Agreement Models for Semi-Supervised Learning
neurips,2019,3,4699,Emmanouil,Platanios,cmu,Carnegie Mellon University,e.a.platanios@cs.cmu.edu,Graph Agreement Models for Semi-Supervised Learning
neurips,2019,4,4699,Sujith,Ravi,google,Google Research,tomkins@google.com,Graph Agreement Models for Semi-Supervised Learning
neurips,2019,5,4699,Andrew,Tomkins,google,Google,sravi@google.com,Graph Agreement Models for Semi-Supervised Learning
neurips,2019,0,148,Bingzhe,Wu,pku,Peeking University,wubingzhe@pku.edu.cn,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection
neurips,2019,1,148,Shiwan,Zhao,pku,IBM Research - China,xuhaoyang@pku.edu.cn,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection
neurips,2019,2,148,Chaochao,Chen,pku,Ant Financial,gsun@pku.edu.cn,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection
neurips,2019,3,148,Haoyang,Xu,ibm,Peking University,zhaosw@cn.ibm.com,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection
neurips,2019,4,148,Li,Wang,antn,Ant Financial,chaochao.ccc@antn.com,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection
neurips,2019,5,148,Xiaolu,Zhang,antn,Ant Financial Services Group,aymond.wangl@antn.com,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection
neurips,2019,6,148,Guangyu,Sun,antn,Peking University,yueyin.zxl@antn.com,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection
neurips,2019,7,148,Jun,Zhou,antn,Ant Financial,jun.zhoujun@antn.com,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection
neurips,2019,0,3636,Stanislav,Fort,,Stanford University / Google Research,,Large Scale Structure of Neural Network Loss Landscapes
neurips,2019,1,3636,Stanislaw,Jastrzebski,,New York University,,Large Scale Structure of Neural Network Loss Landscapes
neurips,2019,0,5286,Horia,Mania,berkeley,UC Berkeley,hmania@berkeley.edu,Model Similarity Mitigates Test Set Overuse
neurips,2019,1,5286,John,Miller,berkeley,"University of California, Berkeley",miller_john@berkeley.edu,Model Similarity Mitigates Test Set Overuse
neurips,2019,2,5286,Ludwig,Schmidt,berkeley,UC Berkeley,ludwig@berkeley.edu,Model Similarity Mitigates Test Set Overuse
neurips,2019,3,5286,Moritz,Hardt,berkeley,"University of California, Berkeley",hardt@berkeley.edu,Model Similarity Mitigates Test Set Overuse
neurips,2019,4,5286,Benjamin,Recht,berkeley,UC Berkeley,brecht@berkeley.edu,Model Similarity Mitigates Test Set Overuse
neurips,2019,0,4081,Liangpeng,Zhang,bham,University of Birmingham,L.Zhang.7@pgr.bham.ac.uk,Explicit Planning for Efficient Exploration in Reinforcement Learning
neurips,2019,1,4081,Ke,Tang,sustc,Southern University of Science and Technology,tangk3@sustc.edu.cn,Explicit Planning for Efficient Exploration in Reinforcement Learning
neurips,2019,2,4081,Xin,Yao,sustc,Southern University of Science and Technology,xiny@sustc.edu.cn,Explicit Planning for Efficient Exploration in Reinforcement Learning
neurips,2019,0,288,Fan-Yun,Sun,,National Taiwan University,,vGraph: A Generative Model for Joint Community Detection and Node Representation Learning
neurips,2019,1,288,Meng,Qu,,Mila,,vGraph: A Generative Model for Joint Community Detection and Node Representation Learning
neurips,2019,2,288,Jordan,Hoffmann,,Harvard University/Mila,,vGraph: A Generative Model for Joint Community Detection and Node Representation Learning
neurips,2019,3,288,Chin-Wei,Huang,,MILA,,vGraph: A Generative Model for Joint Community Detection and Node Representation Learning
neurips,2019,4,288,Jian,Tang,,Mila,,vGraph: A Generative Model for Joint Community Detection and Node Representation Learning
neurips,2019,0,8753,Nishant,Subramani,,AI Foundation,,Can Unconditional Language Models Recover Arbitrary Sentences?
neurips,2019,1,8753,Samuel,Bowman,,New York University,,Can Unconditional Language Models Recover Arbitrary Sentences?
neurips,2019,2,8753,Kyunghyun,Cho,,New York University,,Can Unconditional Language Models Recover Arbitrary Sentences?
neurips,2019,0,8945,Yihao,Feng,utexas,UT Austin,yihao@cs.utexas.edu,A Kernel Loss for Solving the Bellman Equation
neurips,2019,1,8945,Lihong,Li,google,Google Brain,lihong@google.com,A Kernel Loss for Solving the Bellman Equation
neurips,2019,2,8945,Qiang,Liu,utexas,UT Austin,lqiang@cs.utexas.edu,A Kernel Loss for Solving the Bellman Equation
neurips,2019,0,5101,Nikolaos,Ignatiadis,stanford,Stanford University,ignat@stanford.edu,Covariate-Powered Empirical Bayes Estimation
neurips,2019,1,5101,Stefan,Wager,stanford,Stanford University,swager@stanford.edu,Covariate-Powered Empirical Bayes Estimation
neurips,2019,0,5641,Yuan,Cao,ucla,UCLA,yuancao@cs.ucla.edu,Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks
neurips,2019,1,5641,Quanquan,Gu,ucla,UCLA,qgu@cs.ucla.edu,Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks
neurips,2019,0,1516,Yi,Xu,uiowa,University of Iowa,yi-xu@uiowa.edu,Non-asymptotic Analysis of Stochastic Methods for Non-Smooth Non-Convex Regularized Problems
neurips,2019,1,1516,Rong,Jin,uiowa,Alibaba,tianbao-yang@uiowa.edu,Non-asymptotic Analysis of Stochastic Methods for Non-Smooth Non-Convex Regularized Problems
neurips,2019,2,1516,Tianbao,Yang,alibaba-inc,The University of Iowa,jinrong.jr@alibaba-inc.com,Non-asymptotic Analysis of Stochastic Methods for Non-Smooth Non-Convex Regularized Problems
neurips,2019,0,295,Bichuan,Guo,tsinghua,Tsinghua University,gbc16@mails.tsinghua.edu.cn,AGEM: Solving Linear Inverse Problems via Deep Priors and Sampling
neurips,2019,1,295,Yuxing,Han,scau,South China Agriculture University,yuxinghan@scau.edu.cn,AGEM: Solving Linear Inverse Problems via Deep Priors and Sampling
neurips,2019,2,295,Jiangtao,Wen,tsinghua,Tsinghua University,jtwen@tsinghua.edu.cn,AGEM: Solving Linear Inverse Problems via Deep Priors and Sampling
neurips,2019,0,5389,Yaqin,Zhou,ntu,Nanyang Technological University,1yqzhou@ntu.edu.sg,Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks
neurips,2019,1,5389,Shangqing,Liu,ntu,Nanyang Technological University,shangqin001@ntu.edu.sg,Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks
neurips,2019,2,5389,Jingkai,Siow,ntu,Nanyang Technological University,jingkai001@ntu.edu.sg,Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks
neurips,2019,3,5389,Xiaoning,Du,ntu,Nanyang Technological University,xiaoning.du@ntu.edu.sg,Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks
neurips,2019,4,5389,Yang,Liu,ntu,"Nanyang Technology University, Singapore",yangliu@ntu.edu.sg,Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks
neurips,2019,0,1587,Enrique,Fita Sanmartin,,Heidelberg University,fita@stud,Probabilistic Watershed: Sampling all spanning forests for seeded segmentation and semi-supervised learning
neurips,2019,1,1587,Sebastian,Damrich,,Heidelberg University,sebastian.damrich@iwr,Probabilistic Watershed: Sampling all spanning forests for seeded segmentation and semi-supervised learning
neurips,2019,2,1587,Fred,Hamprecht,uni-heidelberg,Heidelberg University,fred.hamprecht@iwr.uni-heidelberg.de,Probabilistic Watershed: Sampling all spanning forests for seeded segmentation and semi-supervised learning
neurips,2019,0,1504,Takuya,Hiraoka,nec,NEC / AIST / RIKEN-AIP,takuya-h1@nec.com,Learning Robust Options by Conditional Value at Risk Optimization
neurips,2019,1,1504,Takahisa,Imagawa,nec,National Institute of Advanced Industrial Science and Technology,tmori@nec.com,Learning Robust Options by Conditional Value at Risk Optimization
neurips,2019,2,1504,Tatsuya,Mori,nec,"NEC, AIST, RIKEN-AIP",takashi.onishi@nec.com,Learning Robust Options by Conditional Value at Risk Optimization
neurips,2019,3,1504,Takashi,Onishi,go,NEC / AIST,imagawa.t@aist.go.jp,Learning Robust Options by Conditional Value at Risk Optimization
neurips,2019,4,1504,Yoshimasa,Tsuruoka,u-tokyo,The University of Tokyo,tsuruoka@logos.t.u-tokyo.ac.jp,Learning Robust Options by Conditional Value at Risk Optimization
neurips,2019,0,6827,Andrei,Kulunchakov,,Inria,,A Generic Acceleration Framework for Stochastic Composite Optimization
neurips,2019,1,6827,Julien,Mairal,,Inria,,A Generic Acceleration Framework for Stochastic Composite Optimization
neurips,2019,0,8277,Runzhe,Yang,princeton,Princeton University,runzhey@cs.princeton.edu,A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation
neurips,2019,1,8277,Xingyuan,Sun,princeton,Princeton University,xs5@cs.princeton.edu,A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation
neurips,2019,2,8277,Karthik,Narasimhan,princeton,Princeton University,karthikn@cs.princeton.edu,A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation
neurips,2019,0,7539,Aymeric,Dieuleveut,ttic,"Ecole Polytechnique, IPParis",kkpatel@ttic.edu,Communication trade-offs for Local-SGD with large step size
neurips,2019,1,7539,Kumar Kshitij,Patel,polytechnique,Toyota Technological Institute at Chicago,aymeric.dieuleveut@polytechnique.edu,Communication trade-offs for Local-SGD with large step size
neurips,2019,0,7624,Renato,Negrinho,,Carnegie Mellon University,,Towards modular and programmable architecture search
neurips,2019,1,7624,Matthew,Gormley,,Carnegie Mellon University,,Towards modular and programmable architecture search
neurips,2019,2,7624,Geoffrey,Gordon,,MSR Montréal & CMU,,Towards modular and programmable architecture search
neurips,2019,3,7624,Darshan,Patil,,Carnegie Mellon University,,Towards modular and programmable architecture search
neurips,2019,4,7624,Nghia,Le,,Carnegie Mellon University,,Towards modular and programmable architecture search
neurips,2019,5,7624,Daniel,Ferreira,,TU Wien,,Towards modular and programmable architecture search
neurips,2019,0,4437,Cheng,Meng,,University of Georgia,,Large-scale optimal transport map estimation using projection pursuit
neurips,2019,1,4437,Yuan,Ke,,University of Georgia,,Large-scale optimal transport map estimation using projection pursuit
neurips,2019,2,4437,Jingyi,Zhang,,The University of Georgia,,Large-scale optimal transport map estimation using projection pursuit
neurips,2019,3,4437,Mengrui,Zhang,,University of Georgia,,Large-scale optimal transport map estimation using projection pursuit
neurips,2019,4,4437,Wenxuan,Zhong,,,,Large-scale optimal transport map estimation using projection pursuit
neurips,2019,5,4437,Ping,Ma,,University of Georgia,,Large-scale optimal transport map estimation using projection pursuit
neurips,2019,0,2372,Boris,Knyazev,uoguelph,University of Guelph,bknyazev@uoguelph.ca,Understanding Attention and Generalization in Graph Neural Networks
neurips,2019,1,2372,Graham,Taylor,uoguelph,University of Guelph,gwtaylor@uoguelph.ca,Understanding Attention and Generalization in Graph Neural Networks
neurips,2019,2,2372,Mohamed,Amer,robust,RobustAI,mohamed@robust.ai,Understanding Attention and Generalization in Graph Neural Networks
neurips,2019,0,5810,Brian,Cheung,berkeley,UC Berkeley,bcheung@berkeley.edu,Superposition of many models into one
neurips,2019,1,5810,Alexander,Terekhov,berkeley,"Awecom, Inc",aterekhov@berkeley.edu,Superposition of many models into one
neurips,2019,2,5810,Yubei,Chen,berkeley,Berkeley AI Research UC Berkeley,yubeic@berkeley.edu,Superposition of many models into one
neurips,2019,3,5810,Pulkit,Agrawal,berkeley,UC Berkeley,pulkitag@berkeley.edu,Superposition of many models into one
neurips,2019,4,5810,Bruno,Olshausen,berkeley,Redwood Center/UC Berkeley,baolshausen@berkeley.edu,Superposition of many models into one
neurips,2019,0,2260,Maxim,Kuznetsov,insilico,Insilico Medicine,kuznetsov@insilico.com,A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models
neurips,2019,1,2260,Daniil,Polykovskiy,insilico,Insilico Medicine,daniil@insilico.com,A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models
neurips,2019,2,2260,Dmitry,Vetrov,insilico,"Higher School of Economics, Samsung AI Center, Moscow",zhebrak@insilico.com,A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models
neurips,2019,3,2260,Alex,Zhebrak,yandex,Insilico Medicine,vetrovd@yandex.ru,A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models
neurips,2019,0,6831,Nicole,Muecke,uni-stuttgart,University of Stuttgart,nicole.muecke@mathematik.uni-stuttgart.de,Beating SGD Saturation with Tail-Averaging  and Minibatching
neurips,2019,1,6831,Gergely,Neu,gmail,Universitat Pompeu Fabra,gergely.neu@gmail.com,Beating SGD Saturation with Tail-Averaging  and Minibatching
neurips,2019,2,6831,Lorenzo,Rosasco,mit,University of Genova- MIT - IIT,lrosasco@mit.edu,Beating SGD Saturation with Tail-Averaging  and Minibatching
neurips,2019,0,835,Magauiya,Zhussip,unist,UNIST,mzhussip@unist.ac.kr,Extending Stein's unbiased risk estimator to train deep denoisers with correlated pairs of noisy images
neurips,2019,1,835,Shakarim,Soltanayev,unist,Ulsan National Institute of Science and Technology,shakarim@unist.ac.kr,Extending Stein's unbiased risk estimator to train deep denoisers with correlated pairs of noisy images
neurips,2019,2,835,Se Young,Chun,unist,UNIST,sychun@unist.ac.kr,Extending Stein's unbiased risk estimator to train deep denoisers with correlated pairs of noisy images
neurips,2019,0,4934,Farnam,Mansouri,,MPI-SWS,,Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models
neurips,2019,1,4934,Yuxin,Chen,,UChicago,,Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models
neurips,2019,2,4934,Ara,Vartanian,,University of Wisconsin -- Madison,,Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models
neurips,2019,3,4934,Jerry,Zhu,,University of Wisconsin-Madison,,Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models
neurips,2019,4,4934,Adish,Singla,,MPI-SWS,,Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models
neurips,2019,0,8400,Amir-massoud,Farahmand,vectorinstitute,Vector Institute and University of Toronto,farahmand@vectorinstitute.ai,Value Function in Frequency Domain and the Characteristic Value Iteration Algorithm
neurips,2019,0,1862,Jun,Sun,gmail,Zhejiang University,sunjun16sj@gmail.com,Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients
neurips,2019,1,1862,Tianyi,Chen,umn,Rensselaer Polytechnic Institute,georgios@umn.edu,Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients
neurips,2019,2,1862,Georgios,Giannakis,rpi,University of Minnesota,chent18@rpi.edu,Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients
neurips,2019,3,1862,Zaiyue,Yang,sustc,Southern University of Science and Technology,yangzy3@sustc.edu.cn,Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients
neurips,2019,0,770,Mingming,Gong,,University of Melbourne,,Twin Auxilary Classifiers GAN
neurips,2019,1,770,Yanwu,Xu,,University of Pittsburgh,,Twin Auxilary Classifiers GAN
neurips,2019,2,770,Chunyuan,Li,,Microsoft Research,,Twin Auxilary Classifiers GAN
neurips,2019,3,770,Kun,Zhang,,CMU,,Twin Auxilary Classifiers GAN
neurips,2019,4,770,Kayhan,Batmanghelich,,University of Pittsburgh,,Twin Auxilary Classifiers GAN
neurips,2019,0,3792,Mark,Herbster,ucl,University College London,m.herbster@cs.ucl.ac.uk,Online Prediction of Switching Graph Labelings with Cluster Specialists
neurips,2019,1,3792,James,Robinson,ucl,UCL,j.robinson@cs.ucl.ac.uk,Online Prediction of Switching Graph Labelings with Cluster Specialists
neurips,2019,0,7607,XIA,XIAO,uconn,University of Connecticut,xia.xiao@uconn.edu,AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters
neurips,2019,1,7607,Zigeng,Wang,uconn,University of Connecticut,zigeng.wang@uconn.edu,AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters
neurips,2019,2,7607,Sanguthevar,Rajasekaran,uconn,University of Connecticut,sanguthevar.rajasekaran@uconn.edu,AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters
neurips,2019,0,5108,Igor,Gitman,microsoft,Microsoft Research,igor.gitman@microsoft.com,Understanding the Role of Momentum in Stochastic Gradient Methods
neurips,2019,1,5108,Hunter,Lang,microsoft,Microsoft Research,hunter.lang@microsoft.com,Understanding the Role of Momentum in Stochastic Gradient Methods
neurips,2019,2,5108,Pengchuan,Zhang,microsoft,Microsoft Research,penzhan@microsoft.com,Understanding the Role of Momentum in Stochastic Gradient Methods
neurips,2019,3,5108,Lin,Xiao,microsoft,Microsoft Research,lin.xiao@microsoft.com,Understanding the Role of Momentum in Stochastic Gradient Methods
neurips,2019,0,1187,Shangtong,Zhang,ox,University of Oxford,shangtong.zhang@cs.ox.ac.uk,DAC: The Double Actor-Critic Architecture for Learning Options
neurips,2019,1,1187,Shimon,Whiteson,ox,University of Oxford,shimon.whiteson@cs.ox.ac.uk,DAC: The Double Actor-Critic Architecture for Learning Options
neurips,2019,0,1664,Matteo,Turchetta,ethz,ETH Zurich,matteotu@inf.ethz.ch,Safe Exploration for Interactive Machine Learning
neurips,2019,1,1664,Felix,Berkenkamp,ethz,ETH Zurich,befelix@inf.ethz.ch,Safe Exploration for Interactive Machine Learning
neurips,2019,2,1664,Andreas,Krause,ethz,ETH Zurich,krausea@ethz.ch,Safe Exploration for Interactive Machine Learning
neurips,2019,0,3926,Akihiro,Kishimoto,,IBM Research,,Depth-First Proof-Number Search with Heuristic Edge Cost and Application to Chemical Synthesis Planning
neurips,2019,1,3926,Beat,Buesser,,IBM Research,,Depth-First Proof-Number Search with Heuristic Edge Cost and Application to Chemical Synthesis Planning
neurips,2019,2,3926,Bei,Chen,,IBM Research,,Depth-First Proof-Number Search with Heuristic Edge Cost and Application to Chemical Synthesis Planning
neurips,2019,3,3926,Adi,Botea,,IBM Research,,Depth-First Proof-Number Search with Heuristic Edge Cost and Application to Chemical Synthesis Planning
neurips,2019,0,3901,Jiabin,Liu,uibe,University of Chinese Academy of Sciences,wangbo@uibe.edu.cn,Learning from Label Proportions with Generative Adversarial Networks
neurips,2019,1,3901,Bo,Wang,126,University of International Business and Economics,liujiabin008@126.com,Learning from Label Proportions with Generative Adversarial Networks
neurips,2019,2,3901,Zhiquan,Qi,foxmail,University of Chinese Academy of Sciences,qizhiquan@foxmail.com,Learning from Label Proportions with Generative Adversarial Networks
neurips,2019,3,3901,YingJie,Tian,ucas,University of Chinese Academy of Sciences,tyj@ucas.ac.cn,Learning from Label Proportions with Generative Adversarial Networks
neurips,2019,4,3901,Yong,Shi,ucas,University of Chinese Academy of Sciences,yshi@ucas.ac.cn,Learning from Label Proportions with Generative Adversarial Networks
neurips,2019,0,7018,David,Gamarnik,mit,Massachusetts Institute of Technology,gamarnik@mit.edu,Sparse High-Dimensional Isotonic Regression
neurips,2019,1,7018,Julia,Gaudio,mit,Massachusetts Institute of Technology,jgaudio@mit.edu,Sparse High-Dimensional Isotonic Regression
neurips,2019,0,6955,Ruibo,Tu,kth,KTH Royal Institute of Technology,ruibo@kth.se,Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation
neurips,2019,1,6955,Kun,Zhang,cmu,CMU,kunz1@cmu.edu,Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation
neurips,2019,2,6955,Bo,Bertilson,ki,KI Karolinska Institutet,bo.bertilson@ki.se,Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation
neurips,2019,3,6955,Hedvig,Kjellstrom,kth,KTH Royal Institute of Technology,hedvig@kth.se,Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation
neurips,2019,4,6955,Cheng,Zhang,microsoft,"Microsoft Research, Cambridge, UK",Cheng.Zhang@microsoft.com,Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation
neurips,2019,0,4976,Nicolas,Carrara,inria,ULille,nicolas.carrara@inria.fr,Budgeted Reinforcement Learning in Continuous State Space
neurips,2019,1,4976,Edouard,Leurent,microsoft,INRIA,romain.laroche@microsoft.com,Budgeted Reinforcement Learning in Continuous State Space
neurips,2019,2,4976,Romain,Laroche,inria,Microsoft Research,odalric.maillard@inria.fr,Budgeted Reinforcement Learning in Continuous State Space
neurips,2019,3,4976,Tanguy,Urvoy,inria,Orange-Labs,edouard.leurent@inria.fr,Budgeted Reinforcement Learning in Continuous State Space
neurips,2019,4,4976,Odalric-Ambrym,Maillard,orange,INRIA,tanguy.urvoy@orange.com,Budgeted Reinforcement Learning in Continuous State Space
neurips,2019,5,4976,Olivier,Pietquin,google,Google Research    Brain Team,pietquin@google.com,Budgeted Reinforcement Learning in Continuous State Space
neurips,2019,0,4792,Anna,Wigren,uu,Uppsala University,anna.wigren@it.uu.se,Parameter elimination in particle Gibbs sampling
neurips,2019,1,4792,Riccardo Sven,Risuleo,uu,Uppsala University,riccardo.risuleo@it.uu.se,Parameter elimination in particle Gibbs sampling
neurips,2019,2,4792,Lawrence,Murray,uber,Uber AI,lawrence.murray@uber.com,Parameter elimination in particle Gibbs sampling
neurips,2019,3,4792,Fredrik,Lindsten,liu,Linköping University,fredrik.lindsten@liu.se,Parameter elimination in particle Gibbs sampling
neurips,2019,0,5118,Tengyang,Xie,,University of Illinois at Urbana-Champaign,,Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling
neurips,2019,1,5118,Yifei,Ma,,Amazon,,Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling
neurips,2019,2,5118,Yu-Xiang,Wang,,UC Santa Barbara,,Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling
neurips,2019,0,8733,Meena,Jagadeesan,harvard,Harvard University,mjagadeesan@college.harvard.edu,Understanding Sparse JL for Feature Hashing
neurips,2019,0,6711,Jean-Bastien,Grill,google,Google DeepMind,jbgrill@google.com,Planning in entropy-regularized Markov decision processes and games
neurips,2019,1,6711,Omar,Darwiche Domingues,inria,Inria,omar.darwiche-domingues@inria.fr,Planning in entropy-regularized Markov decision processes and games
neurips,2019,2,6711,Pierre,Menard,inria,Inria,pierre.menard@inria.fr,Planning in entropy-regularized Markov decision processes and games
neurips,2019,3,6711,Remi,Munos,google,DeepMind,munos@google.com,Planning in entropy-regularized Markov decision processes and games
neurips,2019,4,6711,Michal,Valko,deepmind,DeepMind Paris and Inria Lille - Nord Europe,valkom@deepmind.com,Planning in entropy-regularized Markov decision processes and games
neurips,2019,0,4378,Sergul,Aydore,gmail,Stevens Institute of Technology,sergulaydore@gmail.com,Dynamic Local Regret for Non-convex Online Forecasting
neurips,2019,1,4378,Tianhao,Zhu,gmail,Stevens Institute of Techonlogy,romeo.zhuth@gmail.com,Dynamic Local Regret for Non-convex Online Forecasting
neurips,2019,2,4378,Dean,Foster,amazon,Amazon,foster@amazon.com,Dynamic Local Regret for Non-convex Online Forecasting
neurips,2019,0,6005,Yukai,Liu,caltech,Caltech,yukai@caltech.edu,NAOMI: Non-Autoregressive Multiresolution Sequence Imputation
neurips,2019,1,6005,Rose,Yu,northeastern,Northeastern University,roseyu@northeastern.edu,NAOMI: Non-Autoregressive Multiresolution Sequence Imputation
neurips,2019,2,6005,Stephan,Zheng,salesforce,Salesforce,stephan.zheng@salesforce.com,NAOMI: Non-Autoregressive Multiresolution Sequence Imputation
neurips,2019,3,6005,Eric,Zhan,caltech,Caltech,ezhan@caltech.edu,NAOMI: Non-Autoregressive Multiresolution Sequence Imputation
neurips,2019,4,6005,Yisong,Yue,caltech,Caltech,yyue@caltech.edu,NAOMI: Non-Autoregressive Multiresolution Sequence Imputation
neurips,2019,0,4927,Kevin,Ellis,,MIT,,"Write, Execute, Assess: Program Synthesis with a REPL"
neurips,2019,1,4927,Maxwell,Nye,,MIT,,"Write, Execute, Assess: Program Synthesis with a REPL"
neurips,2019,2,4927,Yewen,Pu,,MIT,,"Write, Execute, Assess: Program Synthesis with a REPL"
neurips,2019,3,4927,Felix,Sosa,,"Harvard and Center for Brains, Minds, and Machines",,"Write, Execute, Assess: Program Synthesis with a REPL"
neurips,2019,4,4927,Josh,Tenenbaum,,MIT,,"Write, Execute, Assess: Program Synthesis with a REPL"
neurips,2019,5,4927,Armando,Solar-Lezama,,MIT,,"Write, Execute, Assess: Program Synthesis with a REPL"
neurips,2019,0,1926,Yaniv,Romano,,Stanford University,,Conformalized Quantile Regression
neurips,2019,1,1926,Evan,Patterson,,Stanford University,,Conformalized Quantile Regression
neurips,2019,2,1926,Emmanuel,Candes,,Stanford University,,Conformalized Quantile Regression
neurips,2019,0,6646,Mark,Rowland,google,DeepMind,markrowland@google.com,Multiagent Evaluation under Incomplete Information
neurips,2019,1,6646,Shayegan,Omidshafiei,google,DeepMind,somidshafiei@google.com,Multiagent Evaluation under Incomplete Information
neurips,2019,2,6646,Karl,Tuyls,google,DeepMind,karltuyls@google.com,Multiagent Evaluation under Incomplete Information
neurips,2019,3,6646,Julien,Perolat,google,DeepMind,perolat@google.com,Multiagent Evaluation under Incomplete Information
neurips,2019,4,6646,Michal,Valko,deepmind,DeepMind Paris and Inria Lille - Nord Europe,valkom@deepmind.com,Multiagent Evaluation under Incomplete Information
neurips,2019,5,6646,Georgios,Piliouras,sutd,Singapore University of Technology and Design,georgios@sutd.edu.sg,Multiagent Evaluation under Incomplete Information
neurips,2019,6,6646,Remi,Munos,google,DeepMind,munos@google.com,Multiagent Evaluation under Incomplete Information
neurips,2019,0,1410,Zhe,Wang,,Ohio State University,,SpiderBoost and Momentum: Faster Variance Reduction Algorithms
neurips,2019,1,1410,Kaiyi,Ji,,The Ohio State University,,SpiderBoost and Momentum: Faster Variance Reduction Algorithms
neurips,2019,2,1410,Yi,Zhou,,University of Utah,,SpiderBoost and Momentum: Faster Variance Reduction Algorithms
neurips,2019,3,1410,Yingbin,Liang,,The Ohio State University,,SpiderBoost and Momentum: Faster Variance Reduction Algorithms
neurips,2019,4,1410,Vahid,Tarokh,,Duke University,,SpiderBoost and Momentum: Faster Variance Reduction Algorithms
neurips,2019,0,3094,Zhilin,Yang,cmu,Recurrent AI,zhiliny@cs.cmu.edu,Mixtape: Breaking the Softmax Bottleneck Efficiently
neurips,2019,1,3094,Thang,Luong,cmu,Google Brain,rsalakhu@cs.cmu.edu,Mixtape: Breaking the Softmax Bottleneck Efficiently
neurips,2019,2,3094,Russ,Salakhutdinov,google,Carnegie Mellon University,thangluong@google.com,Mixtape: Breaking the Softmax Bottleneck Efficiently
neurips,2019,3,3094,Quoc,Le,google,Google,qvl@google.com,Mixtape: Breaking the Softmax Bottleneck Efficiently
neurips,2019,0,5785,Jonathan,Lacotte,,Stanford University,,High-Dimensional Optimization in Adaptive Random Subspaces
neurips,2019,1,5785,Mert,Pilanci,,Stanford,,High-Dimensional Optimization in Adaptive Random Subspaces
neurips,2019,2,5785,Marco,Pavone,,Stanford University,,High-Dimensional Optimization in Adaptive Random Subspaces
neurips,2019,0,8147,Caroline,Haimerl,,New York University,,Flexible information routing in neural populations through stochastic comodulation
neurips,2019,1,8147,Cristina,Savin,,NYU,,Flexible information routing in neural populations through stochastic comodulation
neurips,2019,2,8147,Eero,Simoncelli,,HHMI / New York University,,Flexible information routing in neural populations through stochastic comodulation
neurips,2019,0,5512,Jinhao,Dong,xidian,Xidian University,jhdong@stu.xidian.edu.cn,MarginGAN: Adversarial Training in Semi-Supervised Learning
neurips,2019,1,5512,Tong,Lin,pku,Peking University,lintong@pku.edu.cn,MarginGAN: Adversarial Training in Semi-Supervised Learning
neurips,2019,0,7431,Chhavi,Yadav,nyu,NYU,chhavi@nyu.edu,Cold Case: The Lost MNIST Digits
neurips,2019,1,7431,Leon,Bottou,bottou,Facebook AI Research,leon@bottou.org,Cold Case: The Lost MNIST Digits
neurips,2019,0,462,Remi,Cadene,lip6,Sorbonne University - LIP6,remi.cadene@lip6.fr,RUBi: Reducing Unimodal Biases for Visual Question Answering
neurips,2019,1,462,Corentin,Dancette,lip6,Sorbonne Université,corentin.dancette@lip6.fr,RUBi: Reducing Unimodal Biases for Visual Question Answering
neurips,2019,2,462,Hedi,Ben younes,lip6,Université Pierre & Marie Curie / Heuritech,hedi.ben-younes@lip6.fr,RUBi: Reducing Unimodal Biases for Visual Question Answering
neurips,2019,3,462,Matthieu,Cord,lip6,Sorbonne University,matthieu.cord@lip6.fr,RUBi: Reducing Unimodal Biases for Visual Question Answering
neurips,2019,4,462,Devi,Parikh,gatech,Georgia Tech / Facebook AI Research (FAIR),parkih@gatech.edu,RUBi: Reducing Unimodal Biases for Visual Question Answering
neurips,2019,0,8735,Ruiyi,Zhang,,Duke University,,Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement Learning
neurips,2019,1,8735,Tong,Yu,,Samsung Research America,,Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement Learning
neurips,2019,2,8735,Yilin,Shen,,Samsung Research America,,Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement Learning
neurips,2019,3,8735,Hongxia,Jin,,Samsung Research America,,Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement Learning
neurips,2019,4,8735,Changyou,Chen,,University at Buffalo,,Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement Learning
neurips,2019,0,7160,Andrea,Celli,,Politecnico di Milano,,Learning to Correlate in Multi-Player General-Sum Sequential Games
neurips,2019,1,7160,Alberto,Marchesi,,Politecnico di Milano,,Learning to Correlate in Multi-Player General-Sum Sequential Games
neurips,2019,2,7160,Tommaso,Bianchi,,Politecnico di Milano,,Learning to Correlate in Multi-Player General-Sum Sequential Games
neurips,2019,3,7160,Nicola,Gatti,,Politecnico di Milano,,Learning to Correlate in Multi-Player General-Sum Sequential Games
neurips,2019,0,1932,Ben,Lengerich,cmu,Carnegie Mellon University,blengeri@cs.cmu.edu,Learning Sample-Specific Models with Low-Rank Personalized Regression
neurips,2019,1,1932,Bryon,Aragam,chicagobooth,University of Chicago,bryon@chicagobooth.edu,Learning Sample-Specific Models with Low-Rank Personalized Regression
neurips,2019,2,1932,Eric,Xing,cmu,Petuum Inc. /  Carnegie Mellon University,epxing@cs.cmu.edu,Learning Sample-Specific Models with Low-Rank Personalized Regression
neurips,2019,0,9003,Rodrigo,Toro Icarte,,University of Toronto and Vector Institute,,Learning Reward Machines for Partially Observable Reinforcement Learning
neurips,2019,1,9003,Ethan,Waldie,,University of Toronto & Palantir Technologies,,Learning Reward Machines for Partially Observable Reinforcement Learning
neurips,2019,2,9003,Toryn,Klassen,,University of Toronto,,Learning Reward Machines for Partially Observable Reinforcement Learning
neurips,2019,3,9003,Rick,Valenzano,,Element AI,,Learning Reward Machines for Partially Observable Reinforcement Learning
neurips,2019,4,9003,Margarita,Castro,,University of Toronto,,Learning Reward Machines for Partially Observable Reinforcement Learning
neurips,2019,5,9003,Sheila,McIlraith,,University of Toronto,,Learning Reward Machines for Partially Observable Reinforcement Learning
neurips,2019,0,3113,Himanshu,Sahni,,Georgia Institute of Technology,,Addressing Sample Complexity in Visual Tasks Using HER and Hallucinatory GANs
neurips,2019,1,3113,Toby,Buckley,,Offworld Inc.,,Addressing Sample Complexity in Visual Tasks Using HER and Hallucinatory GANs
neurips,2019,2,3113,Pieter,Abbeel,,"University of California, Berkley & OpenAI",,Addressing Sample Complexity in Visual Tasks Using HER and Hallucinatory GANs
neurips,2019,3,3113,Ilya,Kuzovkin,,Offworld Inc.,,Addressing Sample Complexity in Visual Tasks Using HER and Hallucinatory GANs
neurips,2019,0,2032,Gunpil,Hwang,kaist,KAIST,gphwang@kaist.ac.kr,Bat-G net: Bat-inspired High-Resolution 3D Image Reconstruction using Ultrasonic Echoes
neurips,2019,1,2032,Seohyeon,Kim,kaist,KAIST,dddokman@kaist.ac.kr,Bat-G net: Bat-inspired High-Resolution 3D Image Reconstruction using Ultrasonic Echoes
neurips,2019,2,2032,Hyeon-Min,Bae,kaist,KAIST,hmbae@kaist.ac.kr,Bat-G net: Bat-inspired High-Resolution 3D Image Reconstruction using Ultrasonic Echoes
neurips,2019,0,4775,Robert,Kleinberg,cornell,Cornell University,rdk@cs.cornell.edu,"Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration"
neurips,2019,1,4775,Kevin,Leyton-Brown,microsoft,University of British Columbia,brlucier@microsoft.com,"Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration"
neurips,2019,2,4775,Brendan,Lucier,ubc,Microsoft Research,kevinlb@cs.ubc.ca,"Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration"
neurips,2019,3,4775,Devon,Graham,ubc,University of British Columbia,drgraham@cs.ubc.ca,"Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration"
neurips,2019,0,2607,Jean-Yves,Franceschi,lip6,Sorbonne Université,jean-yves.franceschi@lip6.fr,Unsupervised Scalable Representation Learning for Multivariate Time Series
neurips,2019,1,2607,Aymeric,Dieuleveut,polytechnique,"Ecole Polytechnique, IPParis",aymeric.dieuleveut@polytechnique.edu,Unsupervised Scalable Representation Learning for Multivariate Time Series
neurips,2019,2,2607,Martin,Jaggi,epfl,EPFL,martin.jaggi@epfl.ch,Unsupervised Scalable Representation Learning for Multivariate Time Series
neurips,2019,0,522,Natalia,Neverova,fb,Facebook AI Research,nneverova@fb.com,Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels
neurips,2019,1,522,David,Novotny,fb,Facebook AI Research,dnovotny@fb.com,Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels
neurips,2019,2,522,Andrea,Vedaldi,fb,University of Oxford / Facebook AI Research,vedaldi@fb.com,Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels
neurips,2019,0,1439,Huaian,Diao,,Northeast Normal University,,Total Least Squares Regression in Input Sparsity Time
neurips,2019,1,1439,Zhao,Song,,Harvard University & University of Washington,,Total Least Squares Regression in Input Sparsity Time
neurips,2019,2,1439,David,Woodruff,,Carnegie Mellon University,,Total Least Squares Regression in Input Sparsity Time
neurips,2019,3,1439,Xin,Yang,,University of Washington,,Total Least Squares Regression in Input Sparsity Time
neurips,2019,0,3427,Martin,Trapp,tugraz,Graz University of Technology,martin.trapp@tugraz.at,Bayesian Learning of Sum-Product Networks
neurips,2019,1,3427,Robert,Peharz,cam,University of Cambridge,rp587@cam.ac.uk,Bayesian Learning of Sum-Product Networks
neurips,2019,2,3427,Hong,Ge,cam,University of Cambridge,hg344@cam.ac.uk,Bayesian Learning of Sum-Product Networks
neurips,2019,3,3427,Franz,Pernkopf,tugraz,"Signal Processing and Speech Communication Laboratory, Graz, Austria",pernkopf@tugraz.at,Bayesian Learning of Sum-Product Networks
neurips,2019,4,3427,Zoubin,Ghahramani,cam,Uber and University of Cambridge,zoubin@eng.cam.ac.uk,Bayesian Learning of Sum-Product Networks
neurips,2019,0,91,Tam,Nguyen,,Freiburg Computer Vision Lab,,DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision
neurips,2019,1,91,Maximilian,Dax,,Bosch GmbH,,DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision
neurips,2019,2,91,Chaithanya Kumar,Mummadi,,Bosch Center for Artificial Intelligence,,DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision
neurips,2019,3,91,Nhung,Ngo,,Bosch Center for Artificial Intelligence,,DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision
neurips,2019,4,91,Thi Hoai Phuong,Nguyen,,Karlsruhe Institute of Technology (KIT),,DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision
neurips,2019,5,91,Zhongyu,Lou,,Robert Bosch Gmbh,,DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision
neurips,2019,6,91,Thomas,Brox,,University of Freiburg,,DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision
neurips,2019,0,6209,Kaiqing,Zhang,illinois,University of Illinois at Urbana-Champaign (UIUC),kzhang66@illinois.edu,Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games
neurips,2019,1,6209,Zhuoran,Yang,princeton,Princeton University,zy6@princeton.edu,Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games
neurips,2019,2,6209,Tamer,Basar,illinois,,basar1@illinois.edu,Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games
neurips,2019,0,3568,Gilad,Yehudai,weizmann,Weizmann Institute of Science,gilad.yehudai@weizmann.ac.il,On the Power and Limitations of Random Features for Understanding Neural Networks
neurips,2019,1,3568,Ohad,Shamir,weizmann,Weizmann Institute of Science,ohad.shamir@weizmann.ac.il,On the Power and Limitations of Random Features for Understanding Neural Networks
neurips,2019,0,1748,Simon,Ramstedt,gmail,Mila,simonramstedt@gmail.com,Real-Time Reinforcement Learning
neurips,2019,1,1748,Chris,Pal,polymtl,"Montreal Institute for Learning Algorithms, École Polytechnique, Université de Montréal",christopher.pal@polymtl.ca,Real-Time Reinforcement Learning
neurips,2019,0,3671,Iryna,Korshunova,ugent,Ghent University,iryna.korshunova@ugent.be,Discriminative Topic Modeling with Logistic LDA
neurips,2019,1,3671,Hanchen,Xiong,twitter,Twitter,mfedoryszak@twitter.com,Discriminative Topic Modeling with Logistic LDA
neurips,2019,2,3671,Mateusz,Fedoryszak,twitter,Twitter,hxiong@twitter.com,Discriminative Topic Modeling with Logistic LDA
neurips,2019,3,3671,Lucas,Theis,twitter,Twitter,ltheis@twitter.com,Discriminative Topic Modeling with Logistic LDA
neurips,2019,0,6954,Edoardo,Manino,soton,University of Southampton,E.Manino@soton.ac.uk,Streaming Bayesian Inference for Crowdsourced Classification
neurips,2019,1,6954,Long,Tran-Thanh,soton,University of Southampton,l.tran-thanh@soton.ac.uk,Streaming Bayesian Inference for Crowdsourced Classification
neurips,2019,2,6954,Nicholas,Jennings,imperial,"Imperial College, London",n.jennings@imperial.ac.uk,Streaming Bayesian Inference for Crowdsourced Classification
neurips,2019,0,2530,Charles,Marx,haverford,Haverford College,cmarx@haverford.edu,Disentangling Influence: Using disentangled representations to audit model predictions
neurips,2019,1,2530,Richard,Phillips,cornell,Cornell University,rlp246@cornell.edu,Disentangling Influence: Using disentangled representations to audit model predictions
neurips,2019,2,2530,Sorelle,Friedler,haverford,Haverford College,sorelle@cs.haverford.edu,Disentangling Influence: Using disentangled representations to audit model predictions
neurips,2019,3,2530,Carlos,Scheidegger,arizona,The University of Arizona,cscheid@cs.arizona.edu,Disentangling Influence: Using disentangled representations to audit model predictions
neurips,2019,4,2530,Suresh,Venkatasubramanian,utah,University of Utah,suresh@cs.utah.edu,Disentangling Influence: Using disentangled representations to audit model predictions
neurips,2019,0,1429,Lisha,Chen,rpi,Rensselaer Polytechnic Institute,chenl21@rpi.edu,Deep Structured Prediction for Facial Landmark Detection
neurips,2019,1,1429,Hui,Su,ibm,IBM,huisuibmres@us.ibm.com,Deep Structured Prediction for Facial Landmark Detection
neurips,2019,2,1429,Qiang,Ji,rpi,Rensselaer Polytechnic Institute,jiq@rpi.edu,Deep Structured Prediction for Facial Landmark Detection
neurips,2019,0,2805,Ifigeneia,Apostolopoulou,cmu,Carnegie Mellon University,iapostol@andrew.cmu.edu,Mutually Regressive Point Processes
neurips,2019,1,2805,Scott,Linderman,cmu,Stanford University,mille856@andrew.cmu.edu,Mutually Regressive Point Processes
neurips,2019,2,2805,Kyle,Miller,stanford,Carnegie Mellon University,scott.linderman@stanford.edu,Mutually Regressive Point Processes
neurips,2019,3,2805,Artur,Dubrawski,cmu,Carnegie Mellon University,awd@cs.cmu.edu,Mutually Regressive Point Processes
neurips,2019,0,6036,Ahmed,Alaa,,UCLA,,Demystifying Black-box Models with Symbolic Metamodels
neurips,2019,1,6036,Mihaela,van der Schaar,,"University of Cambridge, Alan Turing Institute and UCLA",,Demystifying Black-box Models with Symbolic Metamodels
neurips,2019,0,5299,Qian,Lou,iu,Indiana University Bloomington,louqian@iu.edu,SHE: A Fast and Accurate Deep Neural Network for Encrypted Data
neurips,2019,1,5299,Lei,Jiang,iu,Indiana University Bloomington,jiang60@iu.edu,SHE: A Fast and Accurate Deep Neural Network for Encrypted Data
neurips,2019,0,5045,Xiangyuan,Zhang,illinois,University of Illinois at Urbana-Champaign,xz7@illinois.edu,Non-Cooperative Inverse Reinforcement Learning
neurips,2019,1,5045,Kaiqing,Zhang,illinois,University of Illinois at Urbana-Champaign (UIUC),kzhang66@illinois.edu,Non-Cooperative Inverse Reinforcement Learning
neurips,2019,2,5045,Erik,Miehling,illinois,University of Illinois at Urbana-Champaign,miehling@illinois.edu,Non-Cooperative Inverse Reinforcement Learning
neurips,2019,3,5045,Tamer,Basar,illinois,,basar1@illinois.edu,Non-Cooperative Inverse Reinforcement Learning
neurips,2019,0,4162,Florian,Schaefer,caltech,Caltech,florian.schaefer@caltech.edu,Competitive Gradient Descent
neurips,2019,1,4162,Anima,Anandkumar,caltech,NVIDIA / Caltech,anima@caltech.edu,Competitive Gradient Descent
neurips,2019,0,2821,Zhengyuan,Zhou,,Stanford University,,Learning in Generalized  Linear Contextual Bandits with Stochastic Delays
neurips,2019,1,2821,Renyuan,Xu,,University of Oxford,,Learning in Generalized  Linear Contextual Bandits with Stochastic Delays
neurips,2019,2,2821,Jose,Blanchet,,Stanford University,,Learning in Generalized  Linear Contextual Bandits with Stochastic Delays
neurips,2019,0,1893,Jianchun,Chen,nyu,New York University,jc7009@nyu.edu,Arbicon-Net: Arbitrary Continuous Geometric Transformation Networks for Image Registration
neurips,2019,1,1893,Lingjing,Wang,nyu,New York University,xl845@nyu.edu,Arbicon-Net: Arbitrary Continuous Geometric Transformation Networks for Image Registration
neurips,2019,2,1893,Xiang,Li,nyu,New York University,lw1474@nyu.edu,Arbicon-Net: Arbitrary Continuous Geometric Transformation Networks for Image Registration
neurips,2019,3,1893,Yi,Fang,nyu,New York University,yfang@nyu.edu,Arbicon-Net: Arbitrary Continuous Geometric Transformation Networks for Image Registration
neurips,2019,0,1475,Chenri,Ni,u-tokyo,The University of Tokyo,nichenri@ms.k.u-tokyo.ac.jp,On the Calibration of Multiclass Classification  with Rejection
neurips,2019,1,1475,Nontawat,Charoenphakdee,u-tokyo,The University of Tokyo / RIKEN,nontawat@ms.k.u-tokyo.ac.jp,On the Calibration of Multiclass Classification  with Rejection
neurips,2019,2,1475,Junya,Honda,u-tokyo,The University of Tokyo / RIKEN,jhonda@k.u-tokyo.ac.jp,On the Calibration of Multiclass Classification  with Rejection
neurips,2019,3,1475,Masashi,Sugiyama,u-tokyo,RIKEN / University of Tokyo,sugi@k.u-tokyo.ac.jp,On the Calibration of Multiclass Classification  with Rejection
neurips,2019,0,539,Zhijian,Liu,,MIT,,Point-Voxel CNN for Efficient 3D Deep Learning
neurips,2019,1,539,Haotian,Tang,,Shanghai Jiao Tong University,,Point-Voxel CNN for Efficient 3D Deep Learning
neurips,2019,2,539,Yujun,Lin,,MIT,,Point-Voxel CNN for Efficient 3D Deep Learning
neurips,2019,3,539,Song,Han,,MIT,,Point-Voxel CNN for Efficient 3D Deep Learning
neurips,2019,0,322,Artem,Sobolev,,Samsung AI Center Moscow,,Importance Weighted Hierarchical Variational Inference
neurips,2019,1,322,Dmitry,Vetrov,,"Higher School of Economics, Samsung AI Center, Moscow",,Importance Weighted Hierarchical Variational Inference
neurips,2019,0,4516,Frederic,Koehler,mit,MIT,fkoehler@mit.edu,Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay
neurips,2019,0,3908,Xiangyi,Chen,,University of Minnesota,,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization
neurips,2019,1,3908,Sijia,Liu,,"MIT-IBM Watson AI Lab, IBM Research AI",,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization
neurips,2019,2,3908,Kaidi,Xu,,Northeastern University,,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization
neurips,2019,3,3908,Xingguo,Li,,Princeton University,,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization
neurips,2019,4,3908,Xue,Lin,,Northeastern University,,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization
neurips,2019,5,3908,Mingyi,Hong,,University of Minnesota,,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization
neurips,2019,6,3908,David,Cox,,MIT-IBM Watson AI Lab,,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization
neurips,2019,0,2469,Mathias,Perslev,ku,University of Copenhagen,map@di.ku.dk,U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging
neurips,2019,1,2469,Michael,Jensen,gmail,University of Copehagen,mhejselbak@gmail.com,U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging
neurips,2019,2,2469,Sune,Darkner,ku,"University of Copenhagen, Denmark",darkner@di.ku.dk,U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging
neurips,2019,3,2469,Poul Jørgen,Jennum,regionh,"Danish Center for Sleep Medicine, Rigshospitalet",poul.joergen.jennum@regionh.dk,U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging
neurips,2019,4,2469,Christian,Igel,diku,University of Copenhagen,igel@diku.dk,U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging
neurips,2019,0,1842,Eunbyung,Park,unc,UNC Chapel Hill / Nuro,eunbyung@cs.unc.edu,Meta-Curvature
neurips,2019,1,1842,Junier,Oliva,unc,UNC - Chapel Hill,joliva@cs.unc.edu,Meta-Curvature
neurips,2019,0,7458,Zhizhou,Ren,tsinghua,Tsinghua University,rzz16@mails.tsinghua.edu.cn,Exploration via Hindsight Goal Generation
neurips,2019,1,7458,Kefan,Dong,tsinghua,Tsinghua University,dkf16@mails.tsinghua.edu.cn,Exploration via Hindsight Goal Generation
neurips,2019,2,7458,Yuan,Zhou,illinois,UIUC,yuanz@illinois.edu,Exploration via Hindsight Goal Generation
neurips,2019,3,7458,Qiang,Liu,utexas,UT Austin,lqiang@cs.utexas.edu,Exploration via Hindsight Goal Generation
neurips,2019,4,7458,Jian,Peng,illinois,University of Illinois at Urbana-Champaign,jianpeng@illinois.edu,Exploration via Hindsight Goal Generation
neurips,2019,0,3852,Matthew,Fellows,,University of Oxford,,VIREL: A Variational Inference Framework for Reinforcement Learning
neurips,2019,1,3852,Anuj,Mahajan,,University of Oxford,,VIREL: A Variational Inference Framework for Reinforcement Learning
neurips,2019,2,3852,Tim G. J.,Rudner,,University of Oxford,,VIREL: A Variational Inference Framework for Reinforcement Learning
neurips,2019,3,3852,Shimon,Whiteson,,University of Oxford,,VIREL: A Variational Inference Framework for Reinforcement Learning
neurips,2019,0,4827,Zeyuan,Allen-Zhu,mit,Microsoft Research,zeyuan@csail.mit.edu,"What Can ResNet Learn Efficiently, Going Beyond Kernels?"
neurips,2019,1,4827,Yuanzhi,Li,cmu,Princeton,yuanzhil@andrew.cmu.edu,"What Can ResNet Learn Efficiently, Going Beyond Kernels?"
neurips,2019,0,4015,Clarice,Poon,bath,University of Bath,cmhsp20@bath.ac.uk,Trajectory of Alternating Direction Method of Multipliers and Adaptive Acceleration
neurips,2019,1,4015,Jingwei,Liang,cam,University of Cambridge,jl993@cam.ac.uk,Trajectory of Alternating Direction Method of Multipliers and Adaptive Acceleration
neurips,2019,0,192,Tatjana,Chavdarova,,DeepMind & Mila & Idiap & EPFL,,Reducing Noise in GAN Training with Variance Reduced Extragradient
neurips,2019,1,192,Gauthier,Gidel,,Mila,,Reducing Noise in GAN Training with Variance Reduced Extragradient
neurips,2019,2,192,François,Fleuret,,Idiap,,Reducing Noise in GAN Training with Variance Reduced Extragradient
neurips,2019,3,192,Simon,Lacoste-Julien,,"Mila, Université de Montréal",,Reducing Noise in GAN Training with Variance Reduced Extragradient
neurips,2019,0,2993,Yiren,Zhao,,University of Cambridge,,Focused Quantization for Sparse CNNs
neurips,2019,1,2993,Xitong,Gao,,"Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",,Focused Quantization for Sparse CNNs
neurips,2019,2,2993,Daniel,Bates,,University of Cambridge,,Focused Quantization for Sparse CNNs
neurips,2019,3,2993,Robert,Mullins,,University of Cambridge,,Focused Quantization for Sparse CNNs
neurips,2019,4,2993,Cheng-Zhong,Xu,,University of Macau,,Focused Quantization for Sparse CNNs
neurips,2019,0,6508,Shinji,Ito,nec,"NEC Corporation,      University of Tokyo",i-shinji@nec.com,Submodular Function Minimization with Noisy Evaluation Oracle
neurips,2019,0,1557,Jaemin,Yoo,snu,Seoul National University,jaeminyoo@snu.ac.kr,Knowledge Extraction with No Observable Data
neurips,2019,1,1557,Minyong,Cho,snu,Seoul National University,k.taebum@snu.ac.kr,Knowledge Extraction with No Observable Data
neurips,2019,2,1557,Taebum,Kim,gmail,Seoul National University,chominyong@gmail.com,Knowledge Extraction with No Observable Data
neurips,2019,3,1557,U,Kang,snu,Seoul National University,ukang@snu.ac.kr,Knowledge Extraction with No Observable Data
neurips,2019,0,6150,Paul,Hand,northeastern,Northeastern University,p.hand@northeastern.edu,Global Guarantees for Blind Demodulation with Generative Priors
neurips,2019,1,6150,Babhru,Joshi,ubc,University of British Columbia,b.joshi@math.ubc.ca,Global Guarantees for Blind Demodulation with Generative Priors
neurips,2019,0,5208,Junteng,Jia,cornell,Cornell,jj585@cornell.edu,Neural Jump Stochastic Differential Equations
neurips,2019,1,5208,Austin,Benson,cornell,Cornell University,arb@cs.cornell.edu,Neural Jump Stochastic Differential Equations
neurips,2019,0,1845,Nathan,Kallus,cornell,Cornell University,kallus@cornell.edu,"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning"
neurips,2019,1,1845,Masatoshi,Uehara,harvard,Harvard University,uehara_m@g.harvard.edu,"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning"
neurips,2019,0,7615,Mohamed,Belghazi,,Facebook AI Research,,Learning about an exponential amount of conditional distributions
neurips,2019,1,7615,Maxime,Oquab,,Facebook AI Research,,Learning about an exponential amount of conditional distributions
neurips,2019,2,7615,David,Lopez-Paz,,Facebook AI Research,,Learning about an exponential amount of conditional distributions
neurips,2019,0,1710,Xiaoming,Yu,pku,Peking University,xiaomingyu@pku.edu.cn,Multi-mapping Image-to-Image Translation via Learning Disentanglement
neurips,2019,1,1710,Yuanqi,Chen,pku,"SECE, Peking University",cyq373@pku.edu.cn,Multi-mapping Image-to-Image Translation via Learning Disentanglement
neurips,2019,2,1710,Shan,Liu,aiit,Tencent,tli@aiit.org.cn,Multi-mapping Image-to-Image Translation via Learning Disentanglement
neurips,2019,3,1710,Thomas,Li,tencent,"Shenzhen Graduate School, Peking University",shanl@tencent.com,Multi-mapping Image-to-Image Translation via Learning Disentanglement
neurips,2019,4,1710,Ge,Li,pku,"SECE, Shenzhen Graduate School, Peking University",geli@ece.pku.edu.cn,Multi-mapping Image-to-Image Translation via Learning Disentanglement
neurips,2019,0,8097,Miika,Aittala,,MIT CSAIL / NVIDIA,,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization
neurips,2019,1,8097,Prafull,Sharma,,MIT,,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization
neurips,2019,2,8097,Lukas,Murmann,,Massachusetts Institute of Technology,,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization
neurips,2019,3,8097,Adam,Yedidia,,Massachusetts Institute of Technology,,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization
neurips,2019,4,8097,Gregory,Wornell,,MIT,,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization
neurips,2019,5,8097,Bill,Freeman,,MIT/Google,,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization
neurips,2019,6,8097,Fredo,Durand,,MIT,,Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization
neurips,2019,0,8930,Tristan,Bepler,mit,MIT,tbepler@mit.edu,Explicitly disentangling image content from translation and rotation with spatial-VAE
neurips,2019,1,8930,Ellen,Zhong,nysbc,Massachusetts Institute of Technology,kkelley@nysbc.org,Explicitly disentangling image content from translation and rotation with spatial-VAE
neurips,2019,2,8930,Kotaro,Kelley,mit,New York Structural Biology Center,zhonge@mit.edu,Explicitly disentangling image content from translation and rotation with spatial-VAE
neurips,2019,3,8930,Edward,Brignole,mit,Massachusetts Institute of Technology,brignole@mit.edu,Explicitly disentangling image content from translation and rotation with spatial-VAE
neurips,2019,4,8930,Bonnie,Berger,mit,MIT,bab@mit.edu,Explicitly disentangling image content from translation and rotation with spatial-VAE
neurips,2019,0,9215,Abhinav,Verma,rice,Rice University,averma@rice.edu,Imitation-Projected Programmatic Reinforcement Learning
neurips,2019,1,9215,Hoang,Le,caltech,California Institute of Technology,hmle@caltech.edu,Imitation-Projected Programmatic Reinforcement Learning
neurips,2019,2,9215,Yisong,Yue,caltech,Caltech,yyue@caltech.edu,Imitation-Projected Programmatic Reinforcement Learning
neurips,2019,3,9215,Swarat,Chaudhuri,rice,Rice University,swarat@rice.edu,Imitation-Projected Programmatic Reinforcement Learning
neurips,2019,0,2668,Basri,Ronen,,Weizmann Inst.,,The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies
neurips,2019,1,2668,David,Jacobs,,"University of Maryland, USA",,The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies
neurips,2019,2,2668,Yoni,Kasten,,Weizmann Institute,,The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies
neurips,2019,3,2668,Shira,Kritchman,,Weizmann Institute,,The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies
neurips,2019,0,2555,Gonzalo,Mena,,Harvard,,Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem
neurips,2019,1,2555,Jonathan,Niles-Weed,,NYU,,Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem
neurips,2019,0,5315,Shiyu,Chang,ibm,IBM T.J. Watson Research Center,shiyu.chang@ibm.com,A Game Theoretic Approach to Class-wise Selective Rationalization
neurips,2019,1,5315,Yang,Zhang,ibm,MIT-IBM Watson AI Lab,yang.zhang2@ibm.com,A Game Theoretic Approach to Class-wise Selective Rationalization
neurips,2019,2,5315,Mo,Yu,ibm,IBM Research,yum@us.ibm.com,A Game Theoretic Approach to Class-wise Selective Rationalization
neurips,2019,3,5315,Tommi,Jaakkola,mit,MIT,tommi@csail.mit.edu,A Game Theoretic Approach to Class-wise Selective Rationalization
neurips,2019,0,2579,Creighton,Heaukulani,gmail,No Affiliation,c.k.heaukulani@gmail.com,Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes
neurips,2019,1,2579,Mark,van der Wilk,prowler,PROWLER.io,mark@prowler.io,Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes
neurips,2019,0,3446,Tomasz,Kumierczyk,helsinki,University of Helsinki,tomasz.kusmierczyk@helsinki.fi,Variational Bayesian Decision-making for Continuous Utilities
neurips,2019,1,3446,Joseph,Sakaya,helsinki,University of Helsinki,joseph.sakaya@helsinki.fi,Variational Bayesian Decision-making for Continuous Utilities
neurips,2019,2,3446,Arto,Klami,helsinki,University of Helsinki,arto.klami@helsinki.fi,Variational Bayesian Decision-making for Continuous Utilities
neurips,2019,0,3435,zengfeng,Huang,fudan,Fudan University,huangzf@fudan.edu.cn,Optimal Sparsity-Sensitive Bounds for  Distributed Mean Estimation
neurips,2019,1,3435,Ziyue,Huang,ust,HKUST,ywanggq@cse.ust.hk,Optimal Sparsity-Sensitive Bounds for  Distributed Mean Estimation
neurips,2019,2,3435,Yilei,WANG,ust,The Hong Kong University of Science and Technology,zhuangbq@cse.ust.hk,Optimal Sparsity-Sensitive Bounds for  Distributed Mean Estimation
neurips,2019,3,3435,Ke,Yi,ust,""" Hong Kong University of Science and Technology, Hong Kong""",yike@cse.ust.hk,Optimal Sparsity-Sensitive Bounds for  Distributed Mean Estimation
neurips,2019,0,8751,Ben,Eysenbach,,Carnegie Mellon University,,Search on the Replay Buffer: Bridging Planning and Reinforcement Learning
neurips,2019,1,8751,Russ,Salakhutdinov,,Carnegie Mellon University,,Search on the Replay Buffer: Bridging Planning and Reinforcement Learning
neurips,2019,2,8751,Sergey,Levine,,UC Berkeley,,Search on the Replay Buffer: Bridging Planning and Reinforcement Learning
neurips,2019,0,8632,Bulat,Ibragimov,sberbank,Yandex Research,gusev.g.g@sberbank.ru,Minimal Variance Sampling in Stochastic Gradient Boosting
neurips,2019,1,8632,Gleb,Gusev,yandex,Sberbank,ibrbulat@yandex.ru,Minimal Variance Sampling in Stochastic Gradient Boosting
neurips,2019,0,5283,Ziyu,Wan,,City University of Hong Kong,,Transductive Zero-Shot Learning with Visual Structure Constraint
neurips,2019,1,5283,Dongdong,Chen,,university of science and technology of china,,Transductive Zero-Shot Learning with Visual Structure Constraint
neurips,2019,2,5283,Yan,Li,,"Institute of Automation, Chinese Academy of Sciences",,Transductive Zero-Shot Learning with Visual Structure Constraint
neurips,2019,3,5283,Xingguang,Yan,,Shenzhen University,,Transductive Zero-Shot Learning with Visual Structure Constraint
neurips,2019,4,5283,Junge,Zhang,,CASIA,,Transductive Zero-Shot Learning with Visual Structure Constraint
neurips,2019,5,5283,Yizhou,Yu,,Deepwise AI Lab,,Transductive Zero-Shot Learning with Visual Structure Constraint
neurips,2019,6,5283,Jing,Liao,,City University of Hong Kong,,Transductive Zero-Shot Learning with Visual Structure Constraint
neurips,2019,0,1384,Adrian,Rivera Cardoso,gatech,Georgia Tech,adrian.riv@gatech.edu,Large Scale Markov Decision Processes with Changing Rewards
neurips,2019,1,1384,He,Wang,gatech,Georgia Institute of Technology,he.wang@isye.gatech.edu,Large Scale Markov Decision Processes with Changing Rewards
neurips,2019,2,1384,Huan,Xu,alibaba-inc,Georgia Inst. of Technology,huan.xu@alibaba-inc.com,Large Scale Markov Decision Processes with Changing Rewards
neurips,2019,0,5182,Xuanqing,Liu,,"University of California, Los Angeles",,A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning
neurips,2019,1,5182,Si,Si,,Google Research,,A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning
neurips,2019,2,5182,Jerry,Zhu,,University of Wisconsin-Madison,,A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning
neurips,2019,3,5182,Yang,Li,,Google,,A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning
neurips,2019,4,5182,Cho-Jui,Hsieh,,UCLA,,A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning
neurips,2019,0,1696,Tomas,Vaskevicius,ox,University of Oxford,tomas.vaskevicius@stats.ox.ac.uk,Implicit Regularization for Optimal Sparse Recovery
neurips,2019,1,1696,Varun,Kanade,ox,University of Oxford,patrick.rebeschini@stats.ox.ac.uk,Implicit Regularization for Optimal Sparse Recovery
neurips,2019,2,1696,Patrick,Rebeschini,ox,University of Oxford,varunk@cs.ox.ac.uk,Implicit Regularization for Optimal Sparse Recovery
neurips,2019,0,5254,Ricky T. Q.,Chen,toronto,U of Toronto,rtqichen@cs.toronto.edu,Residual Flows for Invertible Generative Modeling
neurips,2019,1,5254,Jens,Behrmann,uni-bremen,University of Bremen,jensb@uni-bremen.de,Residual Flows for Invertible Generative Modeling
neurips,2019,2,5254,David,Duvenaud,toronto,University of Toronto,duvenaud@cs.toronto.edu,Residual Flows for Invertible Generative Modeling
neurips,2019,3,5254,Joern-Henrik,Jacobsen,vectorinstitute,Vector Institute,j.jacobsen@vectorinstitute.ai,Residual Flows for Invertible Generative Modeling
neurips,2019,0,3426,Weiwei,Liu,gmail,Wuhan University,liuweiwei863@gmail.com,Copula Multi-label Learning
neurips,2019,0,3146,Florian,Tramer,,Stanford University,,Adversarial Training and Robustness for Multiple Perturbations
neurips,2019,1,3146,Dan,Boneh,,Stanford University,,Adversarial Training and Robustness for Multiple Perturbations
neurips,2019,0,5363,Horia,Mania,berkeley,UC Berkeley,hmania@berkeley.edu,Certainty Equivalence is Efficient for Linear Quadratic Control
neurips,2019,1,5363,Stephen,Tu,berkeley,UC Berkeley,stephentu@berkeley.edu,Certainty Equivalence is Efficient for Linear Quadratic Control
neurips,2019,2,5363,Benjamin,Recht,berkeley,UC Berkeley,brecht@berkeley.edu,Certainty Equivalence is Efficient for Linear Quadratic Control
neurips,2019,0,4229,Dilin,Wang,utexas,UT Austin,dilin@cs.utexas.edu,Stein Variational Gradient Descent With Matrix-Valued Kernels
neurips,2019,1,4229,Ziyang,Tang,utexas,UT Austin,ztang@cs.utexas.edu,Stein Variational Gradient Descent With Matrix-Valued Kernels
neurips,2019,2,4229,Chandrajit,Bajaj,utexas,The University of Texas at Austin,bajaj@cs.utexas.edu,Stein Variational Gradient Descent With Matrix-Valued Kernels
neurips,2019,3,4229,Qiang,Liu,utexas,UT Austin,lqiang@cs.utexas.edu,Stein Variational Gradient Descent With Matrix-Valued Kernels
neurips,2019,0,2416,James,Jordon,,University of Oxford,,Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate
neurips,2019,1,2416,Jinsung,Yoon,,"University of California, Los Angeles",,Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate
neurips,2019,2,2416,Mihaela,van der Schaar,,"University of Cambridge, Alan Turing Institute and UCLA",,Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate
neurips,2019,0,9244,Pavithra,Prabhakar,,Kansas State University,,Abstraction based Output Range Analysis for Neural Networks
neurips,2019,1,9244,Zahra,Rahimi Afzal,,Kansas State University,,Abstraction based Output Range Analysis for Neural Networks
neurips,2019,0,7578,Yao,Fu,columbia,Columbia University,yao.fu@columbia.edu,Paraphrase Generation with Latent Bag of Words
neurips,2019,1,7578,Yansong,Feng,pku,Peking University,fengyansong@pku.edu.cn,Paraphrase Generation with Latent Bag of Words
neurips,2019,2,7578,John,Cunningham,columbia,University of Columbia,jpc2181@columbia.edu,Paraphrase Generation with Latent Bag of Words
neurips,2019,0,554,Aadirupa,Saha,iisc,Indian Institute of Science,aadirupa@iisc.ac.in,Combinatorial Bandits with Relative Feedback
neurips,2019,1,554,Aditya,Gopalan,iisc,Indian Institute of Science,aditya@iisc.ac.in,Combinatorial Bandits with Relative Feedback
neurips,2019,0,5275,Greg,Yang,microsoft,Microsoft Research,gregyang@microsoft.com,Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes
neurips,2019,0,537,Hadrien,Hendrikx,inria,INRIA - PSL,hadrien.hendrikx@inria.fr,An Accelerated Decentralized Stochastic Proximal Algorithm for Finite Sums
neurips,2019,1,537,Francis,Bach,inria,INRIA - Ecole Normale Superieure,francis.bach@inria.fr,An Accelerated Decentralized Stochastic Proximal Algorithm for Finite Sums
neurips,2019,2,537,Laurent,Massoulié,inria,Inria,laurent.massoulie@inria.fr,An Accelerated Decentralized Stochastic Proximal Algorithm for Finite Sums
neurips,2019,0,8058,Kristjan,Greenewald,ibm,IBM Research,kristjan.h.greenewald@ibm.com,Sample Efficient Active Learning of Causal Trees
neurips,2019,1,8058,Dmitriy,Katz,ibm,IBM Research,dkatzrog@us.ibm.com,Sample Efficient Active Learning of Causal Trees
neurips,2019,2,8058,Karthikeyan,Shanmugam,ibm,"IBM Research, NY",karthikeyan.shanmugam2@ibm.com,Sample Efficient Active Learning of Causal Trees
neurips,2019,3,8058,Sara,Magliacane,ibm,MIT-IBM Watson AI Lab,sara.magliacane@ibm.com,Sample Efficient Active Learning of Causal Trees
neurips,2019,4,8058,Murat,Kocaoglu,ibm,MIT-IBM Watson AI Lab,murat@ibm.com,Sample Efficient Active Learning of Causal Trees
neurips,2019,5,8058,Enric,Boix Adsera,mit,MIT,eboix@mit.edu,Sample Efficient Active Learning of Causal Trees
neurips,2019,6,8058,Guy,Bresler,mit,MIT,guy@mit.edu,Sample Efficient Active Learning of Causal Trees
neurips,2019,0,2373,Satoshi,Hara,,Osaka University,,Data Cleansing for Models Trained with SGD
neurips,2019,1,2373,Atsushi,Nitanda,,The University of Tokyo / RIKEN,,Data Cleansing for Models Trained with SGD
neurips,2019,2,2373,Takanori,Maehara,,RIKEN AIP,,Data Cleansing for Models Trained with SGD
neurips,2019,0,9068,Niru,Maheswaranathan,google,Google Brain,nirum@google.com,Universality and individuality in neural dynamics across large populations of recurrent networks
neurips,2019,1,9068,Alex,Williams,stanford,Stanford University,ahwillia@stanford.edu,Universality and individuality in neural dynamics across large populations of recurrent networks
neurips,2019,2,9068,Matthew,Golub,stanford,Stanford University,mgolub@stanford.edu,Universality and individuality in neural dynamics across large populations of recurrent networks
neurips,2019,3,9068,Surya,Ganguli,stanford,Stanford,sganguli@stanford.edu,Universality and individuality in neural dynamics across large populations of recurrent networks
neurips,2019,4,9068,David,Sussillo,google,Google Inc.,sussillo@google.com,Universality and individuality in neural dynamics across large populations of recurrent networks
neurips,2019,0,8437,Ali,Razavi,google,DeepMind,alirazavi@google.com,Generating Diverse High-Fidelity Images with VQ-VAE-2
neurips,2019,1,8437,Aaron,van den Oord,google,Google Deepmind,avdnoord@google.com,Generating Diverse High-Fidelity Images with VQ-VAE-2
neurips,2019,2,8437,Oriol,Vinyals,google,Google DeepMind,vinyals@google.com,Generating Diverse High-Fidelity Images with VQ-VAE-2
neurips,2019,0,6805,Michael,Janner,berkeley,UC Berkeley,janner@eecs.berkeley.edu,When to Trust Your Model: Model-Based Policy Optimization
neurips,2019,1,6805,Justin,Fu,berkeley,UC Berkeley,justinjfu@eecs.berkeley.edu,When to Trust Your Model: Model-Based Policy Optimization
neurips,2019,2,6805,Marvin,Zhang,berkeley,UC Berkeley,marvin@eecs.berkeley.edu,When to Trust Your Model: Model-Based Policy Optimization
neurips,2019,3,6805,Sergey,Levine,berkeley,UC Berkeley,svlevine@eecs.berkeley.edu,When to Trust Your Model: Model-Based Policy Optimization
neurips,2019,0,5841,Andrew,Cotter,google,Google,acotter@google.com,On Making Stochastic Classifiers Deterministic
neurips,2019,1,5841,Maya,Gupta,google,Google,hnarasimhan@google.com,On Making Stochastic Classifiers Deterministic
neurips,2019,2,5841,Harikrishna,Narasimhan,google,Google Research,mayagupta@google.com,On Making Stochastic Classifiers Deterministic
neurips,2019,0,114,Sefi,Bell-Kligler,,Weizmann Istitute of Science,,Blind Super-Resolution Kernel Estimation using an Internal-GAN
neurips,2019,1,114,Assaf,Shocher,,Weizmann Institute of Science,,Blind Super-Resolution Kernel Estimation using an Internal-GAN
neurips,2019,2,114,Michal,Irani,,Weizmann Institute of Science,,Blind Super-Resolution Kernel Estimation using an Internal-GAN
neurips,2019,0,5260,Antreas,Antoniou,ed,University of Edinburgh,a.antoniou@sms.ed.ac.uk,Learning to Learn By Self-Critique
neurips,2019,1,5260,Amos,Storkey,ed,University of Edinburgh,a.storkey@ed.ac.uk,Learning to Learn By Self-Critique
neurips,2019,0,2442,Joshua,Lee,mit,MIT,jk_lee@mit.edu,Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks
neurips,2019,1,2442,Prasanna,Sattigeri,ibm,IBM Research,psattig@us.ibm.com,Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks
neurips,2019,2,2442,Gregory,Wornell,mit,MIT,gww@mit.edu,Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks
neurips,2019,0,4163,Ulysse,Marteau-Ferey,inria,DI ENS / INRIA,ulysse.marteau-ferey@inria.fr,Globally Convergent Newton Methods for Ill-conditioned Generalized Self-concordant Losses
neurips,2019,1,4163,Francis,Bach,inria,INRIA - Ecole Normale Superieure,francis.bach@inria.fr,Globally Convergent Newton Methods for Ill-conditioned Generalized Self-concordant Losses
neurips,2019,2,4163,Alessandro,Rudi,inria,"INRIA, Ecole Normale Superieure",alessandro.rudi@inria.fr,Globally Convergent Newton Methods for Ill-conditioned Generalized Self-concordant Losses
neurips,2019,0,3466,Eran,Malach,huji,Hebrew University Jerusalem Israel,eran.malach@mail.huji.ac.il,Is Deeper Better only when Shallow is Good?
neurips,2019,1,3466,Shai,Shalev-Shwartz,huji,Mobileye & HUJI,shais@cs.huji.ac.il,Is Deeper Better only when Shallow is Good?
neurips,2019,0,3096,Hoi-To,Wai,cuhk,The Chinese University of Hong Kong,htwai@se.cuhk.edu.hk,Variance Reduced Policy Evaluation with Smooth Function Approximation
neurips,2019,1,3096,Mingyi,Hong,umn,University of Minnesota,mhong@umn.edu,Variance Reduced Policy Evaluation with Smooth Function Approximation
neurips,2019,2,3096,Zhuoran,Yang,princeton,Princeton University,zy6@princeton.edu,Variance Reduced Policy Evaluation with Smooth Function Approximation
neurips,2019,3,3096,Zhaoran,Wang,gmail,Northwestern University,zhaoranwang@gmail.com,Variance Reduced Policy Evaluation with Smooth Function Approximation
neurips,2019,4,3096,Kexin,Tang,umn,Shanghai Jiao Tong University,tangk@umn.edu,Variance Reduced Policy Evaluation with Smooth Function Approximation
neurips,2019,0,6975,Yair,Marom,gmail,University of Haifa,yairmrm@gmail.com,k-Means Clustering of Lines for Big Data
neurips,2019,1,6975,Dan,Feldman,gmail,University of Haifa,dannyf.post@gmail.com,k-Means Clustering of Lines for Big Data
neurips,2019,0,8389,Ligeng,Zhu,mit,MIT,ligeng@mit.edu,Deep Leakage from Gradients
neurips,2019,1,8389,Zhijian,Liu,mit,MIT,zhijian@mit.edu,Deep Leakage from Gradients
neurips,2019,2,8389,Song,Han,mit,MIT,songhan@mit.edu,Deep Leakage from Gradients
neurips,2019,0,2964,Amir,Najafi,sharif,Sharif University of Technology,najafy@ce.sharif.edu,Robustness to Adversarial Perturbations in Learning from Incomplete Data
neurips,2019,1,2964,Shin-ichi,Maeda,preferred,Preferred Networks,ichi@preferred.jp,Robustness to Adversarial Perturbations in Learning from Incomplete Data
neurips,2019,2,2964,Masanori,Koyama,preferred,Preferred Networks Inc.,masomatics@preferred.jp,Robustness to Adversarial Perturbations in Learning from Incomplete Data
neurips,2019,3,2964,Takeru,Miyato,preferred,"Preferred Networks, Inc.",miyato@preferred.jp,Robustness to Adversarial Perturbations in Learning from Incomplete Data
neurips,2019,0,8261,Rémy,Degenne,cwi,"Centrum Wiskunde & Informatica, Amsterdam",remy.degenne@cwi.nl,Pure Exploration with Multiple Correct Answers
neurips,2019,1,8261,Wouter,Koolen,cwi,"Centrum Wiskunde & Informatica, Amsterdam",wmkoolen@cwi.nl,Pure Exploration with Multiple Correct Answers
neurips,2019,0,4955,Gabriele,Farina,cmu,Carnegie Mellon University,gfarina@cs.cmu.edu,Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks
neurips,2019,1,4955,Chun Kai,Ling,cmu,Carnegie Mellon University,chunkail@cs.cmu.edu,Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks
neurips,2019,2,4955,Fei,Fang,cmu,Carnegie Mellon University,feif@cs.cmu.edu,Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks
neurips,2019,3,4955,Tuomas,Sandholm,cmu,"CMU, Strategic Machine, Strategy Robot, Optimized Markets",sandholm@cs.cmu.edu,Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks
neurips,2019,0,6139,Vaden,Masrani,,University of British Columbia,,The Thermodynamic Variational Objective
neurips,2019,1,6139,Tuan Anh,Le,,MIT,,The Thermodynamic Variational Objective
neurips,2019,2,6139,Frank,Wood,,University of British Columbia,,The Thermodynamic Variational Objective
neurips,2019,0,786,Edith,Cohen,cohenwang,Google,edith@cohenwang.com,Sampling Sketches for Concave Sublinear Functions of Frequencies
neurips,2019,1,786,Ofir,Geri,stanford,Stanford University,ofirgeri@cs.stanford.edu,Sampling Sketches for Concave Sublinear Functions of Frequencies
neurips,2019,0,4340,Chieh,Wu,,Northeastern University,,Solving Interpretable Kernel Dimensionality Reduction
neurips,2019,1,4340,Jared,Miller,,Northeastern University,,Solving Interpretable Kernel Dimensionality Reduction
neurips,2019,2,4340,Yale,Chang,,Northeastern University,,Solving Interpretable Kernel Dimensionality Reduction
neurips,2019,3,4340,Mario,Sznaier,,Northeastern University,,Solving Interpretable Kernel Dimensionality Reduction
neurips,2019,4,4340,Jennifer,Dy,,Northeastern University,,Solving Interpretable Kernel Dimensionality Reduction
neurips,2019,0,907,Kaidi,Cao,stanford,Stanford University,kaidicao@stanford.edu,Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss
neurips,2019,1,907,Colin,Wei,stanford,Stanford University,colinwei@stanford.edu,Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss
neurips,2019,2,907,Adrien,Gaidon,tri,Toyota Research Institute,adrien.gaidon@tri.global,Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss
neurips,2019,3,907,Nikos,Arechiga,tri,Toyota Research Institute,nikos.arechiga@tri.global,Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss
neurips,2019,4,907,Tengyu,Ma,stanford,Stanford University,tengyuma@stanford.edu,Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss
neurips,2019,0,2776,Jingjing,Wang,uwaterloo,University of Waterloo,jingjing.wang@uwaterloo.ca,Multivariate Triangular Quantile Maps for Novelty Detection
neurips,2019,1,2776,Sun,Sun,uwaterloo,National Research Council,sun.sun@uwaterloo.ca,Multivariate Triangular Quantile Maps for Novelty Detection
neurips,2019,2,2776,Yaoliang,Yu,uwaterloo,University of Waterloo,yaoliang.yu@uwaterloo.ca,Multivariate Triangular Quantile Maps for Novelty Detection
neurips,2019,0,9199,Michalis,Titsias,,DeepMind,,Gradient-based Adaptive Markov Chain Monte Carlo
neurips,2019,1,9199,Petros,Dellaportas,,"University College London, Athens University of Economics and Alan Turing Institute",,Gradient-based Adaptive Markov Chain Monte Carlo
neurips,2019,0,3315,Zeyuan,Allen-Zhu,mit,Microsoft Research,zeyuan@csail.mit.edu,"Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"
neurips,2019,1,3315,Yuanzhi,Li,cmu,Princeton,yuanzhil@andrew.cmu.edu,"Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"
neurips,2019,2,3315,Yingyu,Liang,wisc,University of Wisconsin Madison,yliang@cs.wisc.edu,"Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"
neurips,2019,0,5938,Dheeraj,Baby,ucsb,UC Santa Barbara,dheeraj@ucsb.edu,Online Forecasting of Total-Variation-bounded Sequences
neurips,2019,1,5938,Yu-Xiang,Wang,ucsb,UC Santa Barbara,yuxiangw@cs.ucsb.edu,Online Forecasting of Total-Variation-bounded Sequences
neurips,2019,0,2252,Ryoma,Sato,ist,Kyoto University,r.sato@ml.ist.i,Approximation Ratios of Graph Neural Networks for Combinatorial Problems
neurips,2019,1,2252,Makoto,Yamada,,Kyoto University/RIKEN AIP,myamada@i,Approximation Ratios of Graph Neural Networks for Combinatorial Problems
neurips,2019,2,2252,Hisashi,Kashima,kyoto-u,Kyoto University/RIKEN Center for AIP,kashima@i.kyoto-u.ac.jp,Approximation Ratios of Graph Neural Networks for Combinatorial Problems
neurips,2019,0,32,Jiawang,Bian,,The University of Adelaide,,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video
neurips,2019,1,32,Zhichao,Li,,Tusimple,,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video
neurips,2019,2,32,Naiyan,Wang,,Hong Kong University of Science and Technology,,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video
neurips,2019,3,32,Huangying,Zhan,,The University of Adelaide,,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video
neurips,2019,4,32,Chunhua,Shen,,University of Adelaide,,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video
neurips,2019,5,32,Ming-Ming,Cheng,,Nankai University,,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video
neurips,2019,6,32,Ian,Reid,,University of Adelaide,,Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video
neurips,2019,0,954,Zongsheng,Yue,,Xi'an Jiaotong University,,Variational Denoising Network: Toward Blind Noise Modeling and Removal
neurips,2019,1,954,Hongwei,Yong,,The Hong Kong Polytechnic University,,Variational Denoising Network: Toward Blind Noise Modeling and Removal
neurips,2019,2,954,Qian,Zhao,,Xi'an Jiaotong University,,Variational Denoising Network: Toward Blind Noise Modeling and Removal
neurips,2019,3,954,Deyu,Meng,,Xi'an Jiaotong University,,Variational Denoising Network: Toward Blind Noise Modeling and Removal
neurips,2019,4,954,Lei,Zhang,,The Hong Kong Polytechnic Univ,,Variational Denoising Network: Toward Blind Noise Modeling and Removal
neurips,2019,0,8621,Fariba,Yousefi,sheffield,University of Sheffield,f.yousefi@sheffield.ac.uk,Multi-task Learning for Aggregated Data using Gaussian Processes
neurips,2019,1,8621,Michael,Smith,sheffield,University of Sheffield,m.t.smith@sheffield.ac.uk,Multi-task Learning for Aggregated Data using Gaussian Processes
neurips,2019,2,8621,Mauricio,Álvarez,sheffield,University of Sheffield,mauricio.alvarez@sheffield.ac.uk,Multi-task Learning for Aggregated Data using Gaussian Processes
neurips,2019,0,5472,Alexander,Trott,salesforce,Salesforce Research,atrott@salesforce.com,Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards
neurips,2019,1,5472,Stephan,Zheng,salesforce,Salesforce,stephan.zheng@salesforce.com,Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards
neurips,2019,2,5472,Caiming,Xiong,salesforce,Salesforce,cxiong@salesforce.com,Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards
neurips,2019,3,5472,Richard,Socher,salesforce,Salesforce,rsocher@salesforce.com,Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards
neurips,2019,0,8189,Nishal,Shah,,Stanford University,,Efficient characterization of electrically evoked responses for neural interfaces
neurips,2019,1,8189,Sasidhar,Madugula,,Stanford University,,Efficient characterization of electrically evoked responses for neural interfaces
neurips,2019,2,8189,Pawel,Hottowy,,AGH University of Science and Technology in Kraków,,Efficient characterization of electrically evoked responses for neural interfaces
neurips,2019,3,8189,Alexander,Sher,,"Santa Cruz Institute for Particle Physics, University of California, Santa Cruz",,Efficient characterization of electrically evoked responses for neural interfaces
neurips,2019,4,8189,Alan,Litke,,"Santa Cruz Institute for Particle Physics, University of California, Santa Cruz",,Efficient characterization of electrically evoked responses for neural interfaces
neurips,2019,5,8189,Liam,Paninski,,Columbia University,,Efficient characterization of electrically evoked responses for neural interfaces
neurips,2019,6,8189,E.J.,Chichilnisky,,Stanford University,,Efficient characterization of electrically evoked responses for neural interfaces
neurips,2019,0,4567,Arash,Ardakani,mcgill,McGill University,arash.ardakani@mail.mcgill.ca,The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic
neurips,2019,1,4567,Zhengyun,Ji,mcgill,McGill University,zhengyun.ji@mail.mcgill.ca,The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic
neurips,2019,2,4567,Amir,Ardakani,mcgill,McGill University,amir.ardakani@mail.mcgill.ca,The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic
neurips,2019,3,4567,Warren,Gross,mcgill,McGill University,warren.gross@mcgill.ca,The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic
neurips,2019,0,1908,Sharon,Zhou,stanford,Stanford University,sharonz@cs.stanford.edu,HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models
neurips,2019,1,1908,Mitchell,Gordon,stanford,Stanford University,mgord@cs.stanford.edu,HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models
neurips,2019,2,1908,Ranjay,Krishna,stanford,Stanford University,ranjaykrishna@cs.stanford.edu,HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models
neurips,2019,3,1908,Austin,Narcomey,stanford,Stanford University,aon2@cs.stanford.edu,HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models
neurips,2019,4,1908,Li,Fei-Fei,stanford,Stanford University,feifeili@cs.stanford.edu,HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models
neurips,2019,5,1908,Michael,Bernstein,stanford,Stanford University,msb@cs.stanford.edu,HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models
neurips,2019,0,5823,Rui (Ray),Zhang,monash,"School of Mathematics, Monash University",rui.zhang@monash.edu,McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds
neurips,2019,1,5823,Xingwu,Liu,gmail,University of Chinese Academy of Sciences,yuyiwang920@gmail.com,McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds
neurips,2019,2,5823,Yuyi,Wang,ict,ETH Zurich,liuxingwu@ict.ac.cn,McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds
neurips,2019,3,5823,Liwei,Wang,pku,Peking University,wanglw@cis.pku.edu.cn,McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds
neurips,2019,0,4419,Santosh,Vempala,gatech,Georgia Tech,vempala@gatech.edu,Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry Suffices
neurips,2019,1,4419,Andre,Wibisono,gatech,Georgia Tech,wibisono@gatech.edu,Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry Suffices
neurips,2019,0,3833,Jaehyeok,Shin,cmu,Carnegie Mellon University,shinjaehyeok@cmu.edu,Are sample means in multi-armed bandits positively or negatively biased?
neurips,2019,1,3833,Aaditya,Ramdas,cmu,Carnegie Mellon University,aramdas@cmu.edu,Are sample means in multi-armed bandits positively or negatively biased?
neurips,2019,2,3833,Alessandro,Rinaldo,cmu,CMU,arinaldo@cmu.edu,Are sample means in multi-armed bandits positively or negatively biased?
neurips,2019,0,1916,Shuang,Li,mines,Colorado School of Mines,shuangli@mines.edu,The Landscape of Non-convex Empirical Risk with Degenerate Population Risk
neurips,2019,1,1916,Gongguo,Tang,mines,Colorado School of Mines,gtang@mines.edu,The Landscape of Non-convex Empirical Risk with Degenerate Population Risk
neurips,2019,2,1916,Michael,Wakin,mines,Colorado School of Mines,mwakin@mines.edu,The Landscape of Non-convex Empirical Risk with Degenerate Population Risk
neurips,2019,0,2711,Xiao,Sun,,IBM Thomas J. Watson Research Center,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,1,2711,Jungwook,Choi,,Hanyang University,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,2,2711,Chia-Yu,Chen,,IBM research,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,3,2711,Naigang,Wang,,IBM T. J. Watson Research Center,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,4,2711,Swagath,Venkataramani,,IBM Research,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,5,2711,Vijayalakshmi (Viji),Srinivasan,,IBM TJ Watson,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,6,2711,Xiaodong,Cui,,IBM T. J. Watson Research Center,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,7,2711,Wei,Zhang,,IBM T.J.Watson Research Center,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,8,2711,Kailash,Gopalakrishnan,,IBM Research,,Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks
neurips,2019,0,9118,Chulhee,Yun,mit,MIT,chulheey@mit.edu,Are deep ResNets provably better than linear predictors?
neurips,2019,1,9118,Suvrit,Sra,mit,MIT,suvrit@mit.edu,Are deep ResNets provably better than linear predictors?
neurips,2019,2,9118,Ali,Jadbabaie,mit,MIT,jadbabai@mit.edu,Are deep ResNets provably better than linear predictors?
neurips,2019,0,2810,Yue,Wang,tamu,Rice University,jiangziyu@tamu.edu,E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings
neurips,2019,1,2810,Ziyu,Jiang,tamu,Texas A&M University,chernxh@tamu.edu,E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings
neurips,2019,2,2810,Xiaohan,Chen,tamu,Texas A&M University,atlaswang@tamu.edu,E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings
neurips,2019,3,2810,Pengfei,Xu,rice,Rice University,yw68@rice.edu,E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings
neurips,2019,4,2810,Yang,Zhao,rice,Rice University,px5@rice.edu,E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings
neurips,2019,5,2810,Yingyan,Lin,rice,Rice University,zy34@rice.edu,E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings
neurips,2019,6,2810,Zhangyang,Wang,rice,TAMU,yingyan.lin@rice.edu,E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings
neurips,2019,0,3077,Simon,Du,ias,Institute for Advanced Study,ssdu@ias.edu,Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
neurips,2019,1,3077,Kangcheng,Hou,gmail,Zhejiang University,kangchenghou@gmail.com,Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
neurips,2019,2,3077,Russ,Salakhutdinov,cmu,Carnegie Mellon University,bapoczos@cs.cmu.edu,Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
neurips,2019,3,3077,Barnabas,Poczos,cmu,Carnegie Mellon University,rsalakhu@cs.cmu.edu,Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
neurips,2019,4,3077,Ruosong,Wang,cmu,Carnegie Mellon University,ruosongw@andrew.cmu.edu,Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
neurips,2019,5,3077,Keyulu,Xu,mit,MIT,keyulu@mit.edu,Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels
neurips,2019,0,6048,Baoxiang,Wang,cuhk,The Chinese University of Hong Kong,bxwang@cse.cuhk.edu.hk,Privacy-Preserving Q-Learning with Functional Noise in Continuous Spaces
neurips,2019,1,6048,Nidhi,Hegde,borealisai,Borealis AI,nidhi.hegde@borealisai.com,Privacy-Preserving Q-Learning with Functional Noise in Continuous Spaces
neurips,2019,0,9224,Zhiting,Hu,cmu,Carnegie Mellon University,zhitingh@cs.cmu.edu,Learning Data Manipulation for Augmentation and Weighting
neurips,2019,1,9224,Bowen,Tan,cmu,CMU,btan2@cs.cmu.edu,Learning Data Manipulation for Augmentation and Weighting
neurips,2019,2,9224,Russ,Salakhutdinov,cmu,Carnegie Mellon University,rsalakhu@cs.cmu.edu,Learning Data Manipulation for Augmentation and Weighting
neurips,2019,3,9224,Tom,Mitchell,cmu,Carnegie Mellon University,tom.mitchell@cs.cmu.edu,Learning Data Manipulation for Augmentation and Weighting
neurips,2019,4,9224,Eric,Xing,petuum,Petuum Inc. /  Carnegie Mellon University,eric.xing@petuum.com,Learning Data Manipulation for Augmentation and Weighting
neurips,2019,0,3687,Ho Chung,Law,ox,University of Oxford,ho.law@stats.ox.ac.uk,Hyperparameter Learning via Distributional Transfer
neurips,2019,1,3687,Peilin,Zhao,tencent,Tencent AI Lab,masonzhao@tencent.com,Hyperparameter Learning via Distributional Transfer
neurips,2019,2,3687,Leung Sing,Chan,ox,University of Oxford,leung.chan@stats.ox.ac.uk,Hyperparameter Learning via Distributional Transfer
neurips,2019,3,3687,Junzhou,Huang,tencent,University of Texas at Arlington / Tencent AI Lab,joehhuang@tencent.com,Hyperparameter Learning via Distributional Transfer
neurips,2019,4,3687,Dino,Sejdinovic,ox,University of Oxford,dino.sejdinovic@stats.ox.ac.uk,Hyperparameter Learning via Distributional Transfer
neurips,2019,0,5992,Jiatao,Gu,fb,Facebook AI Research,jgu@fb.com,Levenshtein Transformer
neurips,2019,1,5992,Changhan,Wang,fb,Facebook AI Research,changhan@fb.com,Levenshtein Transformer
neurips,2019,2,5992,Junbo,Zhao,nyu,New York University,jakezhao@cs.nyu.edu,Levenshtein Transformer
neurips,2019,0,634,Chi,Zhang,ucla,"University of California, Los Angeles",chi.zhang@ucla.edu,Learning Perceptual Inference by Contrasting
neurips,2019,1,634,Baoxiong,Jia,ucla,UCLA,baoxiongjia@ucla.edu,Learning Perceptual Inference by Contrasting
neurips,2019,2,634,Feng,Gao,ucla,UCLA,f.gao@ucla.edu,Learning Perceptual Inference by Contrasting
neurips,2019,3,634,Yixin,Zhu,ucla,"University of California, Los Angeles",yixin.zhu@ucla.edu,Learning Perceptual Inference by Contrasting
neurips,2019,4,634,HongJing,Lu,ucla,UCLA,hongjing@ucla.edu,Learning Perceptual Inference by Contrasting
neurips,2019,5,634,Song-Chun,Zhu,ucla,UCLA,sczhu@ucla.edu,Learning Perceptual Inference by Contrasting
neurips,2019,0,2835,Shiyang,Li,ucsb,UCSB,shiyangli@ucsb.edu,Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting
neurips,2019,1,2835,Xiaoyong,Jin,ucsb,UCSB,x_jin@ucsb.edu,Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting
neurips,2019,2,2835,Yao,Xuan,ucsb,"University of California, Santa Barbara",yxuan@ucsb.edu,Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting
neurips,2019,3,2835,Xiyou,Zhou,ucsb,UC Santa Barbara,xiyou@ucsb.edu,Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting
neurips,2019,4,2835,Wenhu,Chen,ucsb,"University of California, Santa Barbara",wenhuchen@ucsb.edu,Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting
neurips,2019,5,2835,Yu-Xiang,Wang,ucsb,UC Santa Barbara,yuxiangw@cs.ucsb.edu,Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting
neurips,2019,6,2835,Xifeng,Yan,ucsb,UCSB,xyan@cs.ucsb.edu,Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting
neurips,2019,0,824,Minne,Li,ucl,University College London,minne.li@cs.ucl.ac.uk,Multi-View Reinforcement Learning
neurips,2019,1,824,Lisheng,Wu,ucl,UCL,lisheng.wu.17@ucl.ac.uk,Multi-View Reinforcement Learning
neurips,2019,2,824,Jun,WANG,googlemail,UCL,haitham.bouammar71@googlemail.com,Multi-View Reinforcement Learning
neurips,2019,3,824,Haitham,Bou Ammar,ucl,UCL,junwang@cs.ucl.ac.uk,Multi-View Reinforcement Learning
neurips,2019,0,3824,Maxence,Ernoult,,Université Paris Sud,,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input
neurips,2019,1,3824,Julie,Grollier,,Unité Mixte de Physique CNRS/Thales,,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input
neurips,2019,2,3824,Damien,Querlioz,,Univ Paris-Sud,,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input
neurips,2019,3,3824,Yoshua,Bengio,,Mila,,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input
neurips,2019,4,3824,Benjamin,Scellier,,"Mila, University of Montreal",,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input
neurips,2019,0,3930,Nika,Haghtalab,cornell,Cornell University,nika@cs.cornell.edu,Toward a Characterization of Loss Functions for Distribution Learning
neurips,2019,1,3930,Cameron,Musco,umass,Microsoft Research,cmusco@cs.umass.edu,Toward a Characterization of Loss Functions for Distribution Learning
neurips,2019,2,3930,Bo,Waggoner,colorado,"U. Colorado, Boulder",bwag@colorado.edu,Toward a Characterization of Loss Functions for Distribution Learning
neurips,2019,0,5449,Zeyuan,Allen-Zhu,mit,Microsoft Research,zeyuan@csail.mit.edu,Can SGD Learn Recurrent Neural Networks with Provable Generalization?
neurips,2019,1,5449,Yuanzhi,Li,cmu,Princeton,yuanzhil@andrew.cmu.edu,Can SGD Learn Recurrent Neural Networks with Provable Generalization?
neurips,2019,0,5963,Simao,Herdade,verizonmedia,Yahoo Research,sherdade@verizonmedia.com,Image Captioning: Transforming Objects into Words
neurips,2019,1,5963,Armin,Kappeler,verizonmedia,Apple,kaboakye@verizonmedia.com,Image Captioning: Transforming Objects into Words
neurips,2019,2,5963,Kofi,Boakye,verizonmedia,Yahoo Research,jvbsoares@verizonmedia.com,Image Captioning: Transforming Objects into Words
neurips,2019,3,5963,Joao,Soares,apple,Yahoo Research,akappeler@apple.com,Image Captioning: Transforming Objects into Words
neurips,2019,0,8485,Kundan,Kumar,,"Lyrebird-AI, Mila",,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,1,8485,Rithesh,Kumar,,Mila / Lyrebird,,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,2,8485,Thibault,de Boissiere,,Lyrebird,,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,3,8485,Lucas,Gestin,,Lyrebird,,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,4,8485,Wei Zhen,Teoh,,Lyrebird,,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,5,8485,Jose,Sotelo,,"MILA, Lyrebird",,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,6,8485,Alexandre,de Brébisson,,"LYREBIRD, MILA",,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,7,8485,Yoshua,Bengio,,Mila,,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,8,8485,Aaron,Courville,,U. Montreal,,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis
neurips,2019,0,789,Pei,Wang,ucsd,UC San Diego,pew062@ucsd.edu,Deliberative Explanations: visualizing network insecurities
neurips,2019,1,789,Nuno,Nvasconcelos,ucsd,UC San Diego,nvasconcelos@ucsd.edu,Deliberative Explanations: visualizing network insecurities
neurips,2019,0,2198,Liyuan,Xu,u-tokyo,Gatsby Computational Neuroscience Unit,liyuan@ms.k.u-tokyo.ac.jp,Uncoupled Regression from Pairwise Comparison Data
neurips,2019,1,2198,Junya,Honda,riken,The Univerisity of Tokyo / RIKEN,gang.niu@riken.jp,Uncoupled Regression from Pairwise Comparison Data
neurips,2019,2,2198,Gang,Niu,u-tokyo,RIKEN,honda@stat.t.u-tokyo.ac.jp,Uncoupled Regression from Pairwise Comparison Data
neurips,2019,3,2198,Masashi,Sugiyama,u-tokyo,RIKEN / University of Tokyo,sugi@k.u-tokyo.ac.jp,Uncoupled Regression from Pairwise Comparison Data
neurips,2019,0,7573,Pier Giuseppe,Sessa,ethz,ETH Zürich,sessap@ethz.ch,No-Regret Learning in Unknown Games with Correlated Payoffs
neurips,2019,1,7573,Ilija,Bogunovic,ethz,ETH Zurich,ilijab@ethz.ch,No-Regret Learning in Unknown Games with Correlated Payoffs
neurips,2019,2,7573,Maryam,Kamgarpour,ethz,ETH Zürich,maryamk@ethz.ch,No-Regret Learning in Unknown Games with Correlated Payoffs
neurips,2019,3,7573,Andreas,Krause,ethz,ETH Zurich,krausea@ethz.ch,No-Regret Learning in Unknown Games with Correlated Payoffs
neurips,2019,0,6489,Xi,Lin,cityu,City University of Hong Kong,qingfu.zhang@cityu.edu.hk,Pareto Multi-Task Learning
neurips,2019,1,6489,Hui-Ling,Zhen,cityu,City University of Hong Kong,cssamk@cityu.edu.hk,Pareto Multi-Task Learning
neurips,2019,2,6489,Zhenhua,Li,cityu,National University of Singapore,xi.lin@my.cityu.edu.hk,Pareto Multi-Task Learning
neurips,2019,3,6489,Qing-Fu,Zhang,cityu,,huilzhen@um.cityu.edu.hk,Pareto Multi-Task Learning
neurips,2019,4,6489,Sam,Kwong,nuaa,City Univeristy of Hong Kong,zhenhua.li@nuaa.edu.cn,Pareto Multi-Task Learning
neurips,2019,0,294,Yitian,Yuan,tsinghua,Tsinghua University,yyt18@mails.tsinghua.edu.cn,Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos
neurips,2019,1,294,Lin,Ma,gmail,Tencent AI Lab,forest.linma@gmail.com,Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos
neurips,2019,2,294,Jingwen,Wang,gmail,Tencent AI Lab,jaywongjaywong@gmail.com,Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos
neurips,2019,3,294,Wei,Liu,columbia,Tencent AI Lab,wl2223@columbia.edu,Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos
neurips,2019,4,294,Wenwu,Zhu,tsinghua,Tsinghua University,wwzhu@tsinghua.edu.cn,Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos
neurips,2019,0,6744,Virginia,Aglietti,warwick,University of Warwick,V.Aglietti@warwick.ac.uk,Structured Variational Inference in Continuous Cox Process Models
neurips,2019,1,6744,Edwin,Bonilla,warwick,CSIRO's Data61,T.Damoulas@warwick.ac.uk,Structured Variational Inference in Continuous Cox Process Models
neurips,2019,2,6744,Theodoros,Damoulas,csiro,University of Warwick & The Alan Turing Institute,Edwin.Bonilla@data61.csiro.au,Structured Variational Inference in Continuous Cox Process Models
neurips,2019,3,6744,Sally,Cripps,sydney,University of Sydney,Sally.Cripps@sydney.edu.au,Structured Variational Inference in Continuous Cox Process Models
neurips,2019,0,1093,Weizhe,Hua,cornell,Cornell University,wh399@cornell.edu,Channel Gating Neural Networks
neurips,2019,1,1093,Yuan,Zhou,cornell,Cornell,yz882@cornell.edu,Channel Gating Neural Networks
neurips,2019,2,1093,Christopher,De Sa,cornell,Cornell,cdesa@cornell.edu,Channel Gating Neural Networks
neurips,2019,3,1093,Zhiru,Zhang,cornell,Cornell Univeristy,zhiruz@cornell.edu,Channel Gating Neural Networks
neurips,2019,4,1093,G. Edward,Suh,cornell,Cornell University,gs272@cornell.edu,Channel Gating Neural Networks
neurips,2019,0,1224,Peilin,Zhong,columbia,Columbia University,peilin@cs.columbia.edu,Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach
neurips,2019,1,1224,Yuchen,Mo,columbia,Columbia University,chang@cs.columbia.edu,Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach
neurips,2019,2,1224,Chang,Xiao,columbia,Columbia University,cxz@cs.columbia.edu,Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach
neurips,2019,3,1224,Pengyu,Chen,columbia,Columbia University,yuchen.mo@columbia.edu,Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach
neurips,2019,4,1224,Changxi,Zheng,columbia,Columbia University,pengyu.chen@columbia.edu,Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach
neurips,2019,0,84,Gautam,Kamath,,University of Waterloo,,Differentially Private Algorithms for Learning Mixtures of Separated Gaussians
neurips,2019,1,84,Or,Sheffet,,University of Alberta,,Differentially Private Algorithms for Learning Mixtures of Separated Gaussians
neurips,2019,2,84,Vikrant,Singhal,,Northeastern University,,Differentially Private Algorithms for Learning Mixtures of Separated Gaussians
neurips,2019,3,84,Jonathan,Ullman,,Northeastern University,,Differentially Private Algorithms for Learning Mixtures of Separated Gaussians
neurips,2019,0,6501,Paulina,Grnarova,,ETH Zurich,,A Domain Agnostic Measure for Monitoring and Evaluating GANs
neurips,2019,1,6501,Kfir Y.,Levy,,Technion,,A Domain Agnostic Measure for Monitoring and Evaluating GANs
neurips,2019,2,6501,Aurelien,Lucchi,,ETH Zurich,,A Domain Agnostic Measure for Monitoring and Evaluating GANs
neurips,2019,3,6501,Nathanael,Perraudin,,Swiss Data Science Center - EPFL / ETH Zurich,,A Domain Agnostic Measure for Monitoring and Evaluating GANs
neurips,2019,4,6501,Ian,Goodfellow,,Google,,A Domain Agnostic Measure for Monitoring and Evaluating GANs
neurips,2019,5,6501,Thomas,Hofmann,,ETH Zurich,,A Domain Agnostic Measure for Monitoring and Evaluating GANs
neurips,2019,6,6501,Andreas,Krause,,ETH Zurich,,A Domain Agnostic Measure for Monitoring and Evaluating GANs
neurips,2019,0,9406,Mohammad Reza,Keshtkaran,emory,Georgia Tech and Emory University,mkeshtk@emory.edu,Enabling hyperparameter optimization in sequential autoencoders for spiking neural data
neurips,2019,1,9406,Chethan,Pandarinath,gatech,Emory University and Georgia Tech,chethan@gatech.edu,Enabling hyperparameter optimization in sequential autoencoders for spiking neural data
neurips,2019,0,3479,Lukas,Hoyer,,Bosch Center for Artificial Intelligence,,Grid Saliency for Context Explanations of Semantic Segmentation
neurips,2019,1,3479,Mauricio,Munoz,,Bosch Center for Artificial Intelligence,,Grid Saliency for Context Explanations of Semantic Segmentation
neurips,2019,2,3479,Prateek,Katiyar,,Bosch Center for Artificial Intelligence,,Grid Saliency for Context Explanations of Semantic Segmentation
neurips,2019,3,3479,Anna,Khoreva,,Bosch Center for Artificial Intelligence,,Grid Saliency for Context Explanations of Semantic Segmentation
neurips,2019,4,3479,Volker,Fischer,,"Robert Bosch GmbH, Bosch Center for Artificial Intelligence",,Grid Saliency for Context Explanations of Semantic Segmentation
neurips,2019,0,7268,Tharun Kumar Reddy,Medini,rice,Rice University,tharun.medini@rice.edu,Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products
neurips,2019,1,7268,Qixuan,Huang,rice,Rice University,qh5@rice.edu,Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products
neurips,2019,2,7268,Yiqiu,Wang,mit,Massachusetts Institute of Technology,yiqiuw@mit.edu,Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products
neurips,2019,3,7268,Vijai,Mohan,amazon,www.amazon.com,vijaim@amazon.com,Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products
neurips,2019,4,7268,Anshumali,Shrivastava,rice,Rice University,anshumali@rice.edu,Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products
neurips,2019,0,635,Yu-Chia,Chen,uw,University of Washington,yuchaz@uw.edu,Selecting the independent coordinates of manifolds with large aspect ratios
neurips,2019,1,635,Marina,Meila,uw,University of Washington,mmp2@uw.edu,Selecting the independent coordinates of manifolds with large aspect ratios
neurips,2019,0,3163,Yangbangyan,Jiang,,"Institute of Information Engineering, Chinese Academy of Sciences",,DM2C: Deep Mixed-Modal Clustering
neurips,2019,1,3163,Qianqian,Xu,,"Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences",,DM2C: Deep Mixed-Modal Clustering
neurips,2019,2,3163,Zhiyong,Yang,,"SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences; SCS, University of Chinese Academy of Sciences",,DM2C: Deep Mixed-Modal Clustering
neurips,2019,3,3163,Xiaochun,Cao,,"Institute of Information Engineering, Chinese Academy of Sciences",,DM2C: Deep Mixed-Modal Clustering
neurips,2019,4,3163,Qingming,Huang,,University of Chinese Academy of Sciences,,DM2C: Deep Mixed-Modal Clustering
neurips,2019,0,1217,Difan,Zou,ucla,"University of California, Los Angeles",knowzou@cs.ucla.edu,An Improved Analysis of Training Over-parameterized Deep Neural Networks
neurips,2019,1,1217,Quanquan,Gu,ucla,UCLA,qgu@cs.ucla.edu,An Improved Analysis of Training Over-parameterized Deep Neural Networks
neurips,2019,0,3603,Adil,SALIM,,KAUST,,Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates
neurips,2019,1,3603,Dmitry,Koralev,,KAUST,,Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates
neurips,2019,2,3603,Peter,Richtarik,,KAUST,,Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates
neurips,2019,0,5120,Santiago,Balseiro,columbia,Columbia University,srb2155@columbia.edu,Contextual Bandits with Cross-Learning
neurips,2019,1,5120,Negin,Golrezaei,mit,University of Southern California,golrezae@mit.edu,Contextual Bandits with Cross-Learning
neurips,2019,2,5120,Mohammad,Mahdian,google,Google Research,mahdian@google.com,Contextual Bandits with Cross-Learning
neurips,2019,3,5120,Vahab,Mirrokni,google,Google Research NYC,mirrokni@google.com,Contextual Bandits with Cross-Learning
neurips,2019,4,5120,Jon,Schneider,google,Google Research,jschnei@google.com,Contextual Bandits with Cross-Learning
neurips,2019,0,3614,Sungbin,Lim,unist,Kakao Brain,sungbin@unist.ac.kr,Fast AutoAugment
neurips,2019,1,3614,Ildoo,Kim,umontreal,Kakao Brain,taesup.kim@umontreal.ca,Fast AutoAugment
neurips,2019,2,3614,Taesup,Kim,kakaobrain,Mila / Kakao Brain,ildoo.kim@kakaobrain.com,Fast AutoAugment
neurips,2019,3,3614,Chiheon,Kim,kakaobrain,Kakao Brain,chiheon.kim@kakaobrain.com,Fast AutoAugment
neurips,2019,4,3614,Sungwoong,Kim,kakaobrain,Kakao Brain,swkim@kakaobrain.com,Fast AutoAugment
neurips,2019,0,2613,Tao,Tu,columbia,Columbia University,tt2531@columbia.edu,A state-space model for inferring effective connectivity of latent neural dynamics from simultaneous EEG/fMRI
neurips,2019,1,2613,John,Paisley,columbia,Columbia University,jpaisley@columbia.edu,A state-space model for inferring effective connectivity of latent neural dynamics from simultaneous EEG/fMRI
neurips,2019,2,2613,Stefan,Haufe,charite,Charité  Universitätsmedizin Berlin,stefan.haufe@charite.de,A state-space model for inferring effective connectivity of latent neural dynamics from simultaneous EEG/fMRI
neurips,2019,3,2613,Paul,Sajda,columbia,Columbia University,psajda@columbia.edu,A state-space model for inferring effective connectivity of latent neural dynamics from simultaneous EEG/fMRI
neurips,2019,0,7677,Chuang,Wang,ia,Harvard University,wangchuang@ia.ac.cn,A Solvable High-Dimensional Model of GAN
neurips,2019,1,7677,Hong,Hu,harvard,Harvard,honghu@g.harvard.edu,A Solvable High-Dimensional Model of GAN
neurips,2019,2,7677,Yue,Lu,harvard,Harvard University,yuelu@seas.harvard.edu,A Solvable High-Dimensional Model of GAN
neurips,2019,0,733,Dong,Liu,,University of Science and Technology of China,,On The Classification-Distortion-Perception Tradeoff
neurips,2019,1,733,Haochen,Zhang,,University of Science and Technology of China,,On The Classification-Distortion-Perception Tradeoff
neurips,2019,2,733,Zhiwei,Xiong,,University of Science and Technology of China,,On The Classification-Distortion-Perception Tradeoff
neurips,2019,0,6073,Yair,Carmon,stanford,Stanford University,yairc@stanford.edu,Variance Reduction for Matrix Games
neurips,2019,1,6073,Yujia,Jin,stanford,Stanford University,yujiajin@stanford.edu,Variance Reduction for Matrix Games
neurips,2019,2,6073,Aaron,Sidford,stanford,Stanford,sidford@stanford.edu,Variance Reduction for Matrix Games
neurips,2019,3,6073,Kevin,Tian,stanford,Stanford University,kjtian@stanford.edu,Variance Reduction for Matrix Games
neurips,2019,0,5349,Hanzhang,Hu,cmu,Carnegie Mellon University,hanzhang@cs.cmu.edu,Efficient Forward Architecture Search
neurips,2019,1,5349,John,Langford,microsoft,Microsoft Research New York,jcl@microsoft.com,Efficient Forward Architecture Search
neurips,2019,2,5349,Rich,Caruana,microsoft,Microsoft,rcaruana@microsoft.com,Efficient Forward Architecture Search
neurips,2019,3,5349,Saurajit,Mukherjee,microsoft,microsoft,saurajim@microsoft.com,Efficient Forward Architecture Search
neurips,2019,4,5349,Eric,Horvitz,microsoft,Microsoft Research,horvitz@microsoft.com,Efficient Forward Architecture Search
neurips,2019,5,5349,Debadeepta,Dey,microsoft,Microsoft Research AI,dedey@microsoft.com,Efficient Forward Architecture Search
neurips,2019,0,3196,Siqi,Wang,nudt,National University of Defense Technology,wangsiqi10c@nudt.edu.cn,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
neurips,2019,1,3196,Yijie,Zeng,ntu,Nanyang Technological University,yzeng004@e.ntu.edu.sg,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
neurips,2019,2,3196,Xinwang,Liu,nudt,National University of Defense Technology,xinwangliu@nudt.edu.cn,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
neurips,2019,3,3196,En,Zhu,nudt,National University of Defense Technology,enzhu@nudt.edu.cn,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
neurips,2019,4,3196,Jianping,Yin,dgut,Dongguan University of Technology,jpyin@dgut.edu.cn,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
neurips,2019,5,3196,Chuanfu,Xu,nudt,National University of Defense Technology,xuchuanfu@nudt.edu.cn,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
neurips,2019,6,3196,Marius,Kloft,uni-kl,TU Kaiserslautern,kloft@cs.uni-kl.de,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
neurips,2019,0,5517,Emmanouil-Vasileios,Vlatakis-Gkaragkounis,columbia,Columbia University,lamflokas@cs.columbia.edu,"Poincaré Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games"
neurips,2019,1,5517,Lampros,Flokas,columbia,Columbia University,emvlatakis@cs.columbia.edu,"Poincaré Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games"
neurips,2019,2,5517,Georgios,Piliouras,sutd,Singapore University of Technology and Design,georgios@sutd.edu.sg,"Poincaré Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games"
neurips,2019,0,9078,Raphael,Townshend,stanford,Stanford University,raphael@cs.stanford.edu,End-to-End Learning on 3D Protein Structure for Interface Prediction
neurips,2019,1,9078,Rishi,Bedi,stanford,System1 Biosciences,psuriana@stanford.edu,End-to-End Learning on 3D Protein Structure for Interface Prediction
neurips,2019,2,9078,Patricia,Suriana,stanford,Stanford University,rbedi@cs.stanford.edu,End-to-End Learning on 3D Protein Structure for Interface Prediction
neurips,2019,3,9078,Ron,Dror,stanford,Stanford University,rondror@cs.stanford.edu,End-to-End Learning on 3D Protein Structure for Interface Prediction
neurips,2019,0,2939,David,Eriksson,uber,Uber AI,eriksson@uber.com,Scalable Global Optimization via Local Bayesian Optimization
neurips,2019,1,2939,Michael,Pearce,warwick,Warwick University,m.a.l.pearce@warwick.ac.uk,Scalable Global Optimization via Local Bayesian Optimization
neurips,2019,2,2939,Jacob,Gardner,uber,Uber AI Labs,jake.gardner@uber.com,Scalable Global Optimization via Local Bayesian Optimization
neurips,2019,3,2939,Ryan,Turner,uber,Uber AI Labs,ryan.turner@uber.com,Scalable Global Optimization via Local Bayesian Optimization
neurips,2019,4,2939,Matthias,Poloczek,uber,Uber AI,poloczek@uber.com,Scalable Global Optimization via Local Bayesian Optimization
neurips,2019,0,920,Boyi,Li,cornell,Cornell University,bl728@cornell.edu,Positional Normalization
neurips,2019,1,920,Felix,Wu,cornell,Cornell University,fw245@cornell.edu,Positional Normalization
neurips,2019,2,920,Kilian,Weinberger,cornell,Cornell University / ASAPP Research,kilian@cornell.edu,Positional Normalization
neurips,2019,3,920,Serge,Belongie,cornell,Cornell University,sjb344@cornell.edu,Positional Normalization
neurips,2019,0,2922,Atilim Gunes,Baydin,,University of Oxford,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,1,2922,Lei,Shao,,Intel Corporation,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,2,2922,Wahid,Bhimji,,Berkeley lab,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,3,2922,Lukas,Heinrich,,New York University,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,4,2922,Saeid,Naderiparizi,,University of British Columbia,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,5,2922,Andreas,Munk,,University of British Columbia,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,6,2922,Jialin,Liu,,Lawrence Berkeley National Lab,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,7,2922,Bradley,Gram-Hansen,,University of Oxford,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,8,2922,Gilles,Louppe,,University of Liège,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,9,2922,Lawrence,Meadows,,Intel Corporation,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,10,2922,Philip,Torr,,University of Oxford,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,11,2922,Victor,Lee,,Intel Corporation,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,12,2922,Kyle,Cranmer,,New York University,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,13,2922,Mr.,Prabhat,,LBL/NERSC,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,14,2922,Frank,Wood,,University of British Columbia,,Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
neurips,2019,0,8472,Yingying,Li,harvard,Harvard University,yingyingli@g.harvard.edu,Online Optimal Control with Linear Dynamics and Predictions: Algorithms and Regret Analysis
neurips,2019,1,8472,Xin,Chen,harvard,Harvard University,chen_xin@g.harvard.edu,Online Optimal Control with Linear Dynamics and Predictions: Algorithms and Regret Analysis
neurips,2019,2,8472,Na,Li,harvard,Harvard University,nali@seas.harvard.edu,Online Optimal Control with Linear Dynamics and Predictions: Algorithms and Regret Analysis
neurips,2019,0,3745,Denis,Mazur,yandex-team,Yandex,denismazur@yandex-team.ru,Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs
neurips,2019,1,3745,Vage,Egiazarian,skoltech,Skoltech,Vage.egiazarian@skoltech.ru,Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs
neurips,2019,2,3745,Stanislav,Morozov,yandex,Yandex,stanis-morozov@yandex.ru,Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs
neurips,2019,3,3745,Artem,Babenko,phystech,Yandex,artem.babenko@phystech.edu,Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs
neurips,2019,0,1408,Jie,Ding,umn,University of Minnesota,dingj@umn.edu,Gradient Information for Representation and Modeling
neurips,2019,1,1408,Robert,Calderbank,duke,Duke University,robert.calderbank@duke.edu,Gradient Information for Representation and Modeling
neurips,2019,2,1408,Vahid,Tarokh,duke,Duke University,vahid.tarokh@duke.edu,Gradient Information for Representation and Modeling
neurips,2019,0,230,Qiming,ZHANG,sydney,University of Sydney,qzha2506@uni.sydney.edu.au,Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation
neurips,2019,1,230,Jing,Zhang,sydney,The University of Sydney,jing.zhang1@sydney.edu.au,Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation
neurips,2019,2,230,Wei,Liu,columbia,Tencent AI Lab,wl2223@columbia.edu,Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation
neurips,2019,3,230,Dacheng,Tao,sydney,University of Sydney,dacheng.tao@sydney.edu.au,Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation
neurips,2019,0,6499,Vighnesh,Shiv,microsoft,Microsoft Research,vishiv@microsoft.com,Novel positional encodings to enable tree-based transformers
neurips,2019,1,6499,Chris,Quirk,microsoft,Microsoft Research,chrisq@microsoft.com,Novel positional encodings to enable tree-based transformers
neurips,2019,0,8393,Spencer,Frei,,UCLA,,Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks
neurips,2019,1,8393,Yuan,Cao,,UCLA,,Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks
neurips,2019,2,8393,Quanquan,Gu,,UCLA,,Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks
neurips,2019,0,1742,Hongteng,Xu,duke,Infinia ML and Duke University,hongteng.xu@duke.edu,Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching
neurips,2019,1,1742,Dixin,Luo,duke,Duke University,dixin.luo@duke.edu,Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching
neurips,2019,2,1742,Lawrence,Carin,duke,Duke University,lcarin@duke.edu,Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching
neurips,2019,0,8975,Daniel,Brooks,,Thales - LIP6,,Riemannian batch normalization for SPD neural networks
neurips,2019,1,8975,Olivier,Schwander,,Sorbonne Université,,Riemannian batch normalization for SPD neural networks
neurips,2019,2,8975,Frederic,Barbaresco,,THALES LAND & AIR SYSTEMS,,Riemannian batch normalization for SPD neural networks
neurips,2019,3,8975,Jean-Yves,Schneider,,THALES LAND & AIR SYSTEMS,,Riemannian batch normalization for SPD neural networks
neurips,2019,4,8975,Matthieu,Cord,,Sorbonne University,,Riemannian batch normalization for SPD neural networks
neurips,2019,0,1810,Yan,Zhang,soton,University of Southampton,yz5n12@ecs.soton.ac.uk,Deep Set Prediction Networks
neurips,2019,1,1810,Jonathon,Hare,soton,University of Southampton,jsh2@ecs.soton.ac.uk,Deep Set Prediction Networks
neurips,2019,2,1810,Adam,Prugel-Bennett,soton,apb@ecs.soton.ac.uk,apb@ecs.soton.ac.uk,Deep Set Prediction Networks
neurips,2019,0,5290,Ben,Sorscher,,Stanford University,,A unified theory for the origin of grid cells through the lens of pattern formation
neurips,2019,1,5290,Gabriel,Mel,,Stanford University,,A unified theory for the origin of grid cells through the lens of pattern formation
neurips,2019,2,5290,Surya,Ganguli,,Stanford,,A unified theory for the origin of grid cells through the lens of pattern formation
neurips,2019,3,5290,Samuel,Ocko,,Stanford,,A unified theory for the origin of grid cells through the lens of pattern formation
neurips,2019,0,5500,Cassidy,Laidlaw,umd,"University of Maryland, College Park",claidlaw@umd.edu,Functional Adversarial Attacks
neurips,2019,1,5500,Soheil,Feizi,umd,University of Maryland,sfeizi@cs.umd.edu,Functional Adversarial Attacks
neurips,2019,0,480,Miao,Zhang,dlut,Dalian University of Technology,miaozhang@dlut.edu.cn,Memory-oriented Decoder for Light Field Salient Object Detection
neurips,2019,1,480,Jingjing,Li,dlut,Dalian University of Technology,lijingjing@mail.dlut.edu.cn,Memory-oriented Decoder for Light Field Salient Object Detection
neurips,2019,2,480,JI,WEI,dlut,Dalian University of Technology,jiwei521@mail.dlut.edu.cn,Memory-oriented Decoder for Light Field Salient Object Detection
neurips,2019,3,480,Yongri,Piao,dlut,Dalian University of Technology,yrpiao@dlut.edu.cn,Memory-oriented Decoder for Light Field Salient Object Detection
neurips,2019,4,480,Huchuan,Lu,dlut,Dalian University of Technology,lhchuan@dlut.edu.cn,Memory-oriented Decoder for Light Field Salient Object Detection
neurips,2019,0,6946,Valerio,Perrone,,Amazon,,Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning
neurips,2019,1,6946,Huibin,Shen,,Amazon,,Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning
neurips,2019,2,6946,Matthias,Seeger,,Amazon,,Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning
neurips,2019,3,6946,Cedric,Archambeau,,Amazon,,Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning
neurips,2019,4,6946,Rodolphe,Jenatton,,Amazon,,Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning
neurips,2019,0,753,Shibani,Santurkar,mit,MIT,shibani@mit.edu,Image Synthesis with a Single (Robust) Classifier
neurips,2019,1,753,Andrew,Ilyas,mit,MIT,tsipras@mit.edu,Image Synthesis with a Single (Robust) Classifier
neurips,2019,2,753,Dimitris,Tsipras,mit,MIT,ailyas@mit.edu,Image Synthesis with a Single (Robust) Classifier
neurips,2019,3,753,Logan,Engstrom,mit,MIT,engstrom@mit.edu,Image Synthesis with a Single (Robust) Classifier
neurips,2019,4,753,Brandon,Tran,mit,Massachusetts Institute of Technology,btran115@mit.edu,Image Synthesis with a Single (Robust) Classifier
neurips,2019,5,753,Aleksander,Madry,mit,MIT,madry@mit.edu,Image Synthesis with a Single (Robust) Classifier
neurips,2019,0,4556,Sujoy,Paul,ucr,UC Riverside,supaul@ece.ucr.edu,Learning from Trajectories via Subgoal Discovery
neurips,2019,1,4556,Jeroen,Vanbaar,merl,"MERL (Mitsubishi Electric Research Laboratories), Cambridge MA",jeroen@merl.com,Learning from Trajectories via Subgoal Discovery
neurips,2019,2,4556,Amit,Roy-Chowdhury,ucr,"University of California, Riverside, USA",amitrc@ece.ucr.edu,Learning from Trajectories via Subgoal Discovery
neurips,2019,0,4727,Ankesh,Anand,,"Mila, University of Montreal",,Unsupervised State Representation Learning in Atari
neurips,2019,1,4727,Evan,Racah,,"Mila, Université de Montréal",,Unsupervised State Representation Learning in Atari
neurips,2019,2,4727,Sherjil,Ozair,,"Mila, Université de Montréal",,Unsupervised State Representation Learning in Atari
neurips,2019,3,4727,Yoshua,Bengio,,Mila,,Unsupervised State Representation Learning in Atari
neurips,2019,4,4727,Marc-Alexandre,Côté,,Microsoft Research,,Unsupervised State Representation Learning in Atari
neurips,2019,5,4727,R Devon,Hjelm,,Microsoft Research,,Unsupervised State Representation Learning in Atari
neurips,2019,0,4447,Gregory,Farquhar,,University of Oxford,,Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning
neurips,2019,1,4447,Shimon,Whiteson,,University of Oxford,,Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning
neurips,2019,2,4447,Jakob,Foerster,,Facebook AI Research,,Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning
neurips,2019,0,5251,Yujia,Xie,gmail,Georgia Institute of Technology,Xie.Yujia000@gmail.com,Meta Learning with Relational Information for Short Sequences
neurips,2019,1,5251,Haoming,Jiang,gatech,Georgia Institute of Technology,jianghm@gatech.edu,Meta Learning with Relational Information for Short Sequences
neurips,2019,2,5251,Feng,Liu,fau,Florida Atlantic University,FLIU2016@fau.edu,Meta Learning with Relational Information for Short Sequences
neurips,2019,3,5251,Tuo,Zhao,gatech,Georgia Tech,tuo.zhao@isye.gatech.edu,Meta Learning with Relational Information for Short Sequences
neurips,2019,4,5251,Hongyuan,Zha,cuhk,Georgia Tech,zhahy@cuhk.edu.cn,Meta Learning with Relational Information for Short Sequences
neurips,2019,0,4697,Alon,Gonen,ucsd,UCSD,algonen@cs.ucsd.edu,Private Learning Implies Online Learning: An Efficient Reduction
neurips,2019,1,4697,Elad,Hazan,princeton,Princeton University,ehazan@princeton.edu,Private Learning Implies Online Learning: An Efficient Reduction
neurips,2019,2,4697,Shay,Moran,gmail,Google AI Princeton,shaymoran1@gmail.com,Private Learning Implies Online Learning: An Efficient Reduction
neurips,2019,0,5059,Zhe,Li,,Baylor College of Medicine,,Learning from brains how to regularize machines
neurips,2019,1,5059,Wieland,Brendel,,"AG Bethge, University of Tübingen",,Learning from brains how to regularize machines
neurips,2019,2,5059,Edgar,Walker,,Baylor College of Medicine,,Learning from brains how to regularize machines
neurips,2019,3,5059,Erick,Cobos,,Baylor College of Medicine,,Learning from brains how to regularize machines
neurips,2019,4,5059,Taliah,Muhammad,,Baylor College of Medicine,,Learning from brains how to regularize machines
neurips,2019,5,5059,Jacob,Reimer,,Baylor College of Medicine,,Learning from brains how to regularize machines
neurips,2019,6,5059,Matthias,Bethge,,University of Tübingen,,Learning from brains how to regularize machines
neurips,2019,7,5059,Fabian,Sinz,,University Tübingen,,Learning from brains how to regularize machines
neurips,2019,8,5059,Zachary,Pitkow,,BCM/Rice,,Learning from brains how to regularize machines
neurips,2019,9,5059,Andreas,Tolias,,Baylor College of Medicine,,Learning from brains how to regularize machines
neurips,2019,0,7077,Ayoub,Belhadji,univ-lille,Ecole Centrale de Lille,ayoub.belhadji@univ-lille.fr,Kernel quadrature with DPPs
neurips,2019,1,7077,Rémi,Bardenet,univ-lille,University of Lille,remi.bardenet@univ-lille.fr,Kernel quadrature with DPPs
neurips,2019,2,7077,Pierre,Chainais,univ-lille,Centrale Lille / CRIStAL CNRS UMR 9189,pierre.chainais@univ-lille.fr,Kernel quadrature with DPPs
neurips,2019,0,4403,Xiao,Li,berkeley,"University of California, Berkeley",sxli@berkeley.edu,A Debiased MDI Feature Importance Measure for Random Forests
neurips,2019,1,4403,Yu,Wang,berkeley,UC Berkeley,wang.yu@berkeley.edu,A Debiased MDI Feature Importance Measure for Random Forests
neurips,2019,2,4403,Sumanta,Basu,cornell,Cornell University,sumbose@cornell.edu,A Debiased MDI Feature Importance Measure for Random Forests
neurips,2019,3,4403,Karl,Kumbier,berkeley,"University of California, Berkeley",kkumbier@berkeley.edu,A Debiased MDI Feature Importance Measure for Random Forests
neurips,2019,4,4403,Bin,Yu,berkeley,UC Berkeley,binyu@berkeley.edu,A Debiased MDI Feature Importance Measure for Random Forests
neurips,2019,0,8045,David,Clark,columbia,Columbia University,dgc2138@cumc.columbia.edu,Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis
neurips,2019,1,8045,Jesse,Livezey,lbl,Lawrence Berkeley National Laboratory,kebouchard@lbl.gov,Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis
neurips,2019,2,8045,Kristofer,Bouchard,lbl,Lawrence Berkeley National Laboratory,jlivezey@lbl.gov,Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis
neurips,2019,0,5902,Yang,Song,,Stanford University,,MintNet: Building Invertible Neural Networks with Masked Convolutions
neurips,2019,1,5902,Chenlin,Meng,,Stanford University,,MintNet: Building Invertible Neural Networks with Masked Convolutions
neurips,2019,2,5902,Stefano,Ermon,,Stanford,,MintNet: Building Invertible Neural Networks with Masked Convolutions
neurips,2019,0,1728,Gedas,Bertasius,,Facebook Research,,Learning Temporal Pose Estimation from Sparsely-Labeled Videos
neurips,2019,1,1728,Christoph,Feichtenhofer,,Facebook AI Research,,Learning Temporal Pose Estimation from Sparsely-Labeled Videos
neurips,2019,2,1728,Du,Tran,,Facebook AI,,Learning Temporal Pose Estimation from Sparsely-Labeled Videos
neurips,2019,3,1728,Jianbo,Shi,,University of Pennsylvania,,Learning Temporal Pose Estimation from Sparsely-Labeled Videos
neurips,2019,4,1728,Lorenzo,Torresani,,Facebook AI Research,,Learning Temporal Pose Estimation from Sparsely-Labeled Videos
neurips,2019,0,2191,ravichandra,addanki,mit,Massachusetts Institute of Technology,addanki@mit.edu,Learning Generalizable Device Placement Algorithms for Distributed Machine Learning
neurips,2019,1,2191,Shaileshh,Bojja Venkatakrishnan,mit,Massachusetts Institute of Technology,bjjvnkt@mit.edu,Learning Generalizable Device Placement Algorithms for Distributed Machine Learning
neurips,2019,2,2191,Shreyan,Gupta,mit,MIT,shreyang@mit.edu,Learning Generalizable Device Placement Algorithms for Distributed Machine Learning
neurips,2019,3,2191,Hongzi,Mao,mit,MIT,hongzi@mit.edu,Learning Generalizable Device Placement Algorithms for Distributed Machine Learning
neurips,2019,4,2191,Mohammad,Alizadeh,mit,Massachusetts Institute of Technology,alizadeh@mit.edu,Learning Generalizable Device Placement Algorithms for Distributed Machine Learning
neurips,2019,0,5151,Negin,Golrezaei,mit,MIT,golrezae@mit.edu,Dynamic Incentive-Aware Learning: Robust Pricing in Contextual Auctions
neurips,2019,1,5151,Adel,Javanmard,usc,USC,ajavanma@usc.edu,Dynamic Incentive-Aware Learning: Robust Pricing in Contextual Auctions
neurips,2019,2,5151,Vahab,Mirrokni,google,Google Research NYC,mirrokni@google.com,Dynamic Incentive-Aware Learning: Robust Pricing in Contextual Auctions
neurips,2019,0,2998,Vrettos,Moulos,berkeley,UC Berkeley,vrettos@berkeley.edu,Optimal Best Markovian Arm Identification with Fixed Confidence
neurips,2019,0,9347,Zhengdao,Chen,nyu,New York University,zc1216@nyu.edu,On the equivalence between graph isomorphism testing and function approximation with GNNs
neurips,2019,1,9347,Soledad,Villar,nyu,New York University,lc3909@nyu.edu,On the equivalence between graph isomorphism testing and function approximation with GNNs
neurips,2019,2,9347,Lei,Chen,nyu,New York University,soledad.villar@nyu.edu,On the equivalence between graph isomorphism testing and function approximation with GNNs
neurips,2019,3,9347,Joan,Bruna,nyu,NYU,bruna@cims.nyu.edu,On the equivalence between graph isomorphism testing and function approximation with GNNs
neurips,2019,0,1291,Jie,Hu,,Xiamen University,,Information Competing Process for Learning Diversified Representations
neurips,2019,1,1291,Rongrong,Ji,,"Xiamen University, China",,Information Competing Process for Learning Diversified Representations
neurips,2019,2,1291,ShengChuan,Zhang,,Xiamen University,,Information Competing Process for Learning Diversified Representations
neurips,2019,3,1291,Xiaoshuai,Sun,,Xiamen University,,Information Competing Process for Learning Diversified Representations
neurips,2019,4,1291,Qixiang,Ye,,"University of Chinese Academy of Sciences, China",,Information Competing Process for Learning Diversified Representations
neurips,2019,5,1291,Chia-Wen,Lin,,National Tsing Hua University,,Information Competing Process for Learning Diversified Representations
neurips,2019,6,1291,Qi,Tian,,Huawei Noahs Ark Lab,,Information Competing Process for Learning Diversified Representations
neurips,2019,0,1761,Yogev,Bar-On,gmail,Tel-Aviv University,baronyogev@gmail.com,Individual Regret in Cooperative Nonstochastic Multi-Armed Bandits
neurips,2019,1,1761,Yishay,Mansour,gmail,Tel Aviv University / Google,mansour.yishay@gmail.com,Individual Regret in Cooperative Nonstochastic Multi-Armed Bandits
neurips,2019,0,6390,Sumith,Kulal,stanford,Stanford University,sumith@cs.stanford.edu,SPoC: Search-based Pseudocode to Code
neurips,2019,1,6390,Panupong,Pasupat,stanford,Stanford University,ppasupat@cs.stanford.edu,SPoC: Search-based Pseudocode to Code
neurips,2019,2,6390,Kartik,Chandra,stanford,Stanford University,kach@cs.stanford.edu,SPoC: Search-based Pseudocode to Code
neurips,2019,3,6390,Mina,Lee,stanford,Stanford University,minalee@cs.stanford.edu,SPoC: Search-based Pseudocode to Code
neurips,2019,4,6390,Oded,Padon,stanford,Stanford University,padon@cs.stanford.edu,SPoC: Search-based Pseudocode to Code
neurips,2019,5,6390,Alex,Aiken,stanford,Stanford University,aaiken@cs.stanford.edu,SPoC: Search-based Pseudocode to Code
neurips,2019,6,6390,Percy,Liang,stanford,Stanford University,pliang@cs.stanford.edu,SPoC: Search-based Pseudocode to Code
neurips,2019,0,782,Chen,Tessler,technion,Technion,chen.tessler@campus.technion.ac.il,Distributional Policy Optimization: An Alternative Approach for Continuous Control
neurips,2019,1,782,Guy,Tennenholtz,gmail,Technion,guytenn@gmail.com,Distributional Policy Optimization: An Alternative Approach for Continuous Control
neurips,2019,2,782,Shie,Mannor,technion,Technion,shie@ee.technion.ac.il,Distributional Policy Optimization: An Alternative Approach for Continuous Control
neurips,2019,0,3504,Sajin,Sasy,,University of Waterloo,,Oblivious Sampling Algorithms for Private Data Analysis
neurips,2019,1,3504,Olga,Ohrimenko,,Microsoft,,Oblivious Sampling Algorithms for Private Data Analysis
neurips,2019,0,9327,Alexey,Ignatiev,monash,"Reason Lab, Faculty of Sciences, University of Lisbon",alexey.ignatiev@monash.edu,On Relating Explanations and Adversarial Examples
neurips,2019,1,9327,Nina,Narodytska,vmware,VMmare Research,nnarodytska@vmware.com,On Relating Explanations and Adversarial Examples
neurips,2019,2,9327,Joao,Marques-Silva,univ-toulouse,"ANITI, Federal University of Toulouse Midi-Pyrénées",joao.marques-silva@univ-toulouse.fr,On Relating Explanations and Adversarial Examples
neurips,2019,0,5971,Aditya,Bhaskara,gmail,University of Utah,bhaskaraaditya@gmail.com,Greedy Sampling for Approximate Clustering in the Presence of Outliers
neurips,2019,1,5971,Sharvaree,Vadgama,gmail,University of Utah,sharvaree.vadgama@gmail.com,Greedy Sampling for Approximate Clustering in the Presence of Outliers
neurips,2019,2,5971,Hong,Xu,gmail,University of Utah,hxu.hongxu@gmail.com,Greedy Sampling for Approximate Clustering in the Presence of Outliers
neurips,2019,0,8876,Nima,Dehmamy,bu,Northeastern University,nimadt@bu.edu,Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology
neurips,2019,1,8876,Albert-Laszlo,Barabasi,neu,Northeastern University,alb@neu.edu,Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology
neurips,2019,2,8876,Rose,Yu,northeastern,Northeastern University,roseyu@northeastern.edu,Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology
neurips,2019,0,3463,Natasa,Tagasovska,unil,University of Lausanne,natasa.tagasovska@unil.ch,Single-Model Uncertainties for Deep Learning
neurips,2019,1,3463,David,Lopez-Paz,fb,Facebook AI Research,dlp@fb.com,Single-Model Uncertainties for Deep Learning
neurips,2019,0,1905,Nathan,Kallus,cornell,Cornell University,kallus@cornell.edu,The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric
neurips,2019,1,1905,Angela,Zhou,cornell,Cornell University,az434@cornell.edu,The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric
neurips,2019,0,3774,Rui,Zhang,gmail,Arizona State University,ruizhang8633@gmail.com,Robust Principal Component Analysis with Adaptive Neighbors
neurips,2019,1,3774,Hanghang,Tong,illinois,University of Illinois at Urbana-Champaign,htong@illinois.edu,Robust Principal Component Analysis with Adaptive Neighbors
neurips,2019,0,3470,Matteo,Togninalli,ethz,ETH Zürich,matteo.togninalli@bsse.ethz.ch,Wasserstein Weisfeiler-Lehman Graph Kernels
neurips,2019,1,3470,Elisabetta,Ghisu,ethz,ETH Zurich,elisabetta.ghisu@bsse.ethz.ch,Wasserstein Weisfeiler-Lehman Graph Kernels
neurips,2019,2,3470,Felipe,Llinares-López,ethz,ETH Zürich,felipe.llinares@bsse.ethz.ch,Wasserstein Weisfeiler-Lehman Graph Kernels
neurips,2019,3,3470,Bastian,Rieck,ethz,ETH Zurich,bastian.rieck@bsse.ethz.ch,Wasserstein Weisfeiler-Lehman Graph Kernels
neurips,2019,4,3470,Karsten,Borgwardt,ethz,ETH Zurich,karsten.borgwardt@bsse.ethz.ch,Wasserstein Weisfeiler-Lehman Graph Kernels
neurips,2019,0,477,Jianlong,Chang,ia,"National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",jianlong.chang@nlpr.ia.ac.cn,DATA: Differentiable ArchiTecture Approximation
neurips,2019,1,477,xinbang,zhang,ia,"Institute of Automation,Chinese  Academy of Science",xinbang.zhang@nlpr.ia.ac.cn,DATA: Differentiable ArchiTecture Approximation
neurips,2019,2,477,Yiwen,Guo,ia,Bytedance AI Lab,gfmeng@nlpr.ia.ac.cn,DATA: Differentiable ArchiTecture Approximation
neurips,2019,3,477,GAOFENG,MENG,ia,"Institute of Automation, Chinese Academy of Sciences",smxiang@nlpr.ia.ac.cn,DATA: Differentiable ArchiTecture Approximation
neurips,2019,4,477,SHIMING,XIANG,ia,"Chinese Academy of Sciences, China",chpan@nlpr.ia.ac.cn,DATA: Differentiable ArchiTecture Approximation
neurips,2019,5,477,Chunhong,Pan,bytedance,"Institute of Automation, Chinese Academy of Sciences",guoyiwen.ai@bytedance.com,DATA: Differentiable ArchiTecture Approximation
neurips,2019,0,7237,Sariel,Har-Peled,illinois,University of Illinois at Urbana-Champaign,sariel@illinois.edu,Near Neighbor: Who is the Fairest of Them All?
neurips,2019,1,7237,Sepideh,Mahabadi,ttic,Toyota Technological Institute at Chicago,mahabadi@ttic.edu,Near Neighbor: Who is the Fairest of Them All?
neurips,2019,0,4840,Yifeng,Fan,illinois,University of Illinois at Urbana-Champaign,yifengf2@illinois.edu,Unsupervised Co-Learning on $G$-Manifolds Across Irreducible Representations
neurips,2019,1,4840,Tingran,Gao,illinois,University of Chicago,zhizhenz@illinois.edu,Unsupervised Co-Learning on $G$-Manifolds Across Irreducible Representations
neurips,2019,2,4840,Zhizhen Jane,Zhao,uchicago,University of Illinois at Urbana Champaign,tingrangao@galton.uchicago.edu,Unsupervised Co-Learning on $G$-Manifolds Across Irreducible Representations
neurips,2019,0,2589,Supratik,Paul,ox,University of Oxford,supratik.paul@cs.ox.ac.uk,Fast Efficient Hyperparameter Tuning for Policy Gradient Methods
neurips,2019,1,2589,Vitaly,Kurin,ox,University of Oxford,vitaly.kurin@cs.ox.ac.uk,Fast Efficient Hyperparameter Tuning for Policy Gradient Methods
neurips,2019,2,2589,Shimon,Whiteson,ox,University of Oxford,shimon.whiteson@cs.ox.ac.uk,Fast Efficient Hyperparameter Tuning for Policy Gradient Methods
neurips,2019,0,1726,Zhiqing,Sun,cmu,Carnegie Mellon University,zhiqings@cs.cmu.edu,Fast Structured Decoding for Sequence Models
neurips,2019,1,1726,Zhuohan,Li,berkeley,UC Berkeley,zhuohan@cs.berkeley.edu,Fast Structured Decoding for Sequence Models
neurips,2019,2,1726,Haoqing,Wang,pku,Peking University,wanghaoqing@pku.edu.cn,Fast Structured Decoding for Sequence Models
neurips,2019,3,1726,Di,He,pku,Peking University,di_he@pku.edu.cn,Fast Structured Decoding for Sequence Models
neurips,2019,4,1726,Zi,Lin,pku,Peking University,zi.lin@pku.edu.cn,Fast Structured Decoding for Sequence Models
neurips,2019,5,1726,Zhihong,Deng,pku,Peking University,zhdeng@pku.edu.cn,Fast Structured Decoding for Sequence Models
neurips,2019,0,3202,Christopher,Criscitiello,gmail,"None, formerly Princeton University",ccriscitiello6@gmail.com,Efficiently escaping saddle points on manifolds
neurips,2019,1,3202,Nicolas,Boumal,princeton,Princeton University,nboumal@math.princeton.edu,Efficiently escaping saddle points on manifolds
neurips,2019,0,3090,Jianghong,Shi,uw,University of Washington,jhshi@uw.edu,Comparison Against Task Driven Artificial Neural Networks Reveals Functional Properties in Mouse Visual Cortex
neurips,2019,1,3090,Eric,Shea-Brown,uw,University of Washington,etsb@uw.edu,Comparison Against Task Driven Artificial Neural Networks Reveals Functional Properties in Mouse Visual Cortex
neurips,2019,2,3090,Michael,Buice,alleninstitute,Allen Institute for Brain Science,michaelbu@alleninstitute.org,Comparison Against Task Driven Artificial Neural Networks Reveals Functional Properties in Mouse Visual Cortex
neurips,2019,0,8531,Mariya,Toneva,cmu,Carnegie Mellon University,mariya@cmu.edu,Interpreting and improving natural-language processing (in machines) with  natural language-processing (in the brain)
neurips,2019,1,8531,Leila,Wehbe,cmu,Carnegie Mellon University,lwehbe@cmu.edu,Interpreting and improving natural-language processing (in machines) with  natural language-processing (in the brain)
neurips,2019,0,1853,Ali,Shafahi,umd,University of Maryland,ashafahi@cs.umd.edu,Adversarial training for free!
neurips,2019,1,1853,Mahyar,Najibi,umd,University of Maryland,najibi@cs.umd.edu,Adversarial training for free!
neurips,2019,2,1853,Mohammad Amin,Ghiasi,umd,University of Maryland,amin@cs.umd.edu,Adversarial training for free!
neurips,2019,3,1853,Zheng,Xu,umd,Google AI,xuzh@cs.umd.edu,Adversarial training for free!
neurips,2019,4,1853,John,Dickerson,umd,University of Maryland,john@cs.umd.edu,Adversarial training for free!
neurips,2019,5,1853,Christoph,Studer,cornell,Cornell University,studer@cornell.edu,Adversarial training for free!
neurips,2019,6,1853,Larry,Davis,umd,University of Maryland,lsd@umiacs.umd.edu,Adversarial training for free!
neurips,2019,7,1853,Gavin,Taylor,usna,US Naval Academy,taylor@usna.edu,Adversarial training for free!
neurips,2019,8,1853,Tom,Goldstein,umd,University of Maryland,tomg@cs.umd.edu,Adversarial training for free!
neurips,2019,0,881,Chundi,Liu,layer6,Layer6 AI,chundi@layer6.ai,Guided Similarity Separation for Image Retrieval
neurips,2019,1,881,Guangwei,Yu,layer6,Layer6,guang@layer6.ai,Guided Similarity Separation for Image Retrieval
neurips,2019,2,881,Maksims,Volkovs,layer6,Layer6 AI,jason@layer6.ai,Guided Similarity Separation for Image Retrieval
neurips,2019,3,881,Cheng,Chang,layer6,Layer6 AI,himanshu@layer6.ai,Guided Similarity Separation for Image Retrieval
neurips,2019,4,881,Himanshu,Rai,layer6,Layer6 AI,jeremy@layer6.ai,Guided Similarity Separation for Image Retrieval
neurips,2019,5,881,Junwei,Ma,layer6,Layer6 AI,satya@layer6.ai,Guided Similarity Separation for Image Retrieval
neurips,2019,6,881,Satya Krishna,Gorti,layer6,Layer6 AI,maks@layer6.ai,Guided Similarity Separation for Image Retrieval
neurips,2019,0,2632,Lixin,Fan,webank,WeBank AI Lab,lixinfan@webank.com,Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks
neurips,2019,1,2632,Kam Woh,Ng,um,University of Malaya,kamwoh@siswa.um.edu.my,Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks
neurips,2019,2,2632,Chee Seng,Chan,um,University of Malaya,cs.chan@um.edu.my,Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks
neurips,2019,0,1674,Charles,Corbière,valeo,Valeo.ai / CNAM,charles.corbiere@valeo.com,Addressing Failure Prediction by Learning Model Confidence
neurips,2019,1,1674,Nicolas,THOME,cnam,Cnam (Conservatoire national des arts et métiers),nicolas.thome@cnam.fr,Addressing Failure Prediction by Learning Model Confidence
neurips,2019,2,1674,Avner,Bar-Hen,cnam,"CNAM, Paris",avner@cnam.fr,Addressing Failure Prediction by Learning Model Confidence
neurips,2019,3,1674,Matthieu,Cord,lip6,Sorbonne University,matthieu.cord@lip6.fr,Addressing Failure Prediction by Learning Model Confidence
neurips,2019,4,1674,Patrick,Pérez,valeo,Valeo.ai,patrick.perez@valeo.com,Addressing Failure Prediction by Learning Model Confidence
neurips,2019,0,7205,Nikita,Ivkin,amazon,Amazon,ivkin@amazon.com,Communication-efficient Distributed SGD with Sketching
neurips,2019,1,7205,Daniel,Rothchild,berkeley,UC Berkeley,drothchild@berkeley.edu,Communication-efficient Distributed SGD with Sketching
neurips,2019,2,7205,Enayat,Ullah,jhu,Johns Hopkins University,enayat@jhu.edu,Communication-efficient Distributed SGD with Sketching
neurips,2019,3,7205,Vladimir,braverman,jhu,Johns Hopkins University,vova@cs.jhu.edu,Communication-efficient Distributed SGD with Sketching
neurips,2019,4,7205,Ion,Stoica,berkeley,UC Berkeley,istoica@berkeley.edu,Communication-efficient Distributed SGD with Sketching
neurips,2019,5,7205,Raman,Arora,jhu,Johns Hopkins University,arora@cs.jhu.edu,Communication-efficient Distributed SGD with Sketching
neurips,2019,0,914,Rui,Li,rit,Rochester Institute of Technology,rxlics@rit.edu,Multivariate Sparse Coding of Nonstationary Covariances with Gaussian Processes
neurips,2019,0,5875,Bo,Dai,,Google Brain,,Exponential Family Estimation via Adversarial Dynamics Embedding
neurips,2019,1,5875,Zhen,Liu,,"MILA, University of Montreal",,Exponential Family Estimation via Adversarial Dynamics Embedding
neurips,2019,2,5875,Hanjun,Dai,,Georgia Institute of Technology,,Exponential Family Estimation via Adversarial Dynamics Embedding
neurips,2019,3,5875,Niao,He,,UIUC,,Exponential Family Estimation via Adversarial Dynamics Embedding
neurips,2019,4,5875,Arthur,Gretton,,"Gatsby Unit, UCL",,Exponential Family Estimation via Adversarial Dynamics Embedding
neurips,2019,5,5875,Le,Song,,Georgia Institute of Technology,,Exponential Family Estimation via Adversarial Dynamics Embedding
neurips,2019,6,5875,Dale,Schuurmans,,Google Inc.,,Exponential Family Estimation via Adversarial Dynamics Embedding
neurips,2019,0,8761,Xueru,Zhang,umich,University of Michigan,xueru@umich.edu,Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness
neurips,2019,1,8761,Mohammadmahdi,Khaliligarekani,umich,university of michigan,khalili@umich.edu,Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness
neurips,2019,2,8761,Cem,Tekin,bilkent,Bilkent University,cemtekin@ee.bilkent.edu.tr,Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness
neurips,2019,3,8761,mingyan,liu,umich,"university of Michigan, Ann Arbor",mingyan@umich.edu,Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness
neurips,2019,0,7064,Don,Dennis,,Carnegie Mellon University,,Shallow RNN:  Accurate Time-series Classification on Resource Constrained Devices
neurips,2019,1,7064,Durmus Alp Emre,Acar,,Boston University,,Shallow RNN:  Accurate Time-series Classification on Resource Constrained Devices
neurips,2019,2,7064,Vikram,Mandikal,,The University of Texas at Austin,,Shallow RNN:  Accurate Time-series Classification on Resource Constrained Devices
neurips,2019,3,7064,Vinu Sankar,Sadasivan,,Indian Institute of Technology Gandhinagar,,Shallow RNN:  Accurate Time-series Classification on Resource Constrained Devices
neurips,2019,4,7064,Venkatesh,Saligrama,,Boston University,,Shallow RNN:  Accurate Time-series Classification on Resource Constrained Devices
neurips,2019,5,7064,Harsha Vardhan,Simhadri,,Microsoft Research,,Shallow RNN:  Accurate Time-series Classification on Resource Constrained Devices
neurips,2019,6,7064,Prateek,Jain,,Microsoft Research,,Shallow RNN:  Accurate Time-series Classification on Resource Constrained Devices
neurips,2019,0,5277,Ricky T. Q.,Chen,toronto,U of Toronto,rtqichen@cs.toronto.edu,Neural Networks with Cheap Differential Operators
neurips,2019,1,5277,David,Duvenaud,toronto,University of Toronto,duvenaud@cs.toronto.edu,Neural Networks with Cheap Differential Operators
neurips,2019,0,4313,Tianyi,Liu,,Georgia Institute of Technolodgy,,Towards Understanding the Importance of Shortcut Connections in Residual Networks
neurips,2019,1,4313,Minshuo,Chen,,Georgia Tech,,Towards Understanding the Importance of Shortcut Connections in Residual Networks
neurips,2019,2,4313,Mo,Zhou,,Duke University,,Towards Understanding the Importance of Shortcut Connections in Residual Networks
neurips,2019,3,4313,Simon,Du,,Institute for Advanced Study,,Towards Understanding the Importance of Shortcut Connections in Residual Networks
neurips,2019,4,4313,Enlu,Zhou,,Georgia Institute of Technology,,Towards Understanding the Importance of Shortcut Connections in Residual Networks
neurips,2019,5,4313,Tuo,Zhao,,Gatech,,Towards Understanding the Importance of Shortcut Connections in Residual Networks
neurips,2019,0,4190,Brian,Axelrod,stanford,Stanford,baxelrod@cs.stanford.edu,A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families
neurips,2019,1,4190,Ilias,Diakonikolas,gmail,UW Madison,sidiropo@gmail.com,A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families
neurips,2019,2,4190,Alistair,Stewart,gmail,University of Southern California,ilias.diakonikolas@gmail.com,A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families
neurips,2019,3,4190,Anastasios,Sidiropoulos,gmail,University of Illinois at Chicago,stewart.al@gmail.com,A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families
neurips,2019,4,4190,Gregory,Valiant,stanford,Stanford University,gvaliant@stanford.edu,A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families
neurips,2019,0,4967,Amirata,Ghorbani,stanford,Stanford University,amiratag@stanford.edu,Towards Automatic Concept-based Explanations
neurips,2019,1,4967,James,Wexler,stanford,,jamesz@stanford.edu,Towards Automatic Concept-based Explanations
neurips,2019,2,4967,James,Zou,google,Stanford University,jwexler@google.com,Towards Automatic Concept-based Explanations
neurips,2019,3,4967,Been,Kim,google,Google,beenkim@google.com,Towards Automatic Concept-based Explanations
neurips,2019,0,6969,Jonas,Kubilius,,"MIT, KU Leuven, Three Thirds",,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,1,6969,Martin,Schrimpf,,MIT,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,2,6969,Kohitij,Kar,,MIT,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,3,6969,Rishi,Rajalingham,,MIT,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,4,6969,Ha,Hong,,Bay Labs Inc.,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,5,6969,Najib,Majaj,,NYU,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,6,6969,Elias,Issa,,Columbia University,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,7,6969,Pouya,Bashivan,,MIT,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,8,6969,Jonathan,Prescott-Roy,,MIT,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,9,6969,Kailyn,Schmidt,,MIT,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,10,6969,Aran,Nayebi,,Stanford University,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,11,6969,Daniel,Bear,,Stanford University,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,12,6969,Daniel,Yamins,,Stanford University,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,13,6969,James,DiCarlo,,Massachusetts Institute of Technology,,Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs
neurips,2019,0,7817,Ximing,Qiao,duke,Duke University,ximing.qiao@duke.edu,Defending Neural Backdoors via Generative Distribution Modeling
neurips,2019,1,7817,Yukun,Yang,duke,Duke University,yukun.yang@duke.edu,Defending Neural Backdoors via Generative Distribution Modeling
neurips,2019,2,7817,Hai,Li,duke,Duke University,hai.li@duke.edu,Defending Neural Backdoors via Generative Distribution Modeling
neurips,2019,0,4985,Sanchit,Kalhan,,Northwestern University,,Correlation clustering with local objectives
neurips,2019,1,4985,Konstantin,Makarychev,,Northwestern University,,Correlation clustering with local objectives
neurips,2019,2,4985,Timothy,Zhou,,University of Illinois at UrbanaChampaign,,Correlation clustering with local objectives
neurips,2019,0,5371,Naman,Agarwal,google,Google,namanagarwal@google.com,Logarithmic Regret for Online Control
neurips,2019,1,5371,Elad,Hazan,princeton,Princeton University,ehazan@princeton.edu,Logarithmic Regret for Online Control
neurips,2019,2,5371,Karan,Singh,princeton,Princeton University,karans@princeton.edu,Logarithmic Regret for Online Control
neurips,2019,0,2590,Ian,Char,cmu,Carnegie Mellon University,ichar@cs.cmu.edu,Offline Contextual Bayesian Optimization
neurips,2019,1,2590,Youngseog,Chung,cmu,Carnegie Mellon University,youngsec@cs.cmu.edu,Offline Contextual Bayesian Optimization
neurips,2019,2,2590,Willie,Neiswanger,cmu,Carnegie Mellon University,willie@cs.cmu.edu,Offline Contextual Bayesian Optimization
neurips,2019,3,2590,Kirthevasan,Kandasamy,cmu,Carnegie Mellon University,schneide@cs.cmu.edu,Offline Contextual Bayesian Optimization
neurips,2019,4,2590,Andrew,Nelson,berkeley,Princeton Plasma Physics Lab,kandasamy@eecs.berkeley.edu,Offline Contextual Bayesian Optimization
neurips,2019,5,2590,Mark,Boyer,pppl,Princeton Plasma Physics Lab,anelson@pppl.gov,Offline Contextual Bayesian Optimization
neurips,2019,6,2590,Egemen,Kolemen,pppl,Princeton Plasma Physics Lab,mboyer@pppl.gov,Offline Contextual Bayesian Optimization
neurips,2019,7,2590,Jeff,Schneider,pppl,Carnegie Mellon University,ekolemen@pppl.gov,Offline Contextual Bayesian Optimization
neurips,2019,0,1434,Atsutoshi,Kumagai,ntt,NTT,atsutoshi.kumagai.ht@hco.ntt.co.jp,Transfer Anomaly Detection by Inferring Latent Domain Representations
neurips,2019,1,1434,Tomoharu,Iwata,ntt,NTT,tomoharu.iwata.gy@hco.ntt.co.jp,Transfer Anomaly Detection by Inferring Latent Domain Representations
neurips,2019,2,1434,Yasuhiro,Fujiwara,ntt,NTT Communication Science Laboratories,yasuhiro.fujiwara.kh@hco.ntt.co.jp,Transfer Anomaly Detection by Inferring Latent Domain Representations
neurips,2019,0,7007,Marin,Bilo,,Technical University of Munich,,Uncertainty on Asynchronous Time Event Prediction
neurips,2019,1,7007,Bertrand,Charpentier,,Technical University of Munich,,Uncertainty on Asynchronous Time Event Prediction
neurips,2019,2,7007,Stephan,Günnemann,,Technical University of Munich,,Uncertainty on Asynchronous Time Event Prediction
neurips,2019,0,2742,Chuan,Guo,cornell,Cornell University,cg563@cornell.edu,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces
neurips,2019,1,2742,Ali,Mousavi,google,Google Brain,alimous@google.com,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces
neurips,2019,2,2742,Xiang,Wu,bytedance,ByteDance,xiang.wu@bytedance.com,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces
neurips,2019,3,2742,Daniel,Holtmann-Rice,google,Google Inc,dhr@google.com,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces
neurips,2019,4,2742,Satyen,Kale,google,Google,satyenkale@google.com,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces
neurips,2019,5,2742,Sashank,Reddi,google,Google,sashank@google.com,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces
neurips,2019,6,2742,Sanjiv,Kumar,google,Google Research,sanjivk@google.com,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces
neurips,2019,0,8763,Digvijay,Boob,gatech,Georgia Institute of Technology,digvijaybb40@gatech.edu,Faster width-dependent algorithm for mixed packing and covering LPs
neurips,2019,1,8763,Saurabh,Sawlani,gatech,Georgia Institute of Technology,sawlani@gatech.edu,Faster width-dependent algorithm for mixed packing and covering LPs
neurips,2019,2,8763,Di,Wang,google,Google AI,wadi@google.com,Faster width-dependent algorithm for mixed packing and covering LPs
neurips,2019,0,5293,Hengyuan,Hu,fb,Facebook,hengyuan@fb.com,Hierarchical Decision Making by Generating and Following Natural Language Instructions
neurips,2019,1,5293,Denis,Yarats,nyu,New York University,denisyarats@cs.nyu.edu,Hierarchical Decision Making by Generating and Following Natural Language Instructions
neurips,2019,2,5293,Qucheng,Gong,fb,Facebook AI Research,qucheng@fb.com,Hierarchical Decision Making by Generating and Following Natural Language Instructions
neurips,2019,3,5293,Yuandong,Tian,fb,Facebook AI Research,yuandong@fb.com,Hierarchical Decision Making by Generating and Following Natural Language Instructions
neurips,2019,4,5293,Mike,Lewis,fb,Facebook AI Research,mikelewis@fb.com,Hierarchical Decision Making by Generating and Following Natural Language Instructions
neurips,2019,0,6585,Mathieu,Blondel,mblondel,Google,mathieu@mblondel.org,Structured Prediction with Projection Oracles
neurips,2019,0,5057,Youssef,Mroueh,,IBM T.J Watson Research Center,,Sobolev Independence Criterion
neurips,2019,1,5057,Tom,Sercu,,Facebook AI Research,,Sobolev Independence Criterion
neurips,2019,2,5057,Mattia,Rigotti,,IBM Research AI,,Sobolev Independence Criterion
neurips,2019,3,5057,Inkit,Padhi,,IBM Research,,Sobolev Independence Criterion
neurips,2019,4,5057,Cicero,Nogueira dos Santos,,Amazon AWS AI,,Sobolev Independence Criterion
neurips,2019,0,7499,Ashia,Wilson,microsoft,UC Berkeley,ashia.wilson@microsoft.com,Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions
neurips,2019,1,7499,Lester,Mackey,microsoft,Microsoft Research,lmackey@microsoft.com,Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions
neurips,2019,2,7499,Andre,Wibisono,gatech,Georgia Tech,wibisono@gatech.edu,Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions
neurips,2019,0,1414,Xiyang,Liu,washington,University of Washington,xiyangl@cs.washington.edu,Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases
neurips,2019,1,1414,Sewoong,Oh,washington,University of Washington,sewoong@cs.washington.edu,Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases
neurips,2019,0,4896,Ghassen,Jerfel,duke,Duke University,gj47@duke.edu,Reconciling meta-learning and continual learning with online mixtures of tasks
neurips,2019,1,4896,Erin,Grant,princeton,UC Berkeley,tomg@princeton.edu,Reconciling meta-learning and continual learning with online mixtures of tasks
neurips,2019,2,4896,Tom,Griffiths,berkeley,Princeton University,eringrant@berkeley.edu,Reconciling meta-learning and continual learning with online mixtures of tasks
neurips,2019,3,4896,Katherine,Heller,duke,Google,kheller@stat.duke.edu,Reconciling meta-learning and continual learning with online mixtures of tasks
neurips,2019,0,4084,Conor,Durkan,ed,University of Edinburgh,conor.durkan@ed.ac.uk,Neural Spline Flows
neurips,2019,1,4084,Artur,Bekasov,ed,University of Edinburgh,artur.bekasov@ed.ac.uk,Neural Spline Flows
neurips,2019,2,4084,Iain,Murray,ed,University of Edinburgh,i.murray@ed.ac.uk,Neural Spline Flows
neurips,2019,3,4084,George,Papamakarios,ed,DeepMind,g.papamakarios@ed.ac.uk,Neural Spline Flows
neurips,2019,0,2381,Yaqi,Xie,nus,National University of Singapore,yaqixie@comp.nus.edu.sg,Embedding Symbolic Knowledge into Deep Networks
neurips,2019,1,2381,Ziwei,Xu,nus,National University of Singapore,ziwei-xu@comp.nus.edu.sg,Embedding Symbolic Knowledge into Deep Networks
neurips,2019,2,2381,Mohan,Kankanhalli,nus,"National University of Singapore,",mohan@comp.nus.edu.sg,Embedding Symbolic Knowledge into Deep Networks
neurips,2019,3,2381,Kuldeep S,Meel,nus,National University of Singapore,meel@comp.nus.edu.sg,Embedding Symbolic Knowledge into Deep Networks
neurips,2019,4,2381,Harold,Soh,nus,National University of Singapore (NUS),harold@comp.nus.edu.sg,Embedding Symbolic Knowledge into Deep Networks
neurips,2019,0,1311,Xiangyu,Zheng,,Peking University,,Partitioning Structure Learning for Segmented Linear Regression Trees
neurips,2019,1,1311,Song Xi,Chen,,Peking University,,Partitioning Structure Learning for Segmented Linear Regression Trees
neurips,2019,0,6102,Trevor,Campbell,ubc,UBC,trevor@stat.ubc.ca,Sparse Variational Inference: Bayesian Coresets from Scratch
neurips,2019,1,6102,Boyan,Beronov,ubc,University of British Columbia,beronov@cs.ubc.ca,Sparse Variational Inference: Bayesian Coresets from Scratch
neurips,2019,0,2684,Andrew,Bennett,cornell,Cornell University,awb222@cornell.edu,Policy Evaluation with Latent Confounders via Optimal Balance
neurips,2019,1,2684,Nathan,Kallus,cornell,Cornell University,kallus@cornell.edu,Policy Evaluation with Latent Confounders via Optimal Balance
neurips,2019,0,1948,Hsin-Ying,Lee,,"University of California, Merced",,Dancing to Music
neurips,2019,1,1948,Xiaodong,Yang,,QCraft,,Dancing to Music
neurips,2019,2,1948,Ming-Yu,Liu,,Nvidia Research,,Dancing to Music
neurips,2019,3,1948,Ting-Chun,Wang,,NVIDIA,,Dancing to Music
neurips,2019,4,1948,Yu-Ding,Lu,,UC Merced,,Dancing to Music
neurips,2019,5,1948,Ming-Hsuan,Yang,,Google / UC Merced,,Dancing to Music
neurips,2019,6,1948,Jan,Kautz,,NVIDIA,,Dancing to Music
neurips,2019,0,1661,Alexej,Klushyn,argmax,Volkswagen Group,alexej.klushyn@argmax.ai,Learning Hierarchical Priors in VAEs
neurips,2019,1,1661,Nutan,Chen,argmax,Volkswagen Group,nutan.chen@argmax.ai,Learning Hierarchical Priors in VAEs
neurips,2019,2,1661,Richard,Kurle,argmax,Volkswagen Group,richardk@argmax.ai,Learning Hierarchical Priors in VAEs
neurips,2019,3,1661,Botond,Cseke,argmax,Volkswagen Group,botond.cseke@argmax.ai,Learning Hierarchical Priors in VAEs
neurips,2019,4,1661,Patrick,van der Smagt,argmax,Volkswagen Group,smagt@argmax.ai,Learning Hierarchical Priors in VAEs
neurips,2019,0,4202,Xuechen,Li,toronto,Google,lxuechen@cs.toronto.edu,Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond
neurips,2019,1,4202,Yi,Wu,toronto,University of Toronto & Vector Institute,dennywu@cs.toronto.edu,Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond
neurips,2019,2,4202,Lester,Mackey,toronto,Microsoft Research,erdogdu@cs.toronto.edu,Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond
neurips,2019,3,4202,Murat,Erdogdu,microsoft,University of Toronto & Vector Institute,lmackey@microsoft.com,Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond
neurips,2019,0,3508,Roman,Beliy,weizmann,weizmann institute,roman.beliy@weizmann.ac.il,From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI
neurips,2019,1,3508,Guy,Gaziv,weizmann,Weizmann Institute of Science,guy.gaziv@weizmann.ac.il,From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI
neurips,2019,2,3508,Assaf,Hoogi,weizmann,Weizmann Institute,assaf.hoogi@weizmann.ac.il,From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI
neurips,2019,3,3508,Francesca,Strappini,gmail,Weizmann Institute of Science,francescastrappini@gmail.com,From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI
neurips,2019,4,3508,Tal,Golan,columbia,Columbia University,tal.golan@columbia.edu,From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI
neurips,2019,5,3508,Michal,Irani,weizmann,Weizmann Institute of Science,michal.irani@weizmann.ac.il,From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI
neurips,2019,0,1474,Boxin,Zhao,uchicago,UChicago,boxinz@uchicago.edu,Direct Estimation of Differential Functional Graphical Models
neurips,2019,1,1474,Y. Samuel,Wang,uchicago,U of Chicago,swang24@uchicago.edu,Direct Estimation of Differential Functional Graphical Models
neurips,2019,2,1474,Mladen,Kolar,chicagobooth,University of Chicago,mkolar@chicagobooth.edu,Direct Estimation of Differential Functional Graphical Models
neurips,2019,0,1776,Wei,Wang,,EPFL,,Backpropagation-Friendly Eigendecomposition
neurips,2019,1,1776,Zheng,Dang,,Xi'an Jiaotong University,,Backpropagation-Friendly Eigendecomposition
neurips,2019,2,1776,Yinlin,Hu,,EPFL,,Backpropagation-Friendly Eigendecomposition
neurips,2019,3,1776,Pascal,Fua,,"EPFL, Switzerland",,Backpropagation-Friendly Eigendecomposition
neurips,2019,4,1776,Mathieu,Salzmann,,EPFL,,Backpropagation-Friendly Eigendecomposition
neurips,2019,0,8237,Andrey,Malinin,yandex-team,Yandex Research,am969@yandex-team.ru,Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness
neurips,2019,1,8237,Mark,Gales,cam,University of Cambridge,mjfg@eng.cam.ac.uk,Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness
neurips,2019,0,5974,Shuangfei,Zhai,apple,Apple,szhai@apple.com,Adversarial Fisher Vectors for Unsupervised Representation Learning
neurips,2019,1,5974,Walter,Talbott,apple,Apple,wtalbott@apple.com,Adversarial Fisher Vectors for Unsupervised Representation Learning
neurips,2019,2,5974,Carlos,Guestrin,apple,Apple & University of Washington,guestrin@apple.com,Adversarial Fisher Vectors for Unsupervised Representation Learning
neurips,2019,3,5974,Joshua,Susskind,apple,Apple Inc.,jsusskind@apple.com,Adversarial Fisher Vectors for Unsupervised Representation Learning
neurips,2019,0,5016,James,Lucas,,University of Toronto,,Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse
neurips,2019,1,5016,George,Tucker,,Google Brain,,Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse
neurips,2019,2,5016,Roger,Grosse,,University of Toronto,,Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse
neurips,2019,3,5016,Mohammad,Norouzi,,Google Brain,,Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse
neurips,2019,0,1875,Kevin,Liang,duke,Duke University,kevin.liang@duke.edu,Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods
neurips,2019,1,1875,Guoyin,Wang,duke,Duke University,guoyin.wang@duke.edu,Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods
neurips,2019,2,1875,Yitong,Li,duke,Duke University,yitong.li@duke.edu,Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods
neurips,2019,3,1875,Ricardo,Henao,duke,Duke University,ricardo.henao@duke.edu,Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods
neurips,2019,4,1875,Lawrence,Carin,duke,Duke University,lcarin@duke.edu,Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods
neurips,2019,0,419,Zhao,Song,gmail,University of Washington,magic.linuxkde@gmail.com,Efficient Symmetric Norm Regression via Linear Sketching
neurips,2019,1,419,Ruosong,Wang,cmu,Carnegie Mellon University,ruosongw@andrew.cmu.edu,Efficient Symmetric Norm Regression via Linear Sketching
neurips,2019,2,419,Lin,Yang,ucla,UCLA,linyang@ee.ucla.edu,Efficient Symmetric Norm Regression via Linear Sketching
neurips,2019,3,419,Hongyang,Zhang,ttic,TTIC,hongyanz@ttic.edu,Efficient Symmetric Norm Regression via Linear Sketching
neurips,2019,4,419,Peilin,Zhong,columbia,Columbia University,pz2225@columbia.edu,Efficient Symmetric Norm Regression via Linear Sketching
neurips,2019,0,3731,Gaël,Letarte,ulaval,Université Laval,gael.letarte.1@ulaval.ca,Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks
neurips,2019,1,3731,Pascal,Germain,inria,INRIA,benjamin.guedj@inria.fr,Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks
neurips,2019,2,3731,Benjamin,Guedj,inria,Inria & University College London,pascal.germain@inria.fr,Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks
neurips,2019,3,3731,Francois,Laviolette,ulaval,Université Laval,francois.laviolette@ift.ulaval.ca,Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks
neurips,2019,0,9264,Ke,Li,berkeley,UC Berkeley,ke.li@eecs.berkeley.edu,Approximate Feature Collisions in Neural Nets
neurips,2019,1,9264,Tianhao,Zhang,nju,Nanjing University,bryanzhang@smail.nju.edu.cn,Approximate Feature Collisions in Neural Nets
neurips,2019,2,9264,Jitendra,Malik,berkeley,University of California at Berkley,malik@eecs.berkeley.edu,Approximate Feature Collisions in Neural Nets
neurips,2019,0,2889,Daniel,McDuff,microsoft,Microsoft Research,damcduff@microsoft.com,Characterizing Bias in Classifiers using Generative Models
neurips,2019,1,2889,Shuang,Ma,microsoft,SUNY Buffalo,yalesong@microsoft.com,Characterizing Bias in Classifiers using Generative Models
neurips,2019,2,2889,Yale,Song,microsoft,Microsoft,akapoor@microsoft.com,Characterizing Bias in Classifiers using Generative Models
neurips,2019,3,2889,Ashish,Kapoor,buffalo,Microsoft,shuangma@buffalo.edu,Characterizing Bias in Classifiers using Generative Models
neurips,2019,0,3951,Sebastian,Mair,leuphana,Leuphana University,mair@leuphana.de,Coresets for Archetypal Analysis
neurips,2019,1,3951,Ulf,Brefeld,leuphana,Leuphana,brefeld@leuphana.de,Coresets for Archetypal Analysis
neurips,2019,0,4034,Sushrut,Karmalkar,utexas,The University of Texas at Austin,sushrutk@cs.utexas.edu,List-decodable Linear Regression
neurips,2019,1,4034,Adam,Klivans,utexas,UT Austin,klivans@cs.utexas.edu,List-decodable Linear Regression
neurips,2019,2,4034,Pravesh,Kothari,princeton,Princeton University and Institute for Advanced Study,kothari@cs.princeton.edu,List-decodable Linear Regression
neurips,2019,0,3770,Shagun,Ajmera,gmail,Indian Institute of Science,ajmerashagun@gmail.com,Infra-slow brain dynamics as a marker for cognitive function and decline
neurips,2019,1,3770,Shreya,Rajagopal,gmail,Indian Institute of Science,shreyakr96@gmail.com,Infra-slow brain dynamics as a marker for cognitive function and decline
neurips,2019,2,3770,Razi,Rehman,gmail,Indian Institute of Science,razirmp@gmail.com,Infra-slow brain dynamics as a marker for cognitive function and decline
neurips,2019,3,3770,Devarajan,Sridharan,iisc,Indian Institute of Science,sridhar@iisc.ac.in,Infra-slow brain dynamics as a marker for cognitive function and decline
neurips,2019,0,1687,Juyeon,Heo,gmail,Sungkyunkwan University,heojuyeon12@gmail.com,Fooling Neural Network Interpretations via Adversarial Model Manipulation
neurips,2019,1,1687,Sunghwan,Joo,skku,Sungkyunkwan University,shjoo840@skku.edu,Fooling Neural Network Interpretations via Adversarial Model Manipulation
neurips,2019,2,1687,Taesup,Moon,skku,Sungkyunkwan University (SKKU),tsmoon@skku.edu,Fooling Neural Network Interpretations via Adversarial Model Manipulation
neurips,2019,0,5058,Chenjun,Xiao,ualberta,University of Alberta,chenjun@ualberta.com,Maximum Entropy Monte-Carlo Planning
neurips,2019,1,5058,Ruitong,Huang,ualberta,Borealis AI,jmei2@ualberta.com,Maximum Entropy Monte-Carlo Planning
neurips,2019,2,5058,Jincheng,Mei,ualberta,University of Alberta,daes@ualberta.com,Maximum Entropy Monte-Carlo Planning
neurips,2019,3,5058,Dale,Schuurmans,ualberta,Google,mmueller@ualberta.com,Maximum Entropy Monte-Carlo Planning
neurips,2019,4,5058,Martin,Müller,borealisai,University of Alberta,ruitong.huang@borealisai.com,Maximum Entropy Monte-Carlo Planning
neurips,2019,0,5951,Yi,Hao,ucsd,"University of California, San Diego",yih179@ucsd.edu,Unified Sample-Optimal Property Estimation in Near-Linear Time
neurips,2019,1,5951,Alon,Orlitsky,ucsd,"University of California, San Diego",alon@ucsd.edu,Unified Sample-Optimal Property Estimation in Near-Linear Time
neurips,2019,0,2092,Yunji,Kim,yonsei,Yonsei University,kim_yunji@yonsei.ac.kr,Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction
neurips,2019,1,2092,Seonghyeon,Nam,yonsei,Yonsei University,shnnam@yonsei.ac.kr,Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction
neurips,2019,2,2092,In,Cho,yonsei,Yonsei University,join@yonsei.ac.kr,Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction
neurips,2019,3,2092,Seon Joo,Kim,yonsei,Yonsei University / Facebook,seonjookim@yonsei.ac.kr,Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction
neurips,2019,0,5852,Xiaoyi,Gu,cmu,Carnegie Mellon University,xgu1@andrew.cmu.edu,Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection
neurips,2019,1,5852,Leman,Akoglu,cmu,CMU,lakoglu@andrew.cmu.edu,Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection
neurips,2019,2,5852,Alessandro,Rinaldo,cmu,CMU,arinaldo@cmu.edu,Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection
neurips,2019,0,2284,Suraj,Srinivas,idiap,Idiap Research Institute & EPFL,suraj.srinivas@idiap.ch,Full-Gradient Representation for Neural Network Visualization
neurips,2019,1,2284,François,Fleuret,idiap,Idiap,francois.fleuret@idiap.ch,Full-Gradient Representation for Neural Network Visualization
neurips,2019,0,964,Lin,Song,xjtu,Xi'an Jiaotong University,stevengrove@stu.xjtu.edu.cn,Learnable Tree Filter for Structure-preserving Feature Transform
neurips,2019,1,964,Yanwei,Li,ia,"Institute of Automation, Chinese Academy of Sciences",liyanwei2017@ia.ac.cn,Learnable Tree Filter for Structure-preserving Feature Transform
neurips,2019,2,964,Zeming,Li,xjtu,Megvii(Face++) Inc,hsun@mail.xjtu.edu.cn,Learnable Tree Filter for Structure-preserving Feature Transform
neurips,2019,3,964,Gang,Yu,xjtu,Megvii Inc,nnzheng@mail.xjtu.edu.cn,Learnable Tree Filter for Structure-preserving Feature Transform
neurips,2019,4,964,Hongbin,Sun,megvii,Xi'an Jiaotong University,lizeming@megvii.com,Learnable Tree Filter for Structure-preserving Feature Transform
neurips,2019,5,964,Jian,Sun,megvii,"Megvii, Face++",yugang@megvii.com,Learnable Tree Filter for Structure-preserving Feature Transform
neurips,2019,6,964,Nanning,Zheng,megvii,Xi'an Jiaotong University,sunjian@megvii.com,Learnable Tree Filter for Structure-preserving Feature Transform
neurips,2019,0,7786,Kirill,Neklyudov,,"Samsung AI Center, Moscow",,The Implicit Metropolis-Hastings Algorithm
neurips,2019,1,7786,Evgenii,Egorov,,Skolkovo Institute of Science and Technology,,The Implicit Metropolis-Hastings Algorithm
neurips,2019,2,7786,Dmitry,Vetrov,,"Higher School of Economics, Samsung AI Center, Moscow",,The Implicit Metropolis-Hastings Algorithm
neurips,2019,0,1451,Chen,Dan,cmu,Carnegie Mellon University,cdan@cs.cmu.edu,Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation
neurips,2019,1,1451,Hong,Wang,gmail,Massachusetts Institute of Technology,Hong.Wang1991@gmail.com,Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation
neurips,2019,2,1451,Hongyang,Zhang,ttic,TTIC,honyanz@ttic.edu,Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation
neurips,2019,3,1451,Yuchen,Zhou,wisc,"University of Wisconsin, Madison",yuchenzhou@stat.wisc.edu,Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation
neurips,2019,4,1451,Pradeep,Ravikumar,cmu,Carnegie Mellon University,pradeepr@cs.cmu.edu,Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation
neurips,2019,0,6099,Shuai,Zheng,amazon,Hong Kong University of Science and Technology / Amazon Web Services,shzheng@amazon.com,Communication-Efficient Distributed Blockwise Momentum SGD with Error-Feedback
neurips,2019,1,6099,Ziyue,Huang,ust,Hong Kong University of Science and Technology,zhuangbq@cse.ust.hk,Communication-Efficient Distributed Blockwise Momentum SGD with Error-Feedback
neurips,2019,2,6099,James,Kwok,ust,Hong Kong University of Science and Technology,jamesk@cse.ust.hk,Communication-Efficient Distributed Blockwise Momentum SGD with Error-Feedback
neurips,2019,0,4148,Lingxiao,Huang,,EPFL,,Coresets for Clustering with Fairness Constraints
neurips,2019,1,4148,Shaofeng,Jiang,,Weizmann Institute of Science,,Coresets for Clustering with Fairness Constraints
neurips,2019,2,4148,Nisheeth,Vishnoi,,Yale University,,Coresets for Clustering with Fairness Constraints
neurips,2019,0,95,Dinghuai,Zhang,pku,Peking University,zhangdinghuai@pku.edu.cn,You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle
neurips,2019,1,95,Tianyuan,Zhang,pku,Peking University,1600012888@pku.edu.cn,You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle
neurips,2019,2,95,Yiping,Lu,stanford,Peking University,yplu@stanford.edu,You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle
neurips,2019,3,95,Zhanxing,Zhu,pku,Peking University,zhanxing.zhu@pku.edu.cn,You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle
neurips,2019,4,95,Bin,Dong,pku,Peking University,dongbin@math.pku.edu.cn,You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle
neurips,2019,0,4058,Pascale,Gourdeau,ox,University of Oxford,pascale.gourdeau@cs.ox.ac.uk,On the Hardness of Robust Classification
neurips,2019,1,4058,Varun,Kanade,ox,University of Oxford,varunk@cs.ox.ac.uk,On the Hardness of Robust Classification
neurips,2019,2,4058,Marta,Kwiatkowska,ox,University of Oxford,marta.kwiatkowska@cs.ox.ac.uk,On the Hardness of Robust Classification
neurips,2019,3,4058,James,Worrell,ox,University of Oxford,james.worrell@cs.ox.ac.uk,On the Hardness of Robust Classification
neurips,2019,0,6386,Carlos,Riquelme,,Google Brain,,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates
neurips,2019,1,6386,Hugo,Penedones,,Google DeepMind,,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates
neurips,2019,2,6386,Damien,Vincent,,Google Brain,,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates
neurips,2019,3,6386,Hartmut,Maennel,,Google,,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates
neurips,2019,4,6386,Sylvain,Gelly,,Google Brain (Zurich),,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates
neurips,2019,5,6386,Timothy,Mann,,DeepMind,,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates
neurips,2019,6,6386,Andre,Barreto,,DeepMind,,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates
neurips,2019,7,6386,Gergely,Neu,,Universitat Pompeu Fabra,,Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates
neurips,2019,0,812,Siyuan,Li,tsinghua,Tsinghua University,sy-li17@mails.tsinghua.edu.cn,Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards
neurips,2019,1,812,Rui,Wang,stanford,Stanford University,rui1@stanford.edu,Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards
neurips,2019,2,812,Minxue,Tang,tsinghua,Tsinghua University,tangmx16@mails.tsinghua.edu.cn,Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards
neurips,2019,3,812,Chongjie,Zhang,tsinghua,Tsinghua University,chongjie@tsinghua.edu.cn,Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards
neurips,2019,0,175,Peter,Anderson,gatech,Georgia Tech,peter.anderson@gatech.edu,Chasing Ghosts: Instruction Following as Bayesian State Tracking
neurips,2019,1,175,Ayush,Shrivastava,gatech,Georgia Institute of Technology,ayshrv@gatech.edu,Chasing Ghosts: Instruction Following as Bayesian State Tracking
neurips,2019,2,175,Devi,Parikh,gatech,Georgia Tech / Facebook AI Research (FAIR),parikh@gatech.edu,Chasing Ghosts: Instruction Following as Bayesian State Tracking
neurips,2019,3,175,Dhruv,Batra,gatech,Georgia Tech / Facebook AI Research (FAIR),dbatra@gatech.edu,Chasing Ghosts: Instruction Following as Bayesian State Tracking
neurips,2019,4,175,Stefan,Lee,oregonstate,Georgia Institute of Technology,leestef@oregonstate.edu,Chasing Ghosts: Instruction Following as Bayesian State Tracking
neurips,2019,0,7406,Junzhe,Zhang,columbia,Columbia University,eb@cs.columbia.edu,Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes
neurips,2019,1,7406,Elias,Bareinboim,columbia,Purdue,junzhez@cs.columbia.edu,Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes
neurips,2019,0,1346,Dror,Simon,technion,Technion,dror.simon@cs.technion.ac.il,Rethinking the CSC Model for Natural Images
neurips,2019,1,1346,Michael,Elad,technion,Technion,elad@cs.technion.ac.il,Rethinking the CSC Model for Natural Images
neurips,2019,0,162,Justin,Domke,,"University of Massachusetts, Amherst",,Divide and Couple: Using Monte Carlo Variational Objectives for Posterior Approximation
neurips,2019,1,162,Daniel,Sheldon,,University of Massachusetts Amherst,,Divide and Couple: Using Monte Carlo Variational Objectives for Posterior Approximation
neurips,2019,0,1189,Tao,Yu,cornell,Cornell University,tyu@cs.cornell.edu,Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models
neurips,2019,1,1189,Christopher,De Sa,cornell,Cornell,cdesa@cs.cornell.edu,Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models
neurips,2019,0,4226,Syrine,Belakaria,wsu,Washington State University,syrine.belakaria@wsu.edu,Max-value Entropy Search for Multi-Objective Bayesian Optimization
neurips,2019,1,4226,Aryan,Deshwal,wsu,Washington State University,aryan.deshwal@wsu.edu,Max-value Entropy Search for Multi-Objective Bayesian Optimization
neurips,2019,2,4226,Janardhan Rao,Doppa,wsu,Washington State University,jana.doppa@wsu.edu,Max-value Entropy Search for Multi-Objective Bayesian Optimization
neurips,2019,0,8412,Gauri,Jagatap,nyu,Iowa State University,gauri.jagatap@nyu.edu,Algorithmic Guarantees for Inverse Imaging with Untrained Network Priors
neurips,2019,1,8412,Chinmay,Hegde,nyu,New York University,chinmay.h@nyu.edu,Algorithmic Guarantees for Inverse Imaging with Untrained Network Priors
neurips,2019,0,8183,Matthieu,Jedor,ens-cachan,ENS Paris-Saclay & Cdiscount,jedor@cmla.ens-cachan.fr,Categorized Bandits
neurips,2019,1,8183,Vianney,Perchet,cdiscount,ENS Paris-Saclay & Criteo AI Lab,jonathan.louedec@cdiscount.com,Categorized Bandits
neurips,2019,2,8183,Jonathan,Louedec,ens-cachan,Cdiscount,perchet@cmla.ens-cachan.fr,Categorized Bandits
neurips,2019,0,6872,Meng,Fang,,Tencent,,Curriculum-guided Hindsight Experience Replay
neurips,2019,1,6872,Tianyi,Zhou,,"University of Washington, Seattle",,Curriculum-guided Hindsight Experience Replay
neurips,2019,2,6872,Yali,Du,,University College London,,Curriculum-guided Hindsight Experience Replay
neurips,2019,3,6872,Lei,Han,,Tencent AI Lab,,Curriculum-guided Hindsight Experience Replay
neurips,2019,4,6872,Zhengyou,Zhang,,Tencent,,Curriculum-guided Hindsight Experience Replay
neurips,2019,0,6891,Jathushan,Rajasegaran,,Inception Institute of Artificial Intelligence,,Random Path Selection for Continual Learning
neurips,2019,1,6891,Munawar,Hayat,,IIAI,,Random Path Selection for Continual Learning
neurips,2019,2,6891,Salman,Khan,,Inception Institute of Artificial Intelligence,,Random Path Selection for Continual Learning
neurips,2019,3,6891,Fahad Shahbaz,Khan,,Inception Institute of Artificial Intelligence,,Random Path Selection for Continual Learning
neurips,2019,4,6891,Ling,Shao,,Inception Institute of Artificial Intelligence,,Random Path Selection for Continual Learning
neurips,2019,0,7329,Mohammad Sadegh,Talebi,inria,Inria,sadegh.talebi@inria.fr,Learning Multiple Markov Chains via Adaptive Allocation
neurips,2019,1,7329,Odalric-Ambrym,Maillard,inria,INRIA,odalric.maillard@inria.fr,Learning Multiple Markov Chains via Adaptive Allocation
neurips,2019,0,2683,Taewan,Kim,utexas,Amazon,jghosh@utexas.edu,On Single Source Robustness in Deep Fusion Models
neurips,2019,1,2683,Joydeep,Ghosh,utexas,UT Austin,twankim@utexas.edu,On Single Source Robustness in Deep Fusion Models
neurips,2019,0,1294,Soeren,Laue,uni-jena,Friedrich Schiller University Jena / Data Assessment Solutions,soeren.laue@uni-jena.de,GENO -- GENeric Optimization for Classical Machine Learning
neurips,2019,1,1294,Matthias,Mitterreiter,uni-jena,Friedrich Schiller University Jena,matthias.mitterreiter@uni-jena.de,GENO -- GENeric Optimization for Classical Machine Learning
neurips,2019,2,1294,Joachim,Giesen,uni-jena,Friedrich-Schiller-Universitat Jena,joachim.giesen@uni-jena.de,GENO -- GENeric Optimization for Classical Machine Learning
neurips,2019,0,805,Stephan,Rabanser,amazon,AWS AI Labs,rabans@amazon.com,Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift
neurips,2019,1,805,Stephan,Günnemann,tum,Technical University of Munich,guennemann@in.tum.de,Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift
neurips,2019,2,805,Zachary,Lipton,cmu,Carnegie Mellon University,zlipton@cmu.edu,Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift
neurips,2019,0,6899,Antonio,Orvieto,,ETH Zurich,,Shadowing Properties of Optimization Algorithms
neurips,2019,1,6899,Aurelien,Lucchi,,ETH Zurich,,Shadowing Properties of Optimization Algorithms
neurips,2019,0,4754,Minmin,Chen,,Google,,Surrogate Objectives for Batch Policy Optimization in One-step Decision Making
neurips,2019,1,4754,Ramki,Gummadi,,Google,,Surrogate Objectives for Batch Policy Optimization in One-step Decision Making
neurips,2019,2,4754,Chris,Harris,,Google,,Surrogate Objectives for Batch Policy Optimization in One-step Decision Making
neurips,2019,3,4754,Dale,Schuurmans,,University of Alberta & Google Brain,,Surrogate Objectives for Batch Policy Optimization in One-step Decision Making
neurips,2019,0,2513,Philip,Paquette,gmail,Université de Montréal - MILA,pcpaquette@gmail.com,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,1,2513,Yuchen,Lu,gmail,University of Montreal,luyuchen.paul@gmail.com,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,2,2513,SETON STEVEN,BOCCO,gmail,MILA,stevenbocco@gmail.com,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,3,2513,Max,Smith,gmail,University of Michigan,max.olan.smith@gmail.com,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,4,2513,Satya,O.-G.,gmail,MILA,s.ortizgagne@gmail.com,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,5,2513,Jonathan,Kummerfeld,umich,University of Michigan,jkummerf@umich.edu,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,6,2513,Joelle,Pineau,umich,McGill University,baveja@umich.edu,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,7,2513,Satinder,Singh,mcgill,University of Michigan,jpineau@cs.mcgill.ca,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,8,2513,Aaron,Courville,gmail,U. Montreal,aaron.courville@gmail.com,No-Press Diplomacy: Modeling Multi-Agent Gameplay
neurips,2019,0,3431,Robert,Pinsler,cam,University of Cambridge,rp586@cam.ac.uk,Bayesian Batch Active Learning as Sparse Subset Approximation
neurips,2019,1,3431,Jonathan,Gordon,cam,University of Cambridge,jg801@cam.ac.uk,Bayesian Batch Active Learning as Sparse Subset Approximation
neurips,2019,2,3431,Eric,Nalisnick,cam,University of Cambridge,etn22@cam.ac.uk,Bayesian Batch Active Learning as Sparse Subset Approximation
neurips,2019,3,3431,José Miguel,Hernández-Lobato,cam,University of Cambridge,jmh233@cam.ac.uk,Bayesian Batch Active Learning as Sparse Subset Approximation
neurips,2019,0,1029,Aaron,Defazio,,Facebook AI Research,,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning
neurips,2019,1,1029,Leon,Bottou,,FAIR,,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning
neurips,2019,0,1730,Sindy,Löwe,,University of Amsterdam,,Putting An End to End-to-End: Gradient-Isolated Learning of Representations
neurips,2019,1,1730,Peter,O'Connor,,Brain Corporation,,Putting An End to End-to-End: Gradient-Isolated Learning of Representations
neurips,2019,2,1730,Bastiaan,Veeling,,AMLab - University of Amsterdam,,Putting An End to End-to-End: Gradient-Isolated Learning of Representations
neurips,2019,0,4315,Elliot,Meyerson,cognizant,Cognizant,elliot.meyerson@cognizant.com,Modular Universal Reparameterization: Deep Multi-task Learning Across Diverse Domains
neurips,2019,1,4315,Risto,Miikkulainen,utexas,The University of Texas at Austin; Cognizant,risto@cs.utexas.edu,Modular Universal Reparameterization: Deep Multi-task Learning Across Diverse Domains
neurips,2019,0,2550,David,Martínez-Rubio,ox,University of Oxford,david.martinez@cs.ox.ac.uk,Decentralized Cooperative Stochastic Bandits
neurips,2019,1,2550,Varun,Kanade,ox,University of Oxford,varunk@cs.ox.ac.uk,Decentralized Cooperative Stochastic Bandits
neurips,2019,2,2550,Patrick,Rebeschini,ox,University of Oxford,patrick.rebeschini@stats.ox.ac.uk,Decentralized Cooperative Stochastic Bandits
neurips,2019,0,526,Chris,Wendler,ethz,ETH Zurich,chris.wendler@inf.ethz.ch,Powerset Convolutional Neural Networks
neurips,2019,1,526,Markus,Püschel,ist,ETH Zurich,dan.alistarh@ist.ac.at,Powerset Convolutional Neural Networks
neurips,2019,2,526,Dan,Alistarh,ethz,IST Austria & NeuralMagic,pueschel@inf.ethz.ch,Powerset Convolutional Neural Networks
neurips,2019,0,7797,Yaniv,Ovadia,google,Princeton University,yovadia@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,1,7797,Emily,Fertig,google,Google Research,emilyaf@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,2,7797,Jie,Ren,google,Google Inc.,jjren@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,3,7797,Zachary,Nado,google,Google Inc.,znado@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,4,7797,D.,Sculley,google,Google Research,dsculley@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,5,7797,Sebastian,Nowozin,google,Google Research Berlin,nowozin@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,6,7797,Joshua,Dillon,google,Google,jvdillon@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,7,7797,Balaji,Lakshminarayanan,google,Google DeepMind,balajiln@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,8,7797,Jasper,Snoek,google,Google Brain,jsnoek@google.com,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift
neurips,2019,0,3918,Erwan,Lecarpentier,isae-supaero,Université de Toulouse,erwan.lecarpentier@isae-supaero.fr,"Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning"
neurips,2019,1,3918,Emmanuel,Rachelson,isae-supaero,ISAE-SUPAERO / University of Toulouse,emmanuel.rachelson@isae-supaero.fr,"Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning"
neurips,2019,0,1837,Su,Jia,cmu,CMU,sjia1@andrew.cmu.edu,Optimal Decision Tree with Noisy Outcomes
neurips,2019,1,1837,viswanath,nagarajan,umich,"Univ Michigan, Ann Arbor",viswa@umich.edu,Optimal Decision Tree with Noisy Outcomes
neurips,2019,2,1837,Fatemeh,Navidi,umich,University of Michigan,navidi@umich.edu,Optimal Decision Tree with Noisy Outcomes
neurips,2019,3,1837,R,Ravi,cmu,CMU,ravi@andrew.cmu.edu,Optimal Decision Tree with Noisy Outcomes
neurips,2019,0,7117,Amit,Daniely,huji,Hebrew University and Google Research,amit.daniely@mail.huji.ac.il,Generalization Bounds for Neural Networks via Approximate Description Length
neurips,2019,1,7117,Elad,Granot,huji,Hebrew University,elad.granot@mail.huji.ac.il,Generalization Bounds for Neural Networks via Approximate Description Length
neurips,2019,0,4164,Dushyant,Rao,,DeepMind,,Continual Unsupervised Representation Learning
neurips,2019,1,4164,Francesco,Visin,,DeepMind,,Continual Unsupervised Representation Learning
neurips,2019,2,4164,Andrei,Rusu,,DeepMind,,Continual Unsupervised Representation Learning
neurips,2019,3,4164,Razvan,Pascanu,,Google DeepMind,,Continual Unsupervised Representation Learning
neurips,2019,4,4164,Yee Whye,Teh,,"University of Oxford, DeepMind",,Continual Unsupervised Representation Learning
neurips,2019,5,4164,Raia,Hadsell,,DeepMind,,Continual Unsupervised Representation Learning
neurips,2019,0,7788,Mehmet Fatih,Sahin,epfl,École Polytechnique Fédérale de Lausanne,mehmet.sahin@epfl.ch,An  Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints
neurips,2019,1,7788,Armin,eftekhari,epfl,EPFL,armin.eftekhari@epfl.ch,An  Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints
neurips,2019,2,7788,Ahmet,Alacaoglu,epfl,EPFL,ahmet.alacaoglu@epfl.ch,An  Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints
neurips,2019,3,7788,Fabian,Latorre,epfl,EPFL,fabian.latorre@epfl.ch,An  Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints
neurips,2019,4,7788,Volkan,Cevher,epfl,EPFL,volkan.cevher@epfl.ch,An  Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints
neurips,2019,0,4659,Yuan,Deng,duke,Duke University,ericdy@cs.duke.edu,A Robust Non-Clairvoyant Dynamic Mechanism for Contextual Auctions
neurips,2019,1,4659,Sébastien,Lahaie,google,Google Research,slahaie@google.com,A Robust Non-Clairvoyant Dynamic Mechanism for Contextual Auctions
neurips,2019,2,4659,Vahab,Mirrokni,google,Google Research NYC,mirrokni@google.com,A Robust Non-Clairvoyant Dynamic Mechanism for Contextual Auctions
neurips,2019,0,8910,Charlie,Tang,apple,Apple Inc.,yichuan_tang@apple.com,Multiple Futures Prediction
neurips,2019,1,8910,Russ,Salakhutdinov,apple,Carnegie Mellon University,rsalakhutdinov@apple.com,Multiple Futures Prediction
neurips,2019,0,1385,Srinath,Sridhar,,Stanford University,,Multiview Aggregation for Learning Category-Specific Shape Reconstruction
neurips,2019,1,1385,Davis,Rempe,,Stanford University,,Multiview Aggregation for Learning Category-Specific Shape Reconstruction
neurips,2019,2,1385,Julien,Valentin,,Google,,Multiview Aggregation for Learning Category-Specific Shape Reconstruction
neurips,2019,3,1385,Bouaziz,Sofien,,,,Multiview Aggregation for Learning Category-Specific Shape Reconstruction
neurips,2019,4,1385,Leonidas,Guibas,,stanford.edu,,Multiview Aggregation for Learning Category-Specific Shape Reconstruction
neurips,2019,0,7857,Sobhan,Miryoosefi,princeton,Princeton University,miryoosefi@cs.princeton.edu,Reinforcement Learning with Convex Constraints
neurips,2019,1,7857,Kianté,Brantley,umd,The University of Maryland College Park,kdbrant@cs.umd.edu,Reinforcement Learning with Convex Constraints
neurips,2019,2,7857,Hal,Daume III,hal3,Microsoft Research &      University of Maryland,me@hal3.name,Reinforcement Learning with Convex Constraints
neurips,2019,3,7857,Miro,Dudik,microsoft,Microsoft Research,mdudik@microsoft.com,Reinforcement Learning with Convex Constraints
neurips,2019,4,7857,Robert,Schapire,microsoft,MIcrosoft Research,schapire@microsoft.com,Reinforcement Learning with Convex Constraints
neurips,2019,0,5131,Colin,Wei,,Stanford University,,Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel
neurips,2019,1,5131,Jason,Lee,,Princeton University,,Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel
neurips,2019,2,5131,Qiang,Liu,,UT Austin,,Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel
neurips,2019,3,5131,Tengyu,Ma,,Stanford University,,Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel
neurips,2019,0,6909,Farnood,Salehi,epfl,EPFL,farnood.salehi@epfl.ch,Learning Hawkes Processes from a handful of events
neurips,2019,1,6909,William,Trouleau,epfl,EPFL,william.trouleau@epfl.ch,Learning Hawkes Processes from a handful of events
neurips,2019,2,6909,Matthias,Grossglauser,epfl,EPFL,matthias.grossglauser@epfl.ch,Learning Hawkes Processes from a handful of events
neurips,2019,3,6909,Patrick,Thiran,epfl,EPFL,patrick.thiran@epfl.ch,Learning Hawkes Processes from a handful of events
neurips,2019,0,6877,Yann,Dauphin,google,Google AI,ynd@google.com,MetaInit: Initializing learning by learning to initialize
neurips,2019,1,6877,Samuel,Schoenholz,google,Google Brain,schsam@google.com,MetaInit: Initializing learning by learning to initialize
neurips,2019,0,5691,Aditya Sharad,Golatkar,ucla,UCLA,aditya29@cs.ucla.edu,"Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence"
neurips,2019,1,5691,Alessandro,Achille,ucla,AWS,achille@cs.ucla.edu,"Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence"
neurips,2019,2,5691,Stefano,Soatto,ucla,UCLA,soatto@cs.ucla.edu,"Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence"
neurips,2019,0,5908,Ke,Wang,pku,Peking University,wangke17@pku.edu.cn,Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation
neurips,2019,1,5908,Hang,Hua,pku,Peking University,huahang@pku.edu.cn,Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation
neurips,2019,2,5908,Xiaojun,Wan,pku,Peking University,wanxiaojun@pku.edu.cn,Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation
neurips,2019,0,7015,Wieland,Brendel,,"AG Bethge, University of Tübingen",,"Accurate, reliable and fast robustness evaluation"
neurips,2019,1,7015,Jonas,Rauber,,University of Tübingen,,"Accurate, reliable and fast robustness evaluation"
neurips,2019,2,7015,Matthias,Kümmerer,,University of Tübingen,,"Accurate, reliable and fast robustness evaluation"
neurips,2019,3,7015,Ivan,Ustyuzhaninov,,University of Tübingen,,"Accurate, reliable and fast robustness evaluation"
neurips,2019,4,7015,Matthias,Bethge,,University of Tübingen,,"Accurate, reliable and fast robustness evaluation"
neurips,2019,0,3379,Ali,Kavis,epfl,EPFL,ali.kavis@epfl.ch,"UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization"
neurips,2019,1,3379,Kfir Y.,Levy,technion,Technion,kfirylevy@technion.ac.il,"UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization"
neurips,2019,2,3379,Francis,Bach,inria,INRIA - Ecole Normale Superieure,francis.bach@inria.fr,"UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization"
neurips,2019,3,3379,Volkan,Cevher,epfl,EPFL,volkan.cevher@epfl.ch,"UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization"
neurips,2019,0,5435,Krzysztof,Choromanski,google,Google Brain Robotics,kchoro@google.com,From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization
neurips,2019,1,5435,Aldo,Pacchiano,berkeley,UC Berkeley,pacchiano@berkeley.edu,From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization
neurips,2019,2,5435,Jack,Parker-Holder,ox,University of Oxford,jackph@robots.ox.ac.uk,From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization
neurips,2019,3,5435,Yunhao,Tang,columbia,Columbia University,yt2541@columbia.edu,From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization
neurips,2019,4,5435,Vikas,Sindhwani,google,Google,sindhwani@google.com,From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization
neurips,2019,0,2673,Soumya,Basu,,University of Texas at Austin,,Blocking Bandits
neurips,2019,1,2673,Rajat,Sen,,Amazon,,Blocking Bandits
neurips,2019,2,2673,Sujay,Sanghavi,,UT-Austin,,Blocking Bandits
neurips,2019,3,2673,Sanjay,Shakkottai,,University of Texas at Austin,,Blocking Bandits
neurips,2019,0,697,Chao,Qu,,Ant Financial Services Group,,Value Propagation for Decentralized Networked Deep Multi-agent  Reinforcement Learning
neurips,2019,1,697,Shie,Mannor,,Technion,,Value Propagation for Decentralized Networked Deep Multi-agent  Reinforcement Learning
neurips,2019,2,697,Huan,Xu,,Georgia Inst. of Technology,,Value Propagation for Decentralized Networked Deep Multi-agent  Reinforcement Learning
neurips,2019,3,697,Yuan,Qi,,Ant Financial Services Group,,Value Propagation for Decentralized Networked Deep Multi-agent  Reinforcement Learning
neurips,2019,4,697,Le,Song,,Ant Financial Services Group,,Value Propagation for Decentralized Networked Deep Multi-agent  Reinforcement Learning
neurips,2019,5,697,Junwu,Xiong,,Ant Financial Services Group,,Value Propagation for Decentralized Networked Deep Multi-agent  Reinforcement Learning
neurips,2019,0,1488,Pratyusha,Sharma,,Carnegie Mellon University/MIT,,Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller
neurips,2019,1,1488,Deepak,Pathak,,"UC Berkeley, FAIR, CMU",,Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller
neurips,2019,2,1488,Abhinav,Gupta,,Facebook AI Research/CMU,,Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller
neurips,2019,0,3364,Yilun,Xu,pku,Peking University,xuyilun@pku.edu.cn,L_DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise
neurips,2019,1,3364,Peng,Cao,pku,Peking University,caopeng2016@pku.edu.cn,L_DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise
neurips,2019,2,3364,Yuqing,Kong,pku,Peking University,yuqing.kong@pku.edu.cn,L_DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise
neurips,2019,3,3364,Yizhou,Wang,pku,Peking University,Yizhou.Wang@pku.edu.cn,L_DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise
neurips,2019,0,3262,Tianyu,Guo,pku,Peking University,tianyuguo@pku.edu.cn,Learning from Bad Data via Generation
neurips,2019,1,3262,Chang,Xu,pku,University of Sydney,shiboxin@pku.edu.cn,Learning from Bad Data via Generation
neurips,2019,2,3262,Boxin,Shi,pku,Peking University,chaoxu@cis.pku.edu.cn,Learning from Bad Data via Generation
neurips,2019,3,3262,Chao,Xu,sydney,Peking University,c.xu@sydney.edu.au,Learning from Bad Data via Generation
neurips,2019,4,3262,Dacheng,Tao,sydney,University of Sydney,dacheng.tao@sydney.edu.au,Learning from Bad Data via Generation
neurips,2019,0,3032,Aming,Wu,tju,Tianjin University,tjwam@tju.edu.cn,Connective Cognition Network for Directional Visual Commonsense Reasoning
neurips,2019,1,3032,Linchao,Zhu,tju,University of Sydney Technology,yahong@tju.edu.cn,Connective Cognition Network for Directional Visual Commonsense Reasoning
neurips,2019,2,3032,Yahong,Han,uts,"Tianjin University, China",Linchao.Zhu@uts.edu.au,Connective Cognition Network for Directional Visual Commonsense Reasoning
neurips,2019,3,3032,Yi,Yang,uts,UTS,yi.yang@uts.edu.au,Connective Cognition Network for Directional Visual Commonsense Reasoning
neurips,2019,0,5521,Wasim,Huleihel,tau,Tel-Aviv University,wasimh@mail.tau.ac.il,Same-Cluster Querying for Overlapping Clusters
neurips,2019,1,5521,Arya,Mazumdar,umass,University of Massachusetts Amherst,arya@cs.umass.edu,Same-Cluster Querying for Overlapping Clusters
neurips,2019,2,5521,Muriel,Medard,mit,MIT,medard@mit.edu,Same-Cluster Querying for Overlapping Clusters
neurips,2019,3,5521,Soumyabrata,Pal,umass,University of Massachusetts Amherst,soumyabratap@umass.edu,Same-Cluster Querying for Overlapping Clusters
neurips,2019,0,3691,Akinori,Tanaka,riken,RIKEN/Keio Univ.,akinori.tanaka@riken.jp,Discriminator optimal transport
neurips,2019,0,913,Mikhail,Yurochkin,ibm,"IBM Research, MIT-IBM Watson AI Lab",mikhail.yurochkin@ibm.com,Hierarchical Optimal Transport for Document Representation
neurips,2019,1,913,Sebastian,Claici,mit,MIT,sclaici@mit.edu,Hierarchical Optimal Transport for Document Representation
neurips,2019,2,913,Edward,Chien,mit,Massachusetts Institute of Technology,edchien@mit.edu,Hierarchical Optimal Transport for Document Representation
neurips,2019,3,913,Farzaneh,Mirzazadeh,ibm,"MIT-IBM Watson AI Lab, IBM Research",farzaneh@ibm.com,Hierarchical Optimal Transport for Document Representation
neurips,2019,4,913,Justin,Solomon,mit,MIT,jsolomon@mit.edu,Hierarchical Optimal Transport for Document Representation
neurips,2019,0,4150,David,Novotny,fb,Facebook AI Research,dnovotny@fb.com,PerspectiveNet: A Scene-consistent Image Generator for New View Synthesis in Real Indoor Environments
neurips,2019,1,4150,Ben,Graham,fb,Facebook Research,benjamingraham@fb.com,PerspectiveNet: A Scene-consistent Image Generator for New View Synthesis in Real Indoor Environments
neurips,2019,2,4150,Jeremy,Reizenstein,fb,Facebook AI Research,reizenstein@fb.com,PerspectiveNet: A Scene-consistent Image Generator for New View Synthesis in Real Indoor Environments
neurips,2019,0,908,Yuan,Deng,,Duke University,,Strategizing against No-regret Learners
neurips,2019,1,908,Jon,Schneider,,Google Research,,Strategizing against No-regret Learners
neurips,2019,2,908,Balasubramanian,Sivan,,Google Research,,Strategizing against No-regret Learners
neurips,2019,0,5689,Tanner,Fiez,uw,University of Washington,ezt@uw.edu,Sequential Experimental Design for Transductive Linear Bandits
neurips,2019,1,5689,Lalit,Jain,washington,University of Washington,lalitj@cs.washington.edu,Sequential Experimental Design for Transductive Linear Bandits
neurips,2019,2,5689,Kevin,Jamieson,washington,U Washington,jamieson@cs.washington.edu,Sequential Experimental Design for Transductive Linear Bandits
neurips,2019,3,5689,Lillian,Ratliff,uw,University of Washington,ratlif@uw.edu,Sequential Experimental Design for Transductive Linear Bandits
neurips,2019,0,2620,Bryan,Wilder,harvard,Harvard University,bwilder@g.harvard.edu,End to end learning and optimization on graphs
neurips,2019,1,2620,Eric,Ewing,usc,University of Southern California,ericewin@usc.edu,End to end learning and optimization on graphs
neurips,2019,2,2620,Bistra,Dilkina,usc,University of Southern California,dilkina@usc.edu,End to end learning and optimization on graphs
neurips,2019,3,2620,Milind,Tambe,harvard,USC,milind_tambe@harvard.edu,End to end learning and optimization on graphs
neurips,2019,0,856,Pan,Zhou,nus,National University of Singapore,pzhou@u.nus.edu,Efficient Meta Learning via Minibatch Proximal Update
neurips,2019,1,856,Xiaotong,Yuan,nuist,Nanjing University of Information Science & Technology,xtyuan@nuist.edu.cn,Efficient Meta Learning via Minibatch Proximal Update
neurips,2019,2,856,Huan,Xu,alibaba-inc,Alibaba Group,Huan.xu@alibaba-inc.com,Efficient Meta Learning via Minibatch Proximal Update
neurips,2019,3,856,Shuicheng,Yan,nus,National University of Singapore,eleyans@nus.edu.sg,Efficient Meta Learning via Minibatch Proximal Update
neurips,2019,4,856,Jiashi,Feng,nus,National University of Singapore,elefjia@nus.edu.sg,Efficient Meta Learning via Minibatch Proximal Update
neurips,2019,0,7042,Ruichu,Cai,gdut,Guangdong University of Technology,cairuichu@gdut.edu.cn,Triad Constraints for Learning Causal Structure of Latent Variables
neurips,2019,1,7042,Feng,Xie,gmail,Guangdong University of Technology,xiefeng009@gmail.com,Triad Constraints for Learning Causal Structure of Latent Variables
neurips,2019,2,7042,Clark,Glymour,cmu,Carnegie Mellon University,cg09@andrew.cmu.edu,Triad Constraints for Learning Causal Structure of Latent Variables
neurips,2019,3,7042,Zhifeng,Hao,gdut,Guangdong University of Technology,zfhao@gdut.edu.cn,Triad Constraints for Learning Causal Structure of Latent Variables
neurips,2019,4,7042,Kun,Zhang,cmu,CMU,kunz1@cmu.edu,Triad Constraints for Learning Causal Structure of Latent Variables
neurips,2019,0,6658,Meelis,Kull,ut,University of Tartu,meelis.kull@ut.ee,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration
neurips,2019,1,6658,Miquel,Perello Nieto,ut,University of Bristol,markus.kangsepp@ut.ee,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration
neurips,2019,2,6658,Markus,Kängsepp,bristol,University of Tartu,hao.song@bristol.ac.uk,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration
neurips,2019,3,6658,Telmo,Silva Filho,bris,Universidade Federal da Paraíba,miquel.perellonieto@bris.ac.uk,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration
neurips,2019,4,6658,Hao,Song,ufpb,University of Bristol,telmo@de.ufpb.br,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration
neurips,2019,5,6658,Peter,Flach,bristol,University of Bristol,peter.flach@bristol.ac.uk,Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration
neurips,2019,0,2374,Shuo,Chen,,Nanjing University of Science and Technology,,Curvilinear Distance Metric Learning
neurips,2019,1,2374,Lei,Luo,,Pitt,,Curvilinear Distance Metric Learning
neurips,2019,2,2374,Jian,Yang,,Nanjing University of Science and Technology,,Curvilinear Distance Metric Learning
neurips,2019,3,2374,Chen,Gong,,Nanjing University of Science and Technology,,Curvilinear Distance Metric Learning
neurips,2019,4,2374,Jun,Li,,MIT,,Curvilinear Distance Metric Learning
neurips,2019,5,2374,Heng,Huang,,University of Pittsburgh,,Curvilinear Distance Metric Learning
neurips,2019,0,4951,Hao(Jackson),Cui,iu,Google,rkhardon@iu.edu,Sampling Networks and Aggregate Simulation for Online POMDP Planning
neurips,2019,1,4951,Roni,Khardon,tufts,"Indiana University, Bloomington",hao.cui@tufts.edu,Sampling Networks and Aggregate Simulation for Online POMDP Planning
neurips,2019,0,8569,Ehsan,Amid,google,"University of California, Santa Cruz",eamid@google.com,Robust Bi-Tempered Logistic Loss Based on Bregman Divergences
neurips,2019,1,8569,Manfred K.,Warmuth,google,Google Brain,manfred@google.com,Robust Bi-Tempered Logistic Loss Based on Bregman Divergences
neurips,2019,2,8569,Rohan,Anil,google,Google,rohananil@google.com,Robust Bi-Tempered Logistic Loss Based on Bregman Divergences
neurips,2019,3,8569,Tomer,Koren,google,Google,tkoren@google.com,Robust Bi-Tempered Logistic Loss Based on Bregman Divergences
neurips,2019,0,4172,Eduard,Eiben,,"Royal Holloway, University of London",,The Parameterized Complexity of Cascading Portfolio Scheduling
neurips,2019,1,4172,Robert,Ganian,,TU Wien,,The Parameterized Complexity of Cascading Portfolio Scheduling
neurips,2019,2,4172,Iyad,Kanj,,"DePaul University, Chicago",,The Parameterized Complexity of Cascading Portfolio Scheduling
neurips,2019,3,4172,Stefan,Szeider,,Vienna University of Technology,,The Parameterized Complexity of Cascading Portfolio Scheduling
neurips,2019,0,8205,Rémy,Degenne,cwi,"Centrum Wiskunde & Informatica, Amsterdam",remy.degenne@cwi.nl,Non-Asymptotic Pure Exploration by Solving Games
neurips,2019,1,8205,Wouter,Koolen,cwi,"Centrum Wiskunde & Informatica, Amsterdam",wmkoolen@cwi.nl,Non-Asymptotic Pure Exploration by Solving Games
neurips,2019,2,8205,Pierre,Ménard,gmail,Institut de Mathématiques de Toulouse,menardprr@gmail.com,Non-Asymptotic Pure Exploration by Solving Games
neurips,2019,0,1359,Kristof,Meding,uni-tuebingen,University Tübingen,kristof.meding@uni-tuebingen.de,Perceiving the arrow of time in autoregressive motion
neurips,2019,1,1359,Dominik,Janzing,mpg,Amazon,bs@tuebingen.mpg.de,Perceiving the arrow of time in autoregressive motion
neurips,2019,2,1359,Bernhard,Schölkopf,amazon,MPI for Intelligent Systems,janzind@amazon.com,Perceiving the arrow of time in autoregressive motion
neurips,2019,3,1359,Felix A.,Wichmann,uni-tuebingen,University of Tübingen,felix.wichmann@uni-tuebingen.de,Perceiving the arrow of time in autoregressive motion
neurips,2019,0,319,Nikolas,Ioannou,ibm,IBM Research,nio@zurich.ibm.com,SySCD: A System-Aware Parallel Coordinate Descent Algorithm
neurips,2019,1,319,Celestine,Mendler-Dünner,berkeley,UC Berkeley,mendler@berkeley.edu,SySCD: A System-Aware Parallel Coordinate Descent Algorithm
neurips,2019,2,319,Thomas,Parnell,ibm,IBM Research,tpa@zurich.ibm.com,SySCD: A System-Aware Parallel Coordinate Descent Algorithm
neurips,2019,0,147,Alex,Lamy,columbia,Columbia University,a.lamy@columbia.edu,Noise-tolerant fair classification
neurips,2019,1,147,Ziyuan,Zhong,columbia,Columbia University,ziyuan.zhong@columbia.edu,Noise-tolerant fair classification
neurips,2019,2,147,Aditya,Menon,columbia,Google,verma@cs.columbia.edu,Noise-tolerant fair classification
neurips,2019,3,147,Nakul,Verma,google,Columbia University,adityakmenon@google.com,Noise-tolerant fair classification
neurips,2019,0,5343,Rakshith Sharma,Srinivasa,osu,Georgia Institute of Technology,lee.8763@osu.edu,Decentralized sketching of low rank matrices
neurips,2019,1,5343,Kiryung,Lee,gatech,Ohio state university,rsrinivasa6@gatech.edu,Decentralized sketching of low rank matrices
neurips,2019,2,5343,Marius,Junge,illinois,University of Illinois,mjunge@illinois.edu,Decentralized sketching of low rank matrices
neurips,2019,3,5343,Justin,Romberg,gatech,Georgia Institute of Technology,jrom@ece.gatech.edu,Decentralized sketching of low rank matrices
neurips,2019,0,354,Gamaleldin,Elsayed,,"Google Research, Brain Team",,Saccader: Improving Accuracy of Hard Attention Models for Vision
neurips,2019,1,354,Simon,Kornblith,,Google Brain,,Saccader: Improving Accuracy of Hard Attention Models for Vision
neurips,2019,2,354,Quoc,Le,,Google,,Saccader: Improving Accuracy of Hard Attention Models for Vision
neurips,2019,0,5819,Maryam,Aliakbarpour,mit,MIT,maryama@mit.edu,Private Testing of Distributions via Sample Permutations
neurips,2019,1,5819,Ilias,Diakonikolas,ucsd,UW Madison,dakane@ucsd.edu,Private Testing of Distributions via Sample Permutations
neurips,2019,2,5819,Daniel,Kane,gmail,UCSD,ilias.diakonikolas@gmail.com,Private Testing of Distributions via Sample Permutations
neurips,2019,3,5819,Ronitt,Rubinfeld,mit,"MIT, TAU",ronitt@csail.mit.edu,Private Testing of Distributions via Sample Permutations
neurips,2019,0,471,Yichao,Zhou,berkeley,UC Berkeley,zyc@berkeley.edu,NeurVPS: Neural Vanishing Point Scanning via Conic Convolution
neurips,2019,1,471,Haozhi,Qi,berkeley,UC Berkeley,hqi@berkeley.edu,NeurVPS: Neural Vanishing Point Scanning via Conic Convolution
neurips,2019,2,471,Jingwei,Huang,stanford,Stanford University,jingweih@stanford.edu,NeurVPS: Neural Vanishing Point Scanning via Conic Convolution
neurips,2019,3,471,Yi,Ma,berkeley,UC Berkeley,yima@eecs.berkeley.edu,NeurVPS: Neural Vanishing Point Scanning via Conic Convolution
neurips,2019,0,2814,Jayadev,Acharya,cornell,Cornell University,acharya@cornell.edu,Estimating Entropy of Distributions in Constant Space
neurips,2019,1,2814,Sourbh,Bhadane,cornell,Cornell University,snb62@cornell.edu,Estimating Entropy of Distributions in Constant Space
neurips,2019,2,2814,Piotr,Indyk,mit,MIT,indyk@mit.edu,Estimating Entropy of Distributions in Constant Space
neurips,2019,3,2814,Ziteng,Sun,cornell,Cornell University,zs335@cornell.edu,Estimating Entropy of Distributions in Constant Space
neurips,2019,0,387,Ruidi,Chen,bu,Boston University,rchen15@bu.edu,Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression
neurips,2019,1,387,Ioannis,Paschalidis,bu,Boston University,yannisp@bu.edu,Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression
neurips,2019,0,2574,Xu,Wang,szu,Shenzhen University,wangxu@szu.edu.cn,Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations
neurips,2019,1,2574,Jingming,He,gmail,Shenzhen University,hejingming519@gmail.com,Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations
neurips,2019,2,2574,Lin,Ma,gmail,Tencent AI Lab,forest.linma@gmail.com,Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations
neurips,2019,0,1586,Weijiang,Yu,gmail,Sun Yat-sen University,weijiangyu8@gmail.com,Heterogeneous Graph Learning for Visual Commonsense Reasoning
neurips,2019,1,1586,Jingwen,Zhou,sysu,Sun Yat-sen University,zhoujw57@mail2.sysu.edu.cn,Heterogeneous Graph Learning for Visual Commonsense Reasoning
neurips,2019,2,1586,Weihao,Yu,gmail,Sun Yat-sen University,weihaoyu6@gmail.com,Heterogeneous Graph Learning for Visual Commonsense Reasoning
neurips,2019,3,1586,Xiaodan,Liang,gmail,Sun Yat-sen University,xdliang328@gmail.com,Heterogeneous Graph Learning for Visual Commonsense Reasoning
neurips,2019,4,1586,Nong,Xiao,sysu,Sun Yat-sen University,xiaon6@sysu.edu.cn,Heterogeneous Graph Learning for Visual Commonsense Reasoning
neurips,2019,0,5147,Rohan,Anil,google,Google,rohananil@google.com,Memory Efficient Adaptive Optimization
neurips,2019,1,5147,Vineet,Gupta,google,Google,vineet@google.com,Memory Efficient Adaptive Optimization
neurips,2019,2,5147,Tomer,Koren,google,Google,tkoren@google.com,Memory Efficient Adaptive Optimization
neurips,2019,3,5147,Yoram,Singer,princeton,Google,y.s@cs.princeton.edu,Memory Efficient Adaptive Optimization
neurips,2019,0,1446,Ryan,Tibshirani,cmu,Carnegie Mellon University,ryantibs@cmu.edu,Conformal Prediction Under Covariate Shift
neurips,2019,1,1446,Rina,Foygel Barber,stanford,University of Chicago,candes@stanford.edu,Conformal Prediction Under Covariate Shift
neurips,2019,2,1446,Emmanuel,Candes,uchicago,Stanford University,rina@uchicago.edu,Conformal Prediction Under Covariate Shift
neurips,2019,3,1446,Aaditya,Ramdas,cmu,CMU,aramdas@cmu.edu,Conformal Prediction Under Covariate Shift
neurips,2019,0,1443,Claudia,Shi,,Columbia University,,Adapting Neural Networks for the Estimation of Treatment Effects
neurips,2019,1,1443,David,Blei,,Columbia University,,Adapting Neural Networks for the Estimation of Treatment Effects
neurips,2019,2,1443,Victor,Veitch,,Columbia University,,Adapting Neural Networks for the Estimation of Treatment Effects
neurips,2019,0,4390,Vikas,Garg,mit,MIT,vgarg@csail.mit.edu,Solving graph compression via optimal transport
neurips,2019,1,4390,Tommi,Jaakkola,mit,MIT,tommi@csail.mit.edu,Solving graph compression via optimal transport
neurips,2019,0,7423,Se-Young,Yun,kaist,KAIST,yunseyoung@kaist.ac.kr,Optimal Sampling and Clustering in the Stochastic Block Model
neurips,2019,1,7423,Alexandre,Proutiere,kth,KTH,alepro@kth.se,Optimal Sampling and Clustering in the Stochastic Block Model
neurips,2019,0,3591,Karlis,Freivalds,lumii,"Institute of Mathematics and Computer Science, University of Latvia",Karlis.Freivalds@lumii.lv,Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time
neurips,2019,1,3591,Emls,Ozoli,lumii,Institute of Mathematics and Computer Science,Emils.Ozolins@lumii.lv,Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time
neurips,2019,2,3591,Agris,ostaks,lumii,Institute of Mathematics and Computer Science,Agris.Sostaks@lumii.lv,Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time
neurips,2019,0,936,Benjamin,Planche,,Siemens Corporate Technology,,Incremental Scene Synthesis
neurips,2019,1,936,Xuejian,Rong,,City University of New York,,Incremental Scene Synthesis
neurips,2019,2,936,Ziyan,Wu,,United Imaging Intelligence,,Incremental Scene Synthesis
neurips,2019,3,936,Srikrishna,Karanam,,United Imaging Intelligence,,Incremental Scene Synthesis
neurips,2019,4,936,Harald,Kosch,,PASSAU,,Incremental Scene Synthesis
neurips,2019,5,936,YingLi,Tian,,City University of New York,,Incremental Scene Synthesis
neurips,2019,6,936,Jan,Ernst,,Siemens Research,,Incremental Scene Synthesis
neurips,2019,7,936,ANDREAS,HUTTER,,"Siemens Corporate Technology, Germany",,Incremental Scene Synthesis
neurips,2019,0,7897,Matthew,Sotoudeh,ucdavis,"University of California, Davis",masotoudeh@ucdavis.edu,Computing Linear Restrictions of Neural Networks
neurips,2019,1,7897,Aditya,Thakur,ucdavis,"University of California, Davis",avthakur@ucdavis.edu,Computing Linear Restrictions of Neural Networks
neurips,2019,0,2928,Harald,Steck,netflix,Netflix,hsteck@netflix.com,Markov Random Fields for Collaborative Filtering
neurips,2019,0,3001,Andrea,Zanette,stanford,Stanford University,zanette@stanford.edu,Limiting Extrapolation in Linear Approximate Value Iteration
neurips,2019,1,3001,Alessandro,Lazaric,fb,Facebook Artificial Intelligence Research,lazaric@fb.com,Limiting Extrapolation in Linear Approximate Value Iteration
neurips,2019,2,3001,Mykel,Kochenderfer,stanford,Stanford University,mykel@stanford.edu,Limiting Extrapolation in Linear Approximate Value Iteration
neurips,2019,3,3001,Emma,Brunskill,stanford,Stanford University,ebrun@cs.stanford.edu,Limiting Extrapolation in Linear Approximate Value Iteration
neurips,2019,0,2243,Frank,Ban,berkeley,UC Berkeley / Google,fban@berkeley.edu,Regularized Weighted Low Rank Approximation
neurips,2019,1,2243,David,Woodruff,cmu,Carnegie Mellon University,dwoodruf@cs.cmu.edu,Regularized Weighted Low Rank Approximation
neurips,2019,2,2243,Richard,Zhang,berkeley,UC Berkeley,qiuyi@berkeley.edu,Regularized Weighted Low Rank Approximation
neurips,2019,0,6220,Sandeep,Kumar,gmail,Hong Kong University of Science and Technology,sandeep0kr@gmail.com,Structured Graph Learning Via Laplacian Spectral Constraints
neurips,2019,1,6220,Jiaxi,Ying,ust,HKUST,jx.ying@connect.ust.hk,Structured Graph Learning Via Laplacian Spectral Constraints
neurips,2019,2,6220,Jose Vinicius,de Miranda Cardoso,ust,Universidade Federal de Campina Grande,jvdmc@connect.ust.hk,Structured Graph Learning Via Laplacian Spectral Constraints
neurips,2019,3,6220,Daniel,Palomar,ust,The Hong Kong University of Science and Technology,palomar@ust.hk,Structured Graph Learning Via Laplacian Spectral Constraints
neurips,2019,0,5096,Michael,Zhang,toronto,University of Toronto / Vector Institute,michael@cs.toronto.edu,"Lookahead Optimizer: k steps forward, 1 step back"
neurips,2019,1,5096,James,Lucas,toronto,University of Toronto,jlucas@cs.toronto.edu,"Lookahead Optimizer: k steps forward, 1 step back"
neurips,2019,2,5096,Jimmy,Ba,toronto,University of Toronto / Vector Institute,hinton@cs.toronto.edu,"Lookahead Optimizer: k steps forward, 1 step back"
neurips,2019,3,5096,Geoffrey,Hinton,toronto,Google & University of Toronto,jba@cs.toronto.edu,"Lookahead Optimizer: k steps forward, 1 step back"
neurips,2019,0,751,Jack,Serrino,mit,MIT,jserrino@mit.edu,Finding Friend and Foe in Multi-Agent Games
neurips,2019,1,751,Max,Kleiman-Weiner,harvard,Harvard/MIT,maxkleimanweiner@fas.harvard.edu,Finding Friend and Foe in Multi-Agent Games
neurips,2019,2,751,David,Parkes,harvard,Harvard University,parkes@eecs.harvard.edu,Finding Friend and Foe in Multi-Agent Games
neurips,2019,3,751,Josh,Tenenbaum,mit,MIT,jbt@mit.edu,Finding Friend and Foe in Multi-Agent Games
neurips,2019,0,6006,Difan,Zou,ucla,"University of California, Los Angeles",knowzou@cs.ucla.edu,Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks
neurips,2019,1,6006,Ziniu,Hu,ucla,UCLA,bull@cs.ucla.edu,Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks
neurips,2019,2,6006,Yewen,Wang,ucla,UCLA,wyw10804@cs.ucla.edu,Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks
neurips,2019,3,6006,Song,Jiang,ucla,"University of California, Los Angeles",songjiang@cs.ucla.edu,Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks
neurips,2019,4,6006,Yizhou,Sun,ucla,UCLA,yzsun@cs.ucla.edu,Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks
neurips,2019,5,6006,Quanquan,Gu,ucla,UCLA,qgu@cs.ucla.edu,Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks
neurips,2019,0,941,Shikun,Liu,imperial,Imperial College London,shikun.liu17@imperial.ac.uk,Self-Supervised Generalisation with Meta Auxiliary Learning
neurips,2019,1,941,Andrew,Davison,imperial,Imperial College London,a.davison@imperial.ac.uk,Self-Supervised Generalisation with Meta Auxiliary Learning
neurips,2019,2,941,Edward,Johns,imperial,Imperial College London,e.johns@imperial.ac.uk,Self-Supervised Generalisation with Meta Auxiliary Learning
neurips,2019,0,5235,Anish,Agarwal,,MIT,,On Robustness of Principal Component Regression
neurips,2019,1,5235,Devavrat,Shah,,Massachusetts Institute of Technology,,On Robustness of Principal Component Regression
neurips,2019,2,5235,Dennis,Shen,,Massachusetts Institute of Technology,,On Robustness of Principal Component Regression
neurips,2019,3,5235,Dogyoon,Song,,Massachusetts Institute of Technology,,On Robustness of Principal Component Regression
neurips,2019,0,5946,Shreyas,Saxena,apple,Apple,shreyas_saxena@apple.com,Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum
neurips,2019,1,5946,Oncel,Tuzel,apple,Apple,otuzel@apple.com,Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum
neurips,2019,2,5946,Dennis,DeCoste,apple,Apple,ddecoste@apple.com,Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum
neurips,2019,0,1562,Ting-I,Hsieh,gmail,National Tsing Hua University,tingihsieh.tw@gmail.com,One-Shot Object Detection with Co-Attention and Co-Excitation
neurips,2019,1,1562,Yi-Chen,Lo,gmail,National Tsing Hua University,howardyclo@gmail.com,One-Shot Object Detection with Co-Attention and Co-Excitation
neurips,2019,2,1562,Hwann-Tzong,Chen,nthu,National Tsing Hua University,htchen@cs.nthu.edu.tw,One-Shot Object Detection with Co-Attention and Co-Excitation
neurips,2019,3,1562,Tyng-Luh,Liu,ailabs,Academia Sinica,liutyng@ailabs.tw,One-Shot Object Detection with Co-Attention and Co-Excitation
neurips,2019,0,6448,Julian,Zimmert,ku,University of Copenhagen,zimmert@di.ku.dk,"Connections Between Mirror Descent, Thompson Sampling and the Information Ratio"
neurips,2019,1,6448,Tor,Lattimore,google,DeepMind,lattimore@google.com,"Connections Between Mirror Descent, Thompson Sampling and the Information Ratio"
neurips,2019,0,3704,Xiaobo,Xia,,The University of Sydney / Xidian University,,Are Anchor Points Really Indispensable in Label-Noise Learning?
neurips,2019,1,3704,Tongliang,Liu,,The University of Sydney,,Are Anchor Points Really Indispensable in Label-Noise Learning?
neurips,2019,2,3704,Nannan,Wang,,Xidian University,,Are Anchor Points Really Indispensable in Label-Noise Learning?
neurips,2019,3,3704,Bo,Han,,RIKEN,,Are Anchor Points Really Indispensable in Label-Noise Learning?
neurips,2019,4,3704,Chen,Gong,,Nanjing University of Science and Technology,,Are Anchor Points Really Indispensable in Label-Noise Learning?
neurips,2019,5,3704,Gang,Niu,,RIKEN,,Are Anchor Points Really Indispensable in Label-Noise Learning?
neurips,2019,6,3704,Masashi,Sugiyama,,RIKEN / University of Tokyo,,Are Anchor Points Really Indispensable in Label-Noise Learning?
neurips,2019,0,2222,Linfeng,Zhang,tsinghua,Tsinghua University,zhang-lf19@mails.tsinghua.edu.cn,SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models
neurips,2019,1,2222,Zhanhong,Tan,tsinghua,Tsinghua University,tanzh19@mails.tsinghua.edu.cn,SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models
neurips,2019,2,2222,Jiebo,Song,tsinghua,Institute for Interdisciplinary Information Core Technology,kaisheng@mail.tsinghua.edu.cn,SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models
neurips,2019,3,2222,Jingwei,Chen,tsinghua,Tsinghua University,clbao@mail.tsinghua.edu.cn,SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models
neurips,2019,4,2222,Chenglong,Bao,iiisct,Tsinghua university,songjb@iiisct.com,SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models
neurips,2019,5,2222,Kaisheng,Ma,hisilicon,Tsinghua University,jean.chenjingwei@hisilicon.com,SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models
neurips,2019,0,86,Paroma,Varma,,Stanford University,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,1,86,Frederic,Sala,,Stanford,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,2,86,Shiori,Sagawa,,Stanford University,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,3,86,Jason,Fries,,Stanford University,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,4,86,Daniel,Fu,,Stanford University,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,5,86,Saelig,Khattar,,Stanford University,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,6,86,Ashwini,Ramamoorthy,,Stanford University,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,7,86,Ke,Xiao,,Stanford University,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,8,86,Kayvon,Fatahalian,,Stanford,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,9,86,James,Priest,,Stanford University,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,10,86,Christopher,Ré,,Stanford,,Multi-Resolution Weak Supervision for Sequential Data
neurips,2019,0,6080,Andy,Shih,ucla,UCLA / Stanford,andyshih@cs.ucla.edu,Smoothing Structured Decomposable Circuits
neurips,2019,1,6080,Guy,Van den Broeck,ucla,UCLA,guyvdb@cs.ucla.edu,Smoothing Structured Decomposable Circuits
neurips,2019,2,6080,Paul,Beame,washington,University of Washington,beame@cs.washington.edu,Smoothing Structured Decomposable Circuits
neurips,2019,3,6080,Antoine,Amarilli,telecom-paris,"LTCI, Télécom ParisTech",antoine.amarilli@telecom-paris.fr,Smoothing Structured Decomposable Circuits
neurips,2019,0,5203,Lingrui,Gan,illinois,University of Illinois at Urbana-Champaign,lgan6@illinois.edu,Bayesian Joint Estimation of Multiple Graphical Models
neurips,2019,1,5203,Xinming,Yang,illinois,University of Illinois at Urbana-Champaign,xyang104@illinois.edu,Bayesian Joint Estimation of Multiple Graphical Models
neurips,2019,2,5203,Naveen,Narisetty,illinois,University of Illinois at Urbana-Champaign,naveen@illinois.edu,Bayesian Joint Estimation of Multiple Graphical Models
neurips,2019,3,5203,Feng,Liang,illinois,Univ. of Illinois Urbana-Champaign,liangf@illinois.edu,Bayesian Joint Estimation of Multiple Graphical Models
neurips,2019,0,3681,Joan,Serrà,,Dolby Laboratories,,Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion
neurips,2019,1,3681,Santiago,Pascual,,Universitat Politècnica de Catalunya,,Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion
neurips,2019,2,3681,Carlos,Segura Perales,,Telefónica Research,,Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion
neurips,2019,0,3487,Michael,Arbel,gmail,UCL,michael.n.arbel@gmail.com,Maximum Mean Discrepancy Gradient Flow
neurips,2019,1,3487,Anna,Korba,ucl,Gatsby Unit - UCL,a.korba@ucl.ac.uk,Maximum Mean Discrepancy Gradient Flow
neurips,2019,2,3487,Adil,SALIM,kaust,KAUST,adil.salim@kaust.edu.sa,Maximum Mean Discrepancy Gradient Flow
neurips,2019,3,3487,Arthur,Gretton,gmail,"Gatsby Unit, UCL",arthur.gretton@gmail.com,Maximum Mean Discrepancy Gradient Flow
neurips,2019,0,6250,Pim,de Haan,,"Qualcomm AI Research, University of Amsterdam",,Causal Confusion in Imitation Learning
neurips,2019,1,6250,Dinesh,Jayaraman,,UC Berkeley,,Causal Confusion in Imitation Learning
neurips,2019,2,6250,Sergey,Levine,,UC Berkeley,,Causal Confusion in Imitation Learning
neurips,2019,0,5592,Yair,Bartal,huji,Hebrew University,yair@cs.huji.ac.il,Dimensionality reduction: theoretical perspective on practical measures
neurips,2019,1,5592,Nova,Fandina,huji,Hebrew University of Jerusalem,fandina@cs.huji.ac.il,Dimensionality reduction: theoretical perspective on practical measures
neurips,2019,2,5592,Ofer,Neiman,bgu,Ben-Gurion University,neimano@cs.bgu.ac.il,Dimensionality reduction: theoretical perspective on practical measures
neurips,2019,0,2016,Xue Bin,Peng,berkeley,UC Berkeley,xbpeng@berkeley.edu,MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies
neurips,2019,1,2016,Michael,Chang,berkeley,"University of California, Berkeley",mbchang@berkeley.edu,MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies
neurips,2019,2,2016,Grace,Zhang,berkeley,UC Berkeley,grace.zhang@berkeley.edu,MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies
neurips,2019,3,2016,Pieter,Abbeel,berkeley,UC Berkeley & covariant.ai,pabbeel@cs.berkeley.edu,MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies
neurips,2019,4,2016,Sergey,Levine,berkeley,UC Berkeley,svlevine@eecs.berkeley.edu,MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies
neurips,2019,0,9024,Aaron,Voelker,uwaterloo,Applied Brain Research,arvoelke@uwaterloo.ca,Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
neurips,2019,1,9024,Ivana,Kaji,uwaterloo,University of Waterloo,i2kajic@uwaterloo.ca,Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
neurips,2019,2,9024,Chris,Eliasmith,uwaterloo,U of Waterloo,celiasmith@uwaterloo.ca,Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
neurips,2019,0,3801,Andreas,Kirsch,ox,University of Oxford,andreas.kirsch@cs.ox.ac.uk,BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning
neurips,2019,1,3801,Joost,van Amersfoort,ox,University of Oxford,joost.van.amersfoort@cs.ox.ac.uk,BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning
neurips,2019,2,3801,Yarin,Gal,ox,University of Oxford,yarin@cs.ox.ac.uk,BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning
neurips,2019,0,8467,Pedro,Mercado,,University of Tübingen,,Generalized Matrix Means for Semi-Supervised Learning with Multilayer Graphs
neurips,2019,1,8467,Francesco,Tudisco,,University of Strathclyde,,Generalized Matrix Means for Semi-Supervised Learning with Multilayer Graphs
neurips,2019,2,8467,Matthias,Hein,,University of Tübingen,,Generalized Matrix Means for Semi-Supervised Learning with Multilayer Graphs
neurips,2019,0,2054,Jonathan,Ullman,berkeley,Northeastern University,asealfon@berkeley.edu,Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy
neurips,2019,1,2054,Adam,Sealfon,neu,UC Berkeley,jullman@ccs.neu.edu,Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy
neurips,2019,0,6597,Mokhtar Z.,Alaya,gmail,"LITIS Lab, University of Rouen",mokhtarzahdi.alaya@gmail.com,Screening Sinkhorn Algorithm for Regularized Optimal Transport
neurips,2019,1,6597,Maxime,Berar,univ-rouen,Université de Rouen,maxime.berar@univ-rouen.fr,Screening Sinkhorn Algorithm for Regularized Optimal Transport
neurips,2019,2,6597,Gilles,Gasso,insa-rouen,LITIS - INSA de Rouen,gilles.gasso@insa-rouen.fr,Screening Sinkhorn Algorithm for Regularized Optimal Transport
neurips,2019,3,6597,Alain,Rakotomamonjy,insa-rouen,Université de Rouen Normandie   Criteo AI Lab,alain.rakoto@insa-rouen.fr,Screening Sinkhorn Algorithm for Regularized Optimal Transport
neurips,2019,0,6481,Thomas,Lucas,,Inria,,Adaptive Density Estimation for Generative Models
neurips,2019,1,6481,Konstantin,Shmelkov,,Huawei,,Adaptive Density Estimation for Generative Models
neurips,2019,2,6481,Karteek,Alahari,,Inria,,Adaptive Density Estimation for Generative Models
neurips,2019,3,6481,Cordelia,Schmid,,Inria / Google,,Adaptive Density Estimation for Generative Models
neurips,2019,4,6481,Jakob,Verbeek,,INRIA,,Adaptive Density Estimation for Generative Models
neurips,2019,0,2401,Heliang,Zheng,ustc,University of Science and Technology of China,1zhenghl@mail.ustc.edu.cn,Learning Deep Bilinear Transformation for Fine-grained Image Representation
neurips,2019,1,2401,Jianlong,Fu,microsoft,Microsoft Research,2jianf@microsoft.com,Learning Deep Bilinear Transformation for Fine-grained Image Representation
neurips,2019,2,2401,Zheng-Jun,Zha,ustc,University of Science and Technology of China,1zhazj@ustc.edu.cn,Learning Deep Bilinear Transformation for Fine-grained Image Representation
neurips,2019,3,2401,Jiebo,Luo,rochester,U. Rochester,3jluo@cs.rochester.edu,Learning Deep Bilinear Transformation for Fine-grained Image Representation
neurips,2019,0,8290,Thomas,PIERROT,instadeep,InstaDeep,t.pierrot@instadeep.com,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,1,8290,Guillaume,Ligner,instadeep,InstaDeep,g.ligner@instadeep.com,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,2,8290,Scott,Reed,google,Google DeepMind,reedscot@google.com,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,3,8290,Olivier,Sigaud,upmc,Sorbonne University,olivier.sigaud@upmc.fr,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,4,8290,Nicolas,Perrin,upmc,"ISIR, Sorbonne Université",perrin@isir.upmc.fr,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,5,8290,Alexandre,Laterre,instadeep,InstaDeep,a.laterre@instadeep.com,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,6,8290,David,Kas,instadeep,InstaDeep,d.kas@instadeep.com,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,7,8290,Karim,Beguir,instadeep,InstaDeep,kb@instadeep.com,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,8,8290,Nando,de Freitas,google,DeepMind,nandodefreitas@google.com,Learning Compositional Neural Programs with Recursive Tree Search and Planning
neurips,2019,0,6086,Mahyar,Fazlyab,upenn,University of Pennsylvania,mahyarfa@seas.upenn.edu,Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks
neurips,2019,1,6086,Alexander,Robey,upenn,University of Pennsylvania,arobey1@seas.upenn.edu,Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks
neurips,2019,2,6086,Hamed,Hassani,upenn,UPenn,hassani@seas.upenn.edu,Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks
neurips,2019,3,6086,Manfred,Morari,upenn,University of Pennsylvania,morari@seas.upenn.edu,Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks
neurips,2019,4,6086,George,Pappas,upenn,University of Pennsylvania,pappasg@seas.upenn.edu,Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks
neurips,2019,0,8723,Samuel,Ainsworth,washington,University of Washington,skainswo@cs.washington.edu,Mo' States Mo' Problems: Emergency Stop Mechanisms from Observation
neurips,2019,1,8723,Matt,Barnes,washington,University of Washington,mbarnes@cs.washington.edu,Mo' States Mo' Problems: Emergency Stop Mechanisms from Observation
neurips,2019,2,8723,Siddhartha,Srinivasa,washington,Amazon + University of Washington,siddh@cs.washington.edu,Mo' States Mo' Problems: Emergency Stop Mechanisms from Observation
neurips,2019,0,6783,Ning,Miao,bytedance,ByteDance AI Lab,miaoning@bytedance.com,Kernelized Bayesian Softmax for Text Generation
neurips,2019,1,6783,Hao,Zhou,bytedance,Bytedance,zhouhao.nlp@bytedance.com,Kernelized Bayesian Softmax for Text Generation
neurips,2019,2,6783,Chengqi,Zhao,bytedance,Bytedance,zhaochengqi.d@bytedance.com,Kernelized Bayesian Softmax for Text Generation
neurips,2019,3,6783,Wenxian,Shi,bytedance,Bytedance,shiwenxian@bytedance.com,Kernelized Bayesian Softmax for Text Generation
neurips,2019,4,6783,Lei,Li,bytedance,ByteDance,lileilab@bytedance.com,Kernelized Bayesian Softmax for Text Generation
neurips,2019,0,4175,Rishidev,Chaudhuri,ucdavis,"University of California, Davis",rchaudhuri@ucdavis.edu,Bipartite expander Hopfield networks as self-decoding high-capacity error correcting codes
neurips,2019,1,4175,Ila,Fiete,mit,Massachusetts Institute of Technology,fiete@mit.edu,Bipartite expander Hopfield networks as self-decoding high-capacity error correcting codes
neurips,2019,0,3358,Zichuan,Lin,,Tsinghua University,,Distributional Reward Decomposition for Reinforcement Learning
neurips,2019,1,3358,Li,Zhao,,Microsoft Research,,Distributional Reward Decomposition for Reinforcement Learning
neurips,2019,2,3358,Derek,Yang,,UC San Diego,,Distributional Reward Decomposition for Reinforcement Learning
neurips,2019,3,3358,Tao,Qin,,Microsoft Research,,Distributional Reward Decomposition for Reinforcement Learning
neurips,2019,4,3358,Tie-Yan,Liu,,Microsoft Research Asia,,Distributional Reward Decomposition for Reinforcement Learning
neurips,2019,5,3358,Guangwen,Yang,,Tsinghua University,,Distributional Reward Decomposition for Reinforcement Learning
neurips,2019,0,4529,Zhuoran,Yang,princeton,Princeton University,zy6@princeton.edu,Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost
neurips,2019,1,4529,Yongxin,Chen,gatech,Georgia Institute of Technology,yongchen@gatech.edu,Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost
neurips,2019,2,4529,Mingyi,Hong,umn,University of Minnesota,mhong@umn.edu,Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost
neurips,2019,3,4529,Zhaoran,Wang,northwestern,Northwestern University,zhaoran.wang@northwestern.edu,Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost
neurips,2019,0,5764,Jun,Yang,toronto,University of Toronto,ssy@cs.toronto.edu,Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes
neurips,2019,1,5764,Shengyang,Sun,toronto,University of Toronto,jun@utstat.toronto.edu,Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes
neurips,2019,2,5764,Daniel,Roy,toronto,Univ of Toronto & Vector,droy@utstat.toronto.edu,Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes
neurips,2019,0,5049,Rixon,Crane,uq,The University of Queensland,r.crane@uq.edu.au,DINGO: Distributed Newton-Type Method for Gradient-Norm Optimization
neurips,2019,1,5049,Fred,Roosta,uq,University of Queensland,fred.roosta@uq.edu.au,DINGO: Distributed Newton-Type Method for Gradient-Norm Optimization
neurips,2019,0,165,Boris,Hanin,tamu,Texas A&M,bhanin@math.tamu.edu,Deep ReLU Networks Have Surprisingly Few Activation Patterns
neurips,2019,1,165,David,Rolnick,upenn,UPenn,drolnick@seas.upenn.edu,Deep ReLU Networks Have Surprisingly Few Activation Patterns
neurips,2019,0,82,Mark,Bun,,Boston University,,Private Hypothesis Selection
neurips,2019,1,82,Gautam,Kamath,,University of Waterloo,,Private Hypothesis Selection
neurips,2019,2,82,Thomas,Steinke,,IBM -- Almaden,,Private Hypothesis Selection
neurips,2019,3,82,Steven,Wu,,University of Minnesota,,Private Hypothesis Selection
neurips,2019,0,5037,Andrei,Barbu,,MIT,,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models
neurips,2019,1,5037,David,Mayo,,MIT,,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models
neurips,2019,2,5037,Julian,Alverio,,MIT,,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models
neurips,2019,3,5037,William,Luo,,MIT,,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models
neurips,2019,4,5037,Christopher,Wang,,Massachusetts Institute of Technology,,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models
neurips,2019,5,5037,Dan,Gutfreund,,IBM Research,,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models
neurips,2019,6,5037,Josh,Tenenbaum,,MIT,,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models
neurips,2019,7,5037,Boris,Katz,,MIT,,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models
neurips,2019,0,7488,Enrique,Sanchez,,Samsung AI Centre,,Object landmark discovery through unsupervised adaptation
neurips,2019,1,7488,Georgios,Tzimiropoulos,,Samsung AI Centre | University of Nottingham,,Object landmark discovery through unsupervised adaptation
neurips,2019,0,186,Yu,Sun,wustl,Washington University in St. Louis,sun.yu@wustl.edu,Block Coordinate Regularization by Denoising
neurips,2019,1,186,Jiaming,Liu,wustl,Washington University in St. Louis,jiaming.liu@wustl.edu,Block Coordinate Regularization by Denoising
neurips,2019,2,186,Ulugbek,Kamilov,wustl,Washington University in St. Louis,kamilov@wustl.edu,Block Coordinate Regularization by Denoising
neurips,2019,0,6047,Qi,Cai,,Northwestern University,,Neural Temporal-Difference Learning Converges to Global Optima
neurips,2019,1,6047,Zhuoran,Yang,,Princeton University,,Neural Temporal-Difference Learning Converges to Global Optima
neurips,2019,2,6047,Jason,Lee,,Princeton University,,Neural Temporal-Difference Learning Converges to Global Optima
neurips,2019,3,6047,Zhaoran,Wang,,Northwestern University,,Neural Temporal-Difference Learning Converges to Global Optima
neurips,2019,0,5090,Blake,Mason,wisc,University of Wisconsin - Madison,bmason3@wisc.edu,Learning Nearest Neighbor Graphs from Noisy Distance Samples
neurips,2019,1,5090,Ardhendu,Tripathy,wisc,University of Wisconsin - Madison,astripathy@wisc.edu,Learning Nearest Neighbor Graphs from Noisy Distance Samples
neurips,2019,2,5090,Robert,Nowak,wisc,University of Wisconsion-Madison,rdnowak@wisc.edu,Learning Nearest Neighbor Graphs from Noisy Distance Samples
neurips,2019,0,2764,Chi,Han,,Tsinghua University,,Visual Concept-Metaconcept Learning
neurips,2019,1,2764,Jiayuan,Mao,,MIT,,Visual Concept-Metaconcept Learning
neurips,2019,2,2764,Chuang,Gan,,MIT-IBM Watson AI Lab,,Visual Concept-Metaconcept Learning
neurips,2019,3,2764,Josh,Tenenbaum,,MIT,,Visual Concept-Metaconcept Learning
neurips,2019,4,2764,Jiajun,Wu,,MIT,,Visual Concept-Metaconcept Learning
neurips,2019,0,93,Vladimir,Kniaz,gosniias,IEEE,vl.kniaz@gosniias.ru,The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection
neurips,2019,1,93,Vladimir,Knyaz,gosniias,State Research Institute of Aviation Systems,knyaz@gosniias.ru,The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection
neurips,2019,2,93,Fabio,Remondino,fbk,"""Fondazione Bruno Kessler, Italy""",remondino@fbk.eu,The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection
neurips,2019,0,7097,Jonathan,Sauder,hpi,Hasso Plattner Institute,jonathan.sauder@student.hpi.de,Self-Supervised Deep Learning on Point Clouds by Reconstructing Space
neurips,2019,1,7097,Bjarne,Sievers,hpi,Hasso-Plattner-Institut,bjarne.sievers@student.hpi.de,Self-Supervised Deep Learning on Point Clouds by Reconstructing Space
neurips,2019,0,5706,Ilias,Diakonikolas,gmail,UW Madison,ilias.diakonikolas@gmail.com,Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering
neurips,2019,1,5706,Daniel,Kane,gmail,UCSD,s.sushrut@gmail.com,Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering
neurips,2019,2,5706,Sushrut,Karmalkar,ucsd,The University of Texas at Austin,dakane@ucsd.edu,Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering
neurips,2019,3,5706,Eric,Price,utexas,University of Texas at Austin,ecprice@cs.utexas.edu,Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering
neurips,2019,4,5706,Alistair,Stewart,gmail,University of Southern California,stewart.al@gmail.com,Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering
neurips,2019,0,7407,Cagatay,Yildiz,aalto,Aalto University,cagatay.yildiz@aalto.fi,ODE2VAE: Deep generative second order ODEs with Bayesian neural networks
neurips,2019,1,7407,Markus,Heinonen,aalto,Aalto University,markus.o.heinonen@aalto.fi,ODE2VAE: Deep generative second order ODEs with Bayesian neural networks
neurips,2019,2,7407,Harri,Lahdesmaki,aalto,Aalto University,harri.lahdesmaki@aalto.fi,ODE2VAE: Deep generative second order ODEs with Bayesian neural networks
neurips,2019,0,7048,Muhammad Muzammal,Naseer,anu,Australian National University (ANU),muzammal.naseer@anu.edu.au,Cross-Domain Transferability of Adversarial Perturbations
neurips,2019,1,7048,Salman,Khan,anu,Inception Institute of Artificial Intelligence,fatih.porikli@anu.edu.au,Cross-Domain Transferability of Adversarial Perturbations
neurips,2019,2,7048,Muhammad Haris,Khan,inceptioniai,Inception Institute of Artificial Intelligence,salman.khan@inceptioniai.org,Cross-Domain Transferability of Adversarial Perturbations
neurips,2019,3,7048,Fahad,Shahbaz Khan,inceptioniai,Inception Institute of Artificial Intelligence,muhammad.haris@inceptioniai.org,Cross-Domain Transferability of Adversarial Perturbations
neurips,2019,4,7048,Fatih,Porikli,inceptioniai,ANU,fahad.khan@inceptioniai.org,Cross-Domain Transferability of Adversarial Perturbations
neurips,2019,0,6225,Chao,Tao,,Indiana University Bloomington,,Thresholding Bandit with Optimal Aggregate Regret
neurips,2019,1,6225,Saúl,Blanco,,Indiana University,,Thresholding Bandit with Optimal Aggregate Regret
neurips,2019,2,6225,Jian,Peng,,University of Illinois at Urbana-Champaign,,Thresholding Bandit with Optimal Aggregate Regret
neurips,2019,3,6225,Yuan,Zhou,,UIUC,,Thresholding Bandit with Optimal Aggregate Regret
neurips,2019,0,7889,Ciara,Pike-Burke,gmail,Universitat Pompeu Fabra,c.pikeburke@gmail.com,Recovering Bandits
neurips,2019,1,7889,Steffen,Grunewalder,lancaster,Lancaster,s.grunewalder@lancaster.ac.uk,Recovering Bandits
neurips,2019,0,5112,Li Kevin,Wenliang,ucl,"Gatsby Unit, UCL",kevinli@gatsby.ucl.ac.uk,A neurally plausible model for online recognition and postdiction in a dynamical environment
neurips,2019,1,5112,Maneesh,Sahani,ucl,"Gatsby Unit, UCL",maneesh@gatsby.ucl.ac.uk,A neurally plausible model for online recognition and postdiction in a dynamical environment
neurips,2019,0,5450,Noga,Alon,princeton,Princeton,nalon@math.princeton.edu,Limits of Private Learning with Access to Public Data
neurips,2019,1,5450,Raef,Bassily,osu,The Ohio State University,bassily.1@osu.edu,Limits of Private Learning with Access to Public Data
neurips,2019,2,5450,Shay,Moran,gmail,Google AI Princeton,shaymoran1@gmail.com,Limits of Private Learning with Access to Public Data
neurips,2019,0,6251,Pan,Li,illinois,Stanford,panli2@illinois.edu,Optimizing Generalized PageRank Methods for Seed-Expansion Community Detection
neurips,2019,1,6251,I,Chien,illinois,UIUC,ichien3@illinois.edu,Optimizing Generalized PageRank Methods for Seed-Expansion Community Detection
neurips,2019,2,6251,Olgica,Milenkovic,illinois,University of Illinois at Urbana-Champaign,milenkov@illinois.edu,Optimizing Generalized PageRank Methods for Seed-Expansion Community Detection
neurips,2019,0,1042,Matthew,Schlegel,ualberta,University of Alberta,mkschleg@ualberta.ca,Importance Resampling for Off-policy Prediction
neurips,2019,1,1042,Wesley,Chung,ualberta,McGill University,wchung@ualberta.ca,Importance Resampling for Off-policy Prediction
neurips,2019,2,1042,Daniel,Graves,huawei,Huawei Technologies Canada,daniel.graves@huawei.com,Importance Resampling for Off-policy Prediction
neurips,2019,3,1042,Jian,Qian,ulberta,University of Alberta,jq1@ulberta.ca,Importance Resampling for Off-policy Prediction
neurips,2019,4,1042,Martha,White,ulberta,University of Alberta,whitem@ulberta.ca,Importance Resampling for Off-policy Prediction
neurips,2019,0,583,Leonidas,Guibas,,stanford.edu,,A Condition Number for Joint Optimization of Cycle-Consistent Networks
neurips,2019,1,583,Qixing,Huang,,The University of Texas at Austin,,A Condition Number for Joint Optimization of Cycle-Consistent Networks
neurips,2019,2,583,Zhenxiao,Liang,,The University of Texas at Austin,,A Condition Number for Joint Optimization of Cycle-Consistent Networks
neurips,2019,0,7732,Nathaniel,Lahn,vt,Virginia Tech,lahnn@vt.edu,A Graph Theoretic Additive Approximation of Optimal Transport
neurips,2019,1,7732,Deepika,Mulchandani,vt,Walmart Labs,deepikak@vt.edu,A Graph Theoretic Additive Approximation of Optimal Transport
neurips,2019,2,7732,Sharath,Raghvendra,vt,Virginia Tech,sharathr@vt.edu,A Graph Theoretic Additive Approximation of Optimal Transport
neurips,2019,0,5909,Sumeet,Katariya,gmail,UW-Madison and Amazon,sumeetsk@gmail.com,MaxGap Bandit: Adaptive Algorithms for Approximate Ranking
neurips,2019,1,5909,Ardhendu,Tripathy,wisc,University of Wisconsin - Madison,astripathy@wisc.edu,MaxGap Bandit: Adaptive Algorithms for Approximate Ranking
neurips,2019,2,5909,Robert,Nowak,wisc,University of Wisconsion-Madison,rdnowak@wisc.edu,MaxGap Bandit: Adaptive Algorithms for Approximate Ranking
neurips,2019,0,6926,Ronald,Ortner,unileoben,Montanuniversitaet Leoben,rortner@unileoben.ac.at,Regret Bounds for Learning State Representations in Reinforcement Learning
neurips,2019,1,6926,Matteo,Pirotta,fb,Facebook AI Research,pirotta@fb.com,Regret Bounds for Learning State Representations in Reinforcement Learning
neurips,2019,2,6926,Alessandro,Lazaric,inria,Facebook Artificial Intelligence Research,ronan.fruit@inria.fr,Regret Bounds for Learning State Representations in Reinforcement Learning
neurips,2019,3,6926,Ronan,Fruit,fb,Inria Lille,lazaric@fb.com,Regret Bounds for Learning State Representations in Reinforcement Learning
neurips,2019,4,6926,Odalric-Ambrym,Maillard,inria,INRIA,odalric.maillard@inria.fr,Regret Bounds for Learning State Representations in Reinforcement Learning
neurips,2019,0,2137,Rob,Brekelmans,usc,University of Southern Caifornia,brekelma@usc.edu,Exact Rate-Distortion in Autoencoders via Echo Noise
neurips,2019,1,2137,Daniel,Moyer,usc,University of Southern California,moyerd@usc.edu,Exact Rate-Distortion in Autoencoders via Echo Noise
neurips,2019,2,2137,Aram,Galstyan,isi,USC Information Sciences Institute,galstyan@isi.edu,Exact Rate-Distortion in Autoencoders via Echo Noise
neurips,2019,3,2137,Greg,Ver Steeg,isi,USC Information Sciences Institute,gregv@isi.edu,Exact Rate-Distortion in Autoencoders via Echo Noise
neurips,2019,0,3213,Jiong,Zhang,utexas,University of Texas at Austin,zhangjiong724@utexas.edu,AutoAssist: A Framework to Accelerate Training of Deep Neural Networks
neurips,2019,1,3213,Hsiang-Fu,Yu,gmail,Amazon,rofu.yu@gmail.com,AutoAssist: A Framework to Accelerate Training of Deep Neural Networks
neurips,2019,2,3213,Inderjit,Dhillon,utexas,UT Austin & Amazon,inderjit@cs.utexas.edu,AutoAssist: A Framework to Accelerate Training of Deep Neural Networks
neurips,2019,0,3529,Lars,Maaløe,corti,Corti,lm@corti.ai,BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling
neurips,2019,1,3529,Marco,Fraccaro,unumed,Unumed,mf@unumed.com,BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling
neurips,2019,2,3529,Valentin,Liévin,dtu,DTU,valv@dtu.dk,BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling
neurips,2019,3,3529,Ole,Winther,dtu,Technical University of Denmark,olwi@dtu.dk,BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling
neurips,2019,0,365,Miaoyan,Wang,wisc,University of Wisconsin - Madison,miaoyan.wang@wisc.edu,Multiway clustering via tensor block models
neurips,2019,1,365,Yuchen,Zeng,wisc,University of Wisconsin - Madison,yzeng58@wisc.edu,Multiway clustering via tensor block models
neurips,2019,0,1598,Wang-Zhou,Dai,nju,Imperial College London,daiwz@lamda.nju.edu.cn,Bridging Machine Learning and Logical Reasoning by Abductive Learning
neurips,2019,1,1598,Qiuling,Xu,nju,Purdue University,xuql@lamda.nju.edu.cn,Bridging Machine Learning and Logical Reasoning by Abductive Learning
neurips,2019,2,1598,Yang,Yu,nju,Nanjing University,yuy@lamda.nju.edu.cn,Bridging Machine Learning and Logical Reasoning by Abductive Learning
neurips,2019,3,1598,Zhi-Hua,Zhou,nju,Nanjing University,zhouzh@lamda.nju.edu.cn,Bridging Machine Learning and Logical Reasoning by Abductive Learning
neurips,2019,0,1113,Fuhai,Chen,gmail,Xiamen University,cfh3c.xmu@gmail.com,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,1,1113,Rongrong,Ji,gmail,"Xiamen University, China",jjyxmu@gmail.com,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,2,1113,Jiayi,Ji,gmail,Xiamen University,xurigexmu@gmail.com,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,3,1113,Xiaoshuai,Sun,xmu,Xiamen University,rrji@xmu.edu.cn,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,4,1113,Baochang,Zhang,xmu,Beihang University,xssun@xmu.edu.cn,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,5,1113,Xuri,Ge,buaa,Xiamen University,bczhang@buaa.edu.cn,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,6,1113,Yongjian,Wu,tencent,"Tencent Technology (Shanghai) Co.,Ltd",littlekenwu@tencent.com,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,7,1113,Feiyue,Huang,tencent,Tencent,garyhuang@tencent.com,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,8,1113,Yan,Wang,pinterest,Microsoft,yanw@pinterest.com,Variational Structured Semantic Inference for Diverse Image Captioning
neurips,2019,0,8844,Melikasadat,Emami,ucla,UCLA,emami@ucla.edu,Input-Output Equivalence of Unitary and Contractive RNNs
neurips,2019,1,8844,Mojtaba,Sahraee Ardakan,ucla,UCLA,msahraee@ucla.edu,Input-Output Equivalence of Unitary and Contractive RNNs
neurips,2019,2,8844,Sundeep,Rangan,nyu,NYU,srangan@nyu.edu,Input-Output Equivalence of Unitary and Contractive RNNs
neurips,2019,3,8844,Alyson,Fletcher,ucla,UCLA,akfletcher@ucla.edu,Input-Output Equivalence of Unitary and Contractive RNNs
neurips,2019,0,4102,Koen,Helwegen,plumerai,Plumerai,koen@plumerai.com,Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization
neurips,2019,1,4102,James,Widdicombe,plumerai,Plumerai,james@plumerai.com,Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization
neurips,2019,2,4102,Lukas,Geiger,plumerai,Plumerai,lukas@plumerai.com,Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization
neurips,2019,3,4102,Zechun,Liu,plumerai,HKUST,roeland@plumerai.com,Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization
neurips,2019,4,4102,Kwang-Ting,Cheng,ust,Hong Kong University of Science and Technology,zliubq@connect.ust.hk,Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization
neurips,2019,5,4102,Roeland,Nusselder,ust,Plumerai,timcheng@ust.hk,Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization
neurips,2019,0,630,Siqi,Liu,pitt,University of Pittsburgh,milos@pitt.edu,Nonparametric Regressive Point Processes Based on Conditional Gaussian Processes
neurips,2019,1,630,Milos,Hauskrecht,pitt,University of Pittsburgh,siqiliu@cs.pitt.edu,Nonparametric Regressive Point Processes Based on Conditional Gaussian Processes
neurips,2019,0,6867,Antonio,Orvieto,,ETH Zurich,,Continuous-time Models for Stochastic Optimization Algorithms
neurips,2019,1,6867,Aurelien,Lucchi,,ETH Zurich,,Continuous-time Models for Stochastic Optimization Algorithms
neurips,2019,0,5085,Akshay,Agrawal,stanford,Stanford University,akshayka@cs.stanford.edu,Differentiable Convex Optimization Layers
neurips,2019,1,5085,Brandon,Amos,fb,Facebook AI,bda@fb.com,Differentiable Convex Optimization Layers
neurips,2019,2,5085,Shane,Barratt,stanford,Stanford University,boyd@stanford.edu,Differentiable Convex Optimization Layers
neurips,2019,3,5085,Stephen,Boyd,stanford,Stanford University,diamond@cs.stanford.edu,Differentiable Convex Optimization Layers
neurips,2019,4,5085,Steven,Diamond,stanford,Stanford University,sbarratt@stanford.edu,Differentiable Convex Optimization Layers
neurips,2019,5,5085,J. Zico,Kolter,cmu,Carnegie Mellon University / Bosch Center for AI,zkolter@cs.cmu.edu,Differentiable Convex Optimization Layers
neurips,2019,0,6217,Mejbah,Alam,intel,Intel Labs,mejbah.alam@intel.com,A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions
neurips,2019,1,6217,Justin,Gottschlich,intel,Intel Labs,justin.gottschlich@intel.com,A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions
neurips,2019,2,6217,Nesime,Tatbul,mit,Intel Labs and MIT,tatbul@csail.mit.edu,A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions
neurips,2019,3,6217,Javier,Turek,intel,Intel Labs,javier.turek@intel.com,A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions
neurips,2019,4,6217,Tim,Mattson,intel,Intel,timothy.g.mattson@intel.com,A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions
neurips,2019,5,6217,Abdullah,Muzahid,tamu,Texas A&M University,abdullah.muzahid@tamu.edu,A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions
neurips,2019,0,2545,Théo,Ryffel,ens,"ENS, CNRS, PSL University, INRIA Paris",theo.ryffel@ens.fr,Partially Encrypted Deep Learning using Functional Encryption
neurips,2019,1,2545,David,Pointcheval,ens,École Normale Supérieure,edufoursans@ens.fr,Partially Encrypted Deep Learning using Functional Encryption
neurips,2019,2,2545,Francis,Bach,ens,INRIA - Ecole Normale Superieure,romain.gay@ens.fr,Partially Encrypted Deep Learning using Functional Encryption
neurips,2019,3,2545,Edouard,Dufour-Sans,ens,Carnegie Mellon University,francis.bach@ens.fr,Partially Encrypted Deep Learning using Functional Encryption
neurips,2019,4,2545,Romain,Gay,ens,UC Berkeley,david.pointcheval@ens.fr,Partially Encrypted Deep Learning using Functional Encryption
neurips,2019,0,6458,Seongjun,Yun,korea,Korea university,ysj5419@korea.ac.kr,Graph Transformer Networks
neurips,2019,1,6458,Minbyul,Jeong,korea,Korea university,minbyuljeong@korea.ac.kr,Graph Transformer Networks
neurips,2019,2,6458,Raehyun,Kim,korea,Korea university,raehyun@korea.ac.kr,Graph Transformer Networks
neurips,2019,3,6458,Jaewoo,Kang,korea,Korea University,kangj@korea.ac.kr,Graph Transformer Networks
neurips,2019,4,6458,Hyunwoo,Kim,korea,Korea University,hyunwoojkim@korea.ac.kr,Graph Transformer Networks
neurips,2019,0,7549,Giancarlo,Kerg,,MILA,,Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics
neurips,2019,1,7549,Kyle,Goyette,,University of Montreal,,Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics
neurips,2019,2,7549,Maximilian,Puelma Touzel,,Mila,,Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics
neurips,2019,3,7549,Gauthier,Gidel,,Mila,,Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics
neurips,2019,4,7549,Eugene,Vorontsov,,Polytechnique Montreal,,Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics
neurips,2019,5,7549,Yoshua,Bengio,,Mila,,Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics
neurips,2019,6,7549,Guillaume,Lajoie,,Université de Montréal / Mila,,Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics
neurips,2019,0,4624,Guillaume,Lample,fb,Facebook AI Research,glample@fb.com,Large Memory Layers with Product Keys
neurips,2019,1,4624,Alexandre,Sablayrolles,fb,Facebook AI Research,asablayrolles@fb.com,Large Memory Layers with Product Keys
neurips,2019,2,4624,Marc'Aurelio,Ranzato,fb,Facebook AI Research,ranzato@fb.com,Large Memory Layers with Product Keys
neurips,2019,3,4624,Ludovic,Denoyer,fb,Facebook - FAIR,denoyer@fb.com,Large Memory Layers with Product Keys
neurips,2019,4,4624,Herve,Jegou,fb,Facebook AI Research,rvj@fb.com,Large Memory Layers with Product Keys
neurips,2019,0,798,Eugene,Ndiaye,riken,Riken AIP,eugene.ndiaye@riken.jp,Computing Full Conformal Prediction Set with Approximate Homotopy
neurips,2019,1,798,Ichiro,Takeuchi,nitech,Nagoya Institute of Technology,takeuchi.ichiro@nitech.ac.jp,Computing Full Conformal Prediction Set with Approximate Homotopy
neurips,2019,0,3111,Ronghui,You,fudan,Fudan University,rhyou18@fudan.edu.cn,AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification
neurips,2019,1,3111,Zihan,Zhang,fudan,Fudan University,zhangzh17@fudan.edu.cn,AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification
neurips,2019,2,3111,Ziye,Wang,fudan,Fudan University,zywang17@fudan.edu.cn,AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification
neurips,2019,3,3111,Suyang,Dai,fudan,Fudan University,sydai16@fudan.edu.cn,AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification
neurips,2019,4,3111,Hiroshi,Mamitsuka,kyoto-u,Kyoto University / Aalto University,mami@kuicr.kyoto-u.ac.jp,AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification
neurips,2019,5,3111,Shanfeng,Zhu,fudan,Fudan University,zhusf@fudan.edu.cn,AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification
neurips,2019,0,2906,Ashudeep,Singh,cornell,Cornell University,ashudeep@cs.cornell.edu,Policy Learning for Fairness in Ranking
neurips,2019,1,2906,Thorsten,Joachims,cornell,Cornell,tj@cs.cornell.edu,Policy Learning for Fairness in Ranking
neurips,2019,0,1600,Zihan,Zhang,tsinghua,Tsinghua University,zihan-zh17@mails.tsinghua.edu.cn,Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function
neurips,2019,1,1600,Xiangyang,Ji,tsinghua,Tsinghua University,xyji@tsinghua.edu.cn,Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function
neurips,2019,0,6579,Emiel,Hoogeboom,uva,University of Amsterdam,e.hoogeboom@uva.nl,Integer Discrete Flows and Lossless Compression
neurips,2019,1,6579,Jorn,Peters,uva,University of Amsterdam,j.peters@uva.nl,Integer Discrete Flows and Lossless Compression
neurips,2019,2,6579,Rianne,van den Berg,gmail,Google Brain,riannevdberg@gmail.com,Integer Discrete Flows and Lossless Compression
neurips,2019,3,6579,Max,Welling,uva,University of Amsterdam / Qualcomm AI Research,m.welling@uva.nl,Integer Discrete Flows and Lossless Compression
neurips,2019,0,7170,Justin,Cosentino,cosentino,Tsinghua University,justin@cosentino.io,Generative Well-intentioned Networks
neurips,2019,1,7170,Jun,Zhu,tsinghua,Tsinghua University,dcszj@mail.tsinghua.edu.cn,Generative Well-intentioned Networks
neurips,2019,0,5760,Jessica,Finocchiaro,colorado,University of Colorado Boulder,jefi8453@colorado.edu,An Embedding Framework for Consistent Polyhedral Surrogates
neurips,2019,1,5760,Rafael,Frongillo,colorado,CU Boulder,raf@colorado.edu,An Embedding Framework for Consistent Polyhedral Surrogates
neurips,2019,2,5760,Bo,Waggoner,colorado,"U. Colorado, Boulder",bwag@colorado.edu,An Embedding Framework for Consistent Polyhedral Surrogates
neurips,2019,0,3462,Ryo,Karakida,go,National Institute of Advanced Industrial Science and Technology,karakida.ryo@aist.go.jp,The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks
neurips,2019,1,3462,Shotaro,Akaho,go,AIST,s.akaho@aist.go.jp,The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks
neurips,2019,2,3462,Shun-ichi,Amari,riken,RIKEN,amari@brain.riken.jp,The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks
neurips,2019,0,9401,Ping,Li,baidu,Baidu Research USA,liping11@baidu.com,Re-randomized Densification for One Permutation Hashing and Bin-wise Consistent Weighted Sampling
neurips,2019,1,9401,Xiaoyun,Li,rutgers,Rutgers University,xiaoyun.li@rutgers.edu,Re-randomized Densification for One Permutation Hashing and Bin-wise Consistent Weighted Sampling
neurips,2019,2,9401,Cun-Hui,Zhang,rutgers,Rutgers,cunhui@stat.rutgers.edu,Re-randomized Densification for One Permutation Hashing and Bin-wise Consistent Weighted Sampling
neurips,2019,0,1084,Gautam,Goel,,Caltech,,Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization
neurips,2019,1,1084,Yiheng,Lin,,"Institute for Interdisciplinary Information Sciences, Tsinghua University",,Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization
neurips,2019,2,1084,Haoyuan,Sun,,California Institute of Technology,,Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization
neurips,2019,3,1084,Adam,Wierman,,California Institute of Technology,,Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization
neurips,2019,0,673,Brett,Daley,northeastern,Northeastern University,b.daley@northeastern.edu,Reconciling -Returns with Experience Replay
neurips,2019,1,673,Christopher,Amato,northeastern,Northeastern University,c.amato@northeastern.edu,Reconciling -Returns with Experience Replay
neurips,2019,0,4979,Giulia,Luise,ucl,University College London,g.luise.16@ucl.ac.uk,Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm
neurips,2019,1,4979,Saverio,Salzo,iit,Istituto Italiano di Tecnologia,saverio.salzo@iit.it,Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm
neurips,2019,2,4979,Massimiliano,Pontil,ucl,IIT & UCL,m.pontil@cs.ucl.ac.uk,Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm
neurips,2019,3,4979,Carlo,Ciliberto,ic,Imperial College London,c.ciliberto@ic.ac.uk,Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm
neurips,2019,0,4666,Shaofeng,Zou,buffalo,"University at Buffalo, the State University of New York",szou3@buffalo.edu,Finite-Sample Analysis for SARSA with Linear Function Approximation
neurips,2019,1,4666,Tengyu,Xu,osu,The Ohio State University,xu.3260@osu.edu,Finite-Sample Analysis for SARSA with Linear Function Approximation
neurips,2019,2,4666,Yingbin,Liang,osu,The Ohio State University,liang.889@osu.edu,Finite-Sample Analysis for SARSA with Linear Function Approximation
neurips,2019,0,3726,Fenglin,Liu,pku,Peking University,fenglinliu98@pku.edu.cn,Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
neurips,2019,1,3726,Yuanxin,Liu,pku,"Institute of Information Engineering, Chinese Academy of Sciences; SCS, University of Chinese Academy of Sciences",renxc@pku.edu.cn,Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
neurips,2019,2,3726,Xuancheng,Ren,pku,Peking University,xusun@pku.edu.cn,Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
neurips,2019,3,3726,Xiaodong,He,iie,JD AI research,liuyuanxin@iie.ac.cn,Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
neurips,2019,4,3726,Xu,Sun,jd,Peking University,xiaodong.he@jd.com,Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
neurips,2019,0,390,Xuanyi,Dong,uts,University of Technology Sydney,xuanyi.dong@student.uts.edu.au,Network Pruning via Transformable Architecture Search
neurips,2019,1,390,Yi,Yang,uts,UTS,yi.yang@uts.edu.au,Network Pruning via Transformable Architecture Search
neurips,2019,0,379,Wang Chi,Cheung,nus,"Department of Industrial Systems Engineering and Management, National University of Singapore",isecwc@nus.edu.sg,Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives
neurips,2019,0,1308,Aviv,Rosenberg,gmail,Tel Aviv University,avivros007@gmail.com,Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function
neurips,2019,1,1308,Yishay,Mansour,gmail,Tel Aviv University / Google,mansour.yishay@gmail.com,Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function
neurips,2019,0,6721,Shin,Matsushima,,The University of Tokyo,,Selective Sampling-based Scalable Sparse Subspace Clustering
neurips,2019,1,6721,Maria,Brbic,,Stanford University,,Selective Sampling-based Scalable Sparse Subspace Clustering
neurips,2019,0,5442,Joe,Kileel,,Princeton University,,On the Expressive Power of Deep Polynomial Neural Networks
neurips,2019,1,5442,Matthew,Trager,,NYU,,On the Expressive Power of Deep Polynomial Neural Networks
neurips,2019,2,5442,Joan,Bruna,,NYU,,On the Expressive Power of Deep Polynomial Neural Networks
neurips,2019,0,9187,Eleanor,Batty,,Columbia University,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,1,9187,Matthew,Whiteway,,Columbia University,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,2,9187,Shreya,Saxena,,Columbia University,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,3,9187,Dan,Biderman,,Columbia University,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,4,9187,Taiga,Abe,,Columbia University,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,5,9187,Simon,Musall,,Cold Spring Harbor Laboratory,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,6,9187,Winthrop,Gillis,,Harvard Medical School,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,7,9187,Jeffrey,Markowitz,,Harvard Medical School,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,8,9187,Anne,Churchland,,Cold Spring Harbor Laboratory,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,9,9187,John,Cunningham,,University of Columbia,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,10,9187,Sandeep,Datta,,Harvard Medical School,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,11,9187,Scott,Linderman,,Stanford University,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,12,9187,Liam,Paninski,,Columbia University,,BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos
neurips,2019,0,7804,Vickram,Rajendran,jhuapl,The Johns Hopkins University Applied Physics Lab,vickram.rajendran@jhuapl.edu,Accurate Layerwise Interpretable Competence Estimation
neurips,2019,1,7804,William,LeVine,jhuapl,The Johns Hopkins University Applied Physics Lab,william.levine@jhuapl.edu,Accurate Layerwise Interpretable Competence Estimation
neurips,2019,0,1610,Belhal,Karimi,polytechnique,Ecole Polytechnique,belhal.karimi@polytechnique.edu,On the Global Convergence of (Fast) Incremental Expectation Maximization Methods
neurips,2019,1,1610,Hoi-To,Wai,cuhk,The Chinese University of Hong Kong,htwai@se.cuhk.edu.hk,On the Global Convergence of (Fast) Incremental Expectation Maximization Methods
neurips,2019,2,1610,Eric,Moulines,polytechnique,Ecole Polytechnique,eric.moulines@polytechnique.edu,On the Global Convergence of (Fast) Incremental Expectation Maximization Methods
neurips,2019,3,1610,Marc,Lavielle,inria,Inria & Ecole Polytechnique,marc.lavielle@inria.fr,On the Global Convergence of (Fast) Incremental Expectation Maximization Methods
neurips,2019,0,1072,Changxiao,Cai,,Princeton University,,Nonconvex Low-Rank Tensor Completion from Noisy Data
neurips,2019,1,1072,Gen,Li,,Tsinghua University,,Nonconvex Low-Rank Tensor Completion from Noisy Data
neurips,2019,2,1072,H. Vincent,Poor,,Princeton University,,Nonconvex Low-Rank Tensor Completion from Noisy Data
neurips,2019,3,1072,Yuxin,Chen,,Princeton University,,Nonconvex Low-Rank Tensor Completion from Noisy Data
neurips,2019,0,7307,Mahmoud,Assran,mcgill,McGill University / Facebook AI Research,mahmoud.assran@mail.mcgill.ca,Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning
neurips,2019,1,7307,Joshua,Romoff,mcgill,McGill University,joshua.romoff@mail.mcgill.ca,Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning
neurips,2019,2,7307,Nicolas,Ballas,fb,Facebook FAIR,ballasn@fb.com,Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning
neurips,2019,3,7307,Joelle,Pineau,fb,Facebook,jpineau@fb.com,Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning
neurips,2019,4,7307,Michael,Rabbat,fb,Facebook FAIR,mikerabbat@fb.com,Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning
neurips,2019,0,6671,Beidi,Chen,,Rice University,,Fast and Accurate Stochastic Gradient Estimation
neurips,2019,1,6671,Yingchen,Xu,,Airbnb,,Fast and Accurate Stochastic Gradient Estimation
neurips,2019,2,6671,Anshumali,Shrivastava,,Rice University,,Fast and Accurate Stochastic Gradient Estimation
neurips,2019,0,3075,Jianxin,Ma,gmail,Alibaba Group,majx13fromthu@gmail.com,Learning Disentangled Representations for Recommendation
neurips,2019,1,3075,Chang,Zhou,alibaba-inc,Alibaba Group,ericzhou.zc@alibaba-inc.com,Learning Disentangled Representations for Recommendation
neurips,2019,2,3075,Peng,Cui,tsinghua,Tsinghua University,cuip@tsinghua.edu.cn,Learning Disentangled Representations for Recommendation
neurips,2019,3,3075,Hongxia,Yang,alibaba-inc,Alibaba Group,yang.yhx@alibaba-inc.com,Learning Disentangled Representations for Recommendation
neurips,2019,4,3075,Wenwu,Zhu,tsinghua,Tsinghua University,wwzhu@tsinghua.edu.cn,Learning Disentangled Representations for Recommendation
neurips,2019,0,2112,Qitian,Wu,sjtu,Shanghai Jiao Tong University,echo740@sjtu.edu.cn,Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling
neurips,2019,1,2112,Zixuan,Zhang,sjtu,Shanghai Jiao Tong University,zzx_gongshi117@sjtu.edu.cn,Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling
neurips,2019,2,2112,Xiaofeng,Gao,sjtu,Shanghai Jiao Tong University,gao-xf@cs.sjtu.edu.cn,Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling
neurips,2019,3,2112,Junchi,Yan,sjtu,Shanghai Jiao Tong University,yanjunchi@sjtu.edu.cn,Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling
neurips,2019,4,2112,Guihai,Chen,nju,Shanghai Jiao Tong University,gchen@nju.edu.cn,Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling
neurips,2019,0,9087,Dan,Hendrycks,berkeley,UC Berkeley,hendrycks@berkeley.edu,Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty
neurips,2019,1,9087,Mantas,Mazeika,illinois,University of Chicago,mantas3@illinois.edu,Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty
neurips,2019,2,9087,Saurav,Kadavath,berkeley,UC Berkeley,sauravkadavath@berkeley.edu,Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty
neurips,2019,3,9087,Dawn,Song,berkeley,UC Berkeley,dawnsong@berkeley.edu,Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty
neurips,2019,0,9251,Arturs,Backurs,ttic,MIT,backurs@ttic.edu,Space and Time Efficient Kernel Density Estimation in High Dimensions
neurips,2019,1,9251,Piotr,Indyk,mit,MIT,indyk@mit.edu,Space and Time Efficient Kernel Density Estimation in High Dimensions
neurips,2019,2,9251,Tal,Wagner,mit,MIT,talw@mit.edu,Space and Time Efficient Kernel Density Estimation in High Dimensions
neurips,2019,0,5787,Xiaoyun,Li,rutgers,Rutgers University,xiaoyun.li@rutgers.edu,Random Projections with Asymmetric Quantization
neurips,2019,1,5787,Ping,Li,baidu,Baidu Research USA,liping11@baidu.com,Random Projections with Asymmetric Quantization
neurips,2019,0,6878,Xuhui,Fan,,University of New South Wales,,Scalable Deep Generative Relational Model with High-Order Node Dependence
neurips,2019,1,6878,Bin,Li,,Fudan University,,Scalable Deep Generative Relational Model with High-Order Node Dependence
neurips,2019,2,6878,Caoyuan,Li,,UTS,,Scalable Deep Generative Relational Model with High-Order Node Dependence
neurips,2019,3,6878,Scott,SIsson,,"University of New South Wales, Sydney",,Scalable Deep Generative Relational Model with High-Order Node Dependence
neurips,2019,4,6878,Ling,Chen,,""" University of Technology, Sydney, Australia""",,Scalable Deep Generative Relational Model with High-Order Node Dependence
neurips,2019,0,1035,Kamil,Ciosek,microsoft,Microsoft,kamil.ciosek@microsoft.com,Better Exploration with Optimistic Actor Critic
neurips,2019,1,1035,Quan,Vuong,ucsd,University of California San Diego,qvuong@ucsd.edu,Better Exploration with Optimistic Actor Critic
neurips,2019,2,1035,Robert,Loftin,microsoft,Microsoft Research,t-roloft@microsoft.com,Better Exploration with Optimistic Actor Critic
neurips,2019,3,1035,Katja,Hofmann,microsoft,Microsoft Research,katja.hofmann@microsoft.com,Better Exploration with Optimistic Actor Critic
neurips,2019,0,5000,Zhiqi,Bu,,University of Pennsylvania,,Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate Message Passing
neurips,2019,1,5000,Jason,Klusowski,,Rutgers University,,Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate Message Passing
neurips,2019,2,5000,Cynthia,Rush,,Columbia University,,Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate Message Passing
neurips,2019,3,5000,Weijie,Su,,"The Wharton School, University of Pennsylvania",,Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate Message Passing
neurips,2019,0,3589,Taufik,Xu,gmail,Tsinghua University,kunxu.thu@gmail.com,Multi-objects Generation with Amortized Structural Regularization
neurips,2019,1,3589,Chongxuan,LI,gmail,Tsinghua University,chongxuanli1991@gmail.com,Multi-objects Generation with Amortized Structural Regularization
neurips,2019,2,3589,Jun,Zhu,tsinghua,Tsinghua University,dcszj@tsinghua.edu.cn,Multi-objects Generation with Amortized Structural Regularization
neurips,2019,3,3589,Bo,Zhang,tsinghua,Tsinghua University,dcszb@tsinghua.edu.cn,Multi-objects Generation with Amortized Structural Regularization
neurips,2019,0,9083,Yingdong,Lu,ibm,IBM Research,yingdong@us.ibm.com,A Family of Robust Stochastic Operators for Reinforcement Learning
neurips,2019,1,9083,Mark,Squillante,ibm,IBM Research,mss@us.ibm.com,A Family of Robust Stochastic Operators for Reinforcement Learning
neurips,2019,2,9083,Chai Wah,Wu,ibm,IBM,cwwu@us.ibm.com,A Family of Robust Stochastic Operators for Reinforcement Learning
neurips,2019,0,2738,Ari,Morcos,fb,Facebook AI Research,arimorcos@fb.com,One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers
neurips,2019,1,2738,Haonan,Yu,fb,Facebook AI Research,michela@fb.com,One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers
neurips,2019,2,2738,Michela,Paganini,gmail,Facebook AI Research,haonanu@gmail.com,One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers
neurips,2019,3,2738,Yuandong,Tian,fb,Facebook AI Research,yuandong@fb.com,One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers
neurips,2019,0,4425,Shanshan,Wu,utexas,University of Texas at Austin,shanshan@utexas.edu,Learning Distributions Generated by One-Layer ReLU Networks
neurips,2019,1,4425,Alexandros,Dimakis,utexas,"University of Texas, Austin",dimakis@austin.utexas.edu,Learning Distributions Generated by One-Layer ReLU Networks
neurips,2019,2,4425,Sujay,Sanghavi,utexas,UT-Austin,sanghavi@mail.utexas.edu,Learning Distributions Generated by One-Layer ReLU Networks
neurips,2019,0,4132,Niklas,Gebauer,gmail,Technische Universität Berlin,n.wa.gebauer@gmail.com,Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules
neurips,2019,1,4132,Michael,Gastegger,tu-berlin,Technische Universität Berlin,michael.gastegger@tu-berlin.de,Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules
neurips,2019,2,4132,Kristof,Schütt,tu-berlin,TU Berlin,kristof.schuett@tu-berlin.de,Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules
neurips,2019,0,3268,Yihe,Dong,gmail,Microsoft,yihedong@gmail.com,Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection
neurips,2019,1,3268,Samuel,Hopkins,berkeley,UC Berkeley,hopkins@berkeley.edu,Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection
neurips,2019,2,3268,Jerry,Li,microsoft,Microsoft,jerrl@microsoft.com,Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection
neurips,2019,0,7751,Kenji,Fukumizu,ism,Institute of Statistical Mathematics / Preferred Networks / RIKEN AIP,fukumizu@ism.ac.jp,Semi-flat minima and saddle points by embedding neural networks to overparameterization
neurips,2019,1,7751,Shoichiro,Yamaguchi,ism,Preferred Networks,mototake@ism.ac.jp,Semi-flat minima and saddle points by embedding neural networks to overparameterization
neurips,2019,2,7751,Yoh-ichi,Mototake,ism,Institute of Statistical Mathematics,mirai@ism.ac.jp,Semi-flat minima and saddle points by embedding neural networks to overparameterization
neurips,2019,3,7751,Mirai,Tanaka,preferred,The Institute of Statistical Mathematics / RIKEN,guguchi@preferred.jp,Semi-flat minima and saddle points by embedding neural networks to overparameterization
neurips,2019,0,2047,Devin,Reich,dowsley,University of Washington Tacoma,rafael@dowsley.net,Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation
neurips,2019,1,2047,Ariel,Todoki,uw,University of Washington Tacoma,dreich@uw.edu,Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation
neurips,2019,2,2047,Rafael,Dowsley,uw,Bar-Ilan University,atodoki@uw.edu,Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation
neurips,2019,3,2047,Martine,De Cock,uw,University of Washington Tacoma,mdecock@uw.edu,Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation
neurips,2019,4,2047,anderson,nascimento,uw,UW,andclay@uw.edu,Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation
neurips,2019,0,1704,Matthew,Joseph,upenn,University of Pennsylvania,majos@cis.upenn.edu,Locally Private Gaussian Estimation
neurips,2019,1,1704,Janardhan,Kulkarni,google,Microsoft Research,maojm@google.com,Locally Private Gaussian Estimation
neurips,2019,2,1704,Jieming,Mao,microsoft,Google Research,jakul@microsoft.com,Locally Private Gaussian Estimation
neurips,2019,3,1704,Steven,Wu,umn,University of Minnesota,zsw@umn.edu,Locally Private Gaussian Estimation
neurips,2019,0,4559,Zhihui,Zhu,jhu,Johns Hopkins University,zzhu29@jhu.edu,Distributed Low-rank Matrix Factorization With Exact Consensus
neurips,2019,1,4559,Qiuwei,Li,mines,Colorado School of Mines,qiuli@mines.edu,Distributed Low-rank Matrix Factorization With Exact Consensus
neurips,2019,2,4559,Xinshuo,Yang,mines,Colorado School of Mines,xinshuoyang@mines.edu,Distributed Low-rank Matrix Factorization With Exact Consensus
neurips,2019,3,4559,Gongguo,Tang,mines,Colorado School of Mines,gtang@mines.edu,Distributed Low-rank Matrix Factorization With Exact Consensus
neurips,2019,4,4559,Michael,Wakin,mines,Colorado School of Mines,mwakin@mines.edu,Distributed Low-rank Matrix Factorization With Exact Consensus
neurips,2019,0,3861,Laurence,Aitchison,gmail,University of Cambridge,laurence.aitchison@gmail.com,Tensor Monte Carlo: Particle Methods for the GPU era
neurips,2019,0,5361,Zhibing,Zhao,rpi,RPI,zhaoz6@rpi.edu,Learning Mixtures of Plackett-Luce Models from Structured Partial Orders
neurips,2019,1,5361,Lirong,Xia,rpi,RPI,xial@cs.rpi.edu,Learning Mixtures of Plackett-Luce Models from Structured Partial Orders
neurips,2019,0,7726,Victor,Garcia Satorras,uva,University of Amsterdam,v.garciasatorras@uva.nl,Combining Generative and Discriminative Models for Hybrid Inference
neurips,2019,1,7726,Zeynep,Akata,uni-tuebingen,University of Amsterdam,zeynep.akata@uni-tuebingen.de,Combining Generative and Discriminative Models for Hybrid Inference
neurips,2019,2,7726,Max,Welling,uva,University of Amsterdam / Qualcomm AI Research,m.welling@uva.nl,Combining Generative and Discriminative Models for Hybrid Inference
neurips,2019,0,326,Yuhui,Wang,nuaa,Nanjing University of Aeronautics and Astronautics,y.wang@nuaa.edu.cn,Trust Region-Guided Proximal Policy Optimization
neurips,2019,1,326,Hao,He,nuaa,Nanjing University of Aeronautics and Astronautics,hugo@nuaa.edu.cn,Trust Region-Guided Proximal Policy Optimization
neurips,2019,2,326,Xiaoyang,Tan,nuaa,"Nanjing University of Aeronautics and Astronautics, China",x.tan@nuaa.edu.cn,Trust Region-Guided Proximal Policy Optimization
neurips,2019,3,326,Yaozhong,Gan,nuaa,"Nanjing University of Aeronautics and Astronautics, China",yzgancn@nuaa.edu.cn,Trust Region-Guided Proximal Policy Optimization
neurips,2019,0,5955,Shuai,Zhao,gmail,Zhejiang University,zhaoshuaimcc@gmail.com,Region Mutual Information Loss for Semantic Segmentation
neurips,2019,1,5955,Yang,Wang,hust,Huazhong University of Science and Technology,wangyang_sky@hust.edu.cn,Region Mutual Information Loss for Semantic Segmentation
neurips,2019,2,5955,Zheng,Yang,fabu,FABU,yangzheng@fabu.ai,Region Mutual Information Loss for Semantic Segmentation
neurips,2019,3,5955,Deng,Cai,zju,ZJU,dcai@zju.edu.cn,Region Mutual Information Loss for Semantic Segmentation
neurips,2019,0,4864,Junyu,Zhang,,University of Minnesota,,A Stochastic Composite Gradient Method with Incremental Variance Reduction
neurips,2019,1,4864,Lin,Xiao,,Microsoft Research,,A Stochastic Composite Gradient Method with Incremental Variance Reduction
neurips,2019,0,4134,Akshay,Balsubramani,stanford,Stanford,abalsubr@stanford.edu,An adaptive nearest neighbor rule for classification
neurips,2019,1,4134,Sanjoy,Dasgupta,ucsd,UC San Diego,dasgupta@eng.ucsd.edu,An adaptive nearest neighbor rule for classification
neurips,2019,2,4134,yoav,Freund,ucsd,UCSD,yfreund@eng.ucsd.edu,An adaptive nearest neighbor rule for classification
neurips,2019,3,4134,Shay,Moran,princeton,Google AI Princeton,shaym@princeton.edu,An adaptive nearest neighbor rule for classification
neurips,2019,0,5712,Ehsan,Hajiramezanali,tamu,Texas A&M University,ehsanr@tamu.edu,Variational Graph Recurrent Neural Networks
neurips,2019,1,5712,Arman,Hasanzadeh,tamu,Texas A&M University,armanihm@tamu.edu,Variational Graph Recurrent Neural Networks
neurips,2019,2,5712,Krishna,Narayanan,tamu,Texas A&M University,duffieldng@tamu.edu,Variational Graph Recurrent Neural Networks
neurips,2019,3,5712,Nick,Duffield,tamu,Texas A&M University,krn@tamu.edu,Variational Graph Recurrent Neural Networks
neurips,2019,4,5712,Mingyuan,Zhou,tamu,University of Texas at Austin,xqian@tamu.edu,Variational Graph Recurrent Neural Networks
neurips,2019,5,5712,Xiaoning,Qian,utexas,Texas A&M,mingyuan.zhou@mccombs.utexas.edu,Variational Graph Recurrent Neural Networks
neurips,2019,0,7862,Johannes,Kirschner,ethz,ETH Zurich,krausea@ethz.ch,Stochastic Bandits with Context Distributions
neurips,2019,1,7862,Andreas,Krause,ethz,ETH Zurich,jkirschner@inf.ethz.ch,Stochastic Bandits with Context Distributions
neurips,2019,0,6163,Joshua,Tobin,,OpenAI,,Geometry-Aware Neural Rendering
neurips,2019,1,6163,Wojciech,Zaremba,,OpenAI,,Geometry-Aware Neural Rendering
neurips,2019,2,6163,Pieter,Abbeel,,UC Berkeley & covariant.ai,,Geometry-Aware Neural Rendering
neurips,2019,0,2410,Cyprien,de Masson d'Autume,google,Google DeepMind,cyprien@google.com,Training Language GANs from Scratch
neurips,2019,1,2410,Shakir,Mohamed,google,DeepMind,mihaelacr@google.com,Training Language GANs from Scratch
neurips,2019,2,2410,Mihaela,Rosca,google,Google DeepMind,jwrae@google.com,Training Language GANs from Scratch
neurips,2019,3,2410,Jack,Rae,google,"DeepMind, UCL",shakir@google.com,Training Language GANs from Scratch
neurips,2019,0,8164,Othman,El Balghiti,columbia,Columbia University,oe2161@columbia.edu,Generalization Bounds in the Predict-then-Optimize Framework
neurips,2019,1,8164,Adam,Elmachtoub,berkeley,Columbia University,pgrigas@berkeley.edu,Generalization Bounds in the Predict-then-Optimize Framework
neurips,2019,2,8164,Paul,Grigas,columbia,UC Berkeley,adam@ieor.columbia.edu,Generalization Bounds in the Predict-then-Optimize Framework
neurips,2019,3,8164,Ambuj,Tewari,umich,University of Michigan,tewaria@umich.edu,Generalization Bounds in the Predict-then-Optimize Framework
neurips,2019,0,3007,Andrea,Zanette,stanford,Stanford University,zanette@stanford.edu,Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model
neurips,2019,1,3007,Mykel,Kochenderfer,stanford,Stanford University,mykel@stanford.edu,Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model
neurips,2019,2,3007,Emma,Brunskill,stanford,Stanford University,ebrun@cs.stanford.edu,Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model
neurips,2019,0,5866,Chih-Kuan,Yeh,,Carnegie Mellon University,,On the (In)fidelity and Sensitivity of Explanations
neurips,2019,1,5866,Cheng-Yu,Hsieh,,National Taiwan University,,On the (In)fidelity and Sensitivity of Explanations
neurips,2019,2,5866,Arun,Suggala,,Carnegie Mellon University,,On the (In)fidelity and Sensitivity of Explanations
neurips,2019,3,5866,David,Inouye,,Carnegie Mellon University,,On the (In)fidelity and Sensitivity of Explanations
neurips,2019,4,5866,Pradeep,Ravikumar,,Carnegie Mellon University,,On the (In)fidelity and Sensitivity of Explanations
neurips,2019,0,7377,He,Lyu,msu,Michigan State University,lyuhe@msu.edu,Manifold denoising by Nonlinear Robust Principal Component Analysis
neurips,2019,1,7377,Ningyu,Sha,msu,MSU,shaningy@msu.edu,Manifold denoising by Nonlinear Robust Principal Component Analysis
neurips,2019,2,7377,Shuyang,Qin,msu,Michigan State University,qinshuya@msu.edu,Manifold denoising by Nonlinear Robust Principal Component Analysis
neurips,2019,3,7377,Ming,Yan,msu,Michigan State University,myan@msu.edu,Manifold denoising by Nonlinear Robust Principal Component Analysis
neurips,2019,4,7377,Yuying,Xie,msu,Michigan State University,xyy@msu.edu,Manifold denoising by Nonlinear Robust Principal Component Analysis
neurips,2019,5,7377,Rongrong,Wang,msu,Michigan State University,wangron6@msu.edu,Manifold denoising by Nonlinear Robust Principal Component Analysis
neurips,2019,0,4062,Debarghya,Ghoshdastidar,tum,Technical University Munich,ghoshdas@in.tum.de,Foundations of Comparison-Based Hierarchical Clustering
neurips,2019,1,4062,Michaël,Perrot,mpg,Max Planck Institute for Intelligent Systems,michael.perrot@tuebingen.mpg.de,Foundations of Comparison-Based Hierarchical Clustering
neurips,2019,2,4062,Ulrike,von Luxburg,uni-tuebingen,University of Tübingen,luxburg@informatik.uni-tuebingen.de,Foundations of Comparison-Based Hierarchical Clustering
neurips,2019,0,2840,Pang Wei,Koh,,Stanford University,,On the Accuracy of Influence Functions for Measuring Group Effects
neurips,2019,1,2840,Kai-Siang,Ang,,Stanford University,,On the Accuracy of Influence Functions for Measuring Group Effects
neurips,2019,2,2840,Hubert,Teo,,Stanford University,,On the Accuracy of Influence Functions for Measuring Group Effects
neurips,2019,3,2840,Percy,Liang,,Stanford University,,On the Accuracy of Influence Functions for Measuring Group Effects
neurips,2019,0,2770,Weiyang,Liu,gatech,Georgia Institute of Technology,wyliu@gatech.edu,Neural Similarity Learning
neurips,2019,1,2770,Zhen,Liu,umontreal,"MILA, University of Montreal",zhen.liu.2@umontreal.ca,Neural Similarity Learning
neurips,2019,2,2770,James,Rehg,gatech,Georgia Tech,rehg@gatech.edu,Neural Similarity Learning
neurips,2019,3,2770,Le,Song,gatech,Georgia Institute of Technology,lsong@cc.gatech.edu,Neural Similarity Learning
neurips,2019,0,6625,Majid,Abdolshah,,Deakin University,,Multi-objective Bayesian optimisation with preferences over objectives
neurips,2019,1,6625,Alistair,Shilton,,Deakin University,,Multi-objective Bayesian optimisation with preferences over objectives
neurips,2019,2,6625,Santu,Rana,,Deakin University,,Multi-objective Bayesian optimisation with preferences over objectives
neurips,2019,3,6625,Sunil,Gupta,,Deakin University,,Multi-objective Bayesian optimisation with preferences over objectives
neurips,2019,4,6625,Svetha,Venkatesh,,Deakin University,,Multi-objective Bayesian optimisation with preferences over objectives
neurips,2019,0,2674,Wei,Qian,cornell,Cornell Univeristy,wq34@cornell.edu,Global Convergence of Least Squares EM for Demixing Two Log-Concave Densities
neurips,2019,1,2674,Yuqian,Zhang,cornell,Cornell University,yz2557@cornell.edu,Global Convergence of Least Squares EM for Demixing Two Log-Concave Densities
neurips,2019,2,2674,Yudong,Chen,cornell,Cornell University,yudong.chen@cornell.edu,Global Convergence of Least Squares EM for Demixing Two Log-Concave Densities
neurips,2019,0,6256,Amanda,Gentzel,,UMass Amherst,,The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data
neurips,2019,1,6256,Dan,Garant,,C&S Wholesale Grocers,,The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data
neurips,2019,2,6256,David,Jensen,,Univ. of Massachusetts,,The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data
neurips,2019,0,1711,Yusuke,Tanaka,,NTT,,Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs
neurips,2019,1,1711,Toshiyuki,Tanaka,,Kyoto University,,Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs
neurips,2019,2,1711,Tomoharu,Iwata,,NTT,,Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs
neurips,2019,3,1711,Takeshi,Kurashima,,NTT Corporation,,Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs
neurips,2019,4,1711,Maya,Okawa,,NTT,,Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs
neurips,2019,5,1711,Yasunori,Akagi,,"NTT Service Evolution Laboratories, NTT Corporation",,Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs
neurips,2019,6,1711,Hiroyuki,Toda,,"NTT Service Evolution Laboratories, NTT Corporation, Japan",,Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs
neurips,2019,0,107,Thanh Huy,Nguyen,,Telecom ParisTech,,First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise
neurips,2019,1,107,Umut,Simsekli,,Institut Polytechnique de Paris/ University of Oxford,,First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise
neurips,2019,2,107,Mert,Gurbuzbalaban,,Rutgers,,First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise
neurips,2019,3,107,Gaël,RICHARD,,Télécom ParisTech,,First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise
neurips,2019,0,3080,Bin,Shi,berkeley,UC Berkeley,binshi@berkeley.edu,Acceleration via Symplectic Discretization of High-Resolution Differential Equations
neurips,2019,1,3080,Simon,Du,ias,Institute for Advanced Study,ssdu@ias.edu,Acceleration via Symplectic Discretization of High-Resolution Differential Equations
neurips,2019,2,3080,Weijie,Su,upenn,"The Wharton School, University of Pennsylvania",suw@wharton.upenn.edu,Acceleration via Symplectic Discretization of High-Resolution Differential Equations
neurips,2019,3,3080,Michael,Jordan,berkeley,UC Berkeley,jordan@cs.berkeley.edu,Acceleration via Symplectic Discretization of High-Resolution Differential Equations
neurips,2019,0,4701,Jennifer,Cardona,stanford,Stanford University,jcard27@stanford.edu,Seeing the Wind: Visual Wind Speed Prediction with a Coupled Convolutional and Recurrent Neural Network
neurips,2019,1,4701,Michael,Howland,stanford,Stanford University,mhowland@stanford.edu,Seeing the Wind: Visual Wind Speed Prediction with a Coupled Convolutional and Recurrent Neural Network
neurips,2019,2,4701,John,Dabiri,caltech,Stanford University,jodabiri@caltech.edu,Seeing the Wind: Visual Wind Speed Prediction with a Coupled Convolutional and Recurrent Neural Network
neurips,2019,0,1362,Eliya,Nachmani,,Tel Aviv University and Facebook AI Research,,Hyper-Graph-Network Decoders for Block Codes
neurips,2019,1,1362,Lior,Wolf,,Facebook AI Research,,Hyper-Graph-Network Decoders for Block Codes
neurips,2019,0,8347,Vayer,Titouan,irisa,IRISA,titouan.vayer@irisa.fr,Sliced Gromov-Wasserstein
neurips,2019,1,8347,Rémi,Flamary,unice,Université Côte d'Azur,remi.flamary@unice.fr,Sliced Gromov-Wasserstein
neurips,2019,2,8347,Nicolas,Courty,univ-rennes2,"IRISA, Universite Bretagne-Sud",romain.tavenard@univ-rennes2.fr,Sliced Gromov-Wasserstein
neurips,2019,3,8347,Romain,Tavenard,irisa,LETG-Rennes /  IRISA-Obelix,laetitia.chapel@irisa.fr,Sliced Gromov-Wasserstein
neurips,2019,4,8347,Laetitia,Chapel,irisa,IRISA,nicolas.courty@irisa.fr,Sliced Gromov-Wasserstein
neurips,2019,0,4413,Shanshan,Wu,utexas,University of Texas at Austin,shanshan@utexas.edu,Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models
neurips,2019,1,4413,Sujay,Sanghavi,utexas,UT-Austin,sanghavi@mail.utexas.edu,Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models
neurips,2019,2,4413,Alexandros,Dimakis,utexas,"University of Texas, Austin",dimakis@austin.utexas.edu,Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models
neurips,2019,0,1003,Talfan,Evans,,University College London,,Coordinated hippocampal-entorhinal replay as structural inference
neurips,2019,1,1003,Neil,Burgess,,University College London,,Coordinated hippocampal-entorhinal replay as structural inference
neurips,2019,0,5035,Zhihui,Zhu,jhu,Johns Hopkins University,zzhu29@jhu.edu,A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning
neurips,2019,1,5035,Tianyu,Ding,jhu,Johns Hopkins University,tding1@jhu.edu,A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning
neurips,2019,2,5035,Daniel,Robinson,shanghaitech,Johns Hopkins University,mtsakiris@shanghaitech.edu.cn,A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning
neurips,2019,3,5035,Manolis,Tsakiris,lehigh,ShanghaiTech University,daniel.p.robinson@lehigh.edu,A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning
neurips,2019,4,5035,René,Vidal,jhu,Mathematical Institute for Data Science Johns Hopkins University,rvidal@jhu.edu,A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning
neurips,2019,0,4597,Karl,Krauth,berkeley,UC berkeley,karlk@berkeley.edu,Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator
neurips,2019,1,4597,Stephen,Tu,berkeley,UC Berkeley,stephentu@berkeley.edu,Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator
neurips,2019,2,4597,Benjamin,Recht,berkeley,UC Berkeley,brecht@berkeley.edu,Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator
neurips,2019,0,6476,Fariborz,Salehi,,California Institute of Technology,,The Impact of Regularization on High-dimensional Logistic Regression
neurips,2019,1,6476,Ehsan,Abbasi,,Caltech,,The Impact of Regularization on High-dimensional Logistic Regression
neurips,2019,2,6476,Babak,Hassibi,,Caltech,,The Impact of Regularization on High-dimensional Logistic Regression
neurips,2019,0,467,Jinwoo,Choi,vt,Virginia Tech,jinchoi@vt.edu,Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition
neurips,2019,1,467,Chen,Gao,vt,Virginia Tech,chengao@vt.edu,Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition
neurips,2019,2,467,Joseph C. E.,Messou,vt,Virginia Tech,mejc2014@vt.edu,Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition
neurips,2019,3,467,Jia-Bin,Huang,vt,Virginia Tech,jbhuang@vt.edu,Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition
neurips,2019,0,4930,Boaz,Barak,,Harvard University,,(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs
neurips,2019,1,4930,Chi-Ning,Chou,,Harvard University,,(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs
neurips,2019,2,4930,Zhixian,Lei,,Harvard University,,(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs
neurips,2019,3,4930,Tselil,Schramm,,Harvard University,,(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs
neurips,2019,4,4930,Yueqi,Sheng,,Harvard University,,(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs
neurips,2019,0,2122,Xing,Yan,gmail,City University of Hong Kong,yanxing128@gmail.com,Cross-sectional Learning of Extremal Dependence among Financial Assets
neurips,2019,1,2122,Qi,Wu,cityu,City University of Hong Kong,qiwu55@cityu.edu.hk,Cross-sectional Learning of Extremal Dependence among Financial Assets
neurips,2019,2,2122,Wen,Zhang,gmail,JD Digital,zhangwen.jd@gmail.com,Cross-sectional Learning of Extremal Dependence among Financial Assets
neurips,2019,0,238,Patrick,Putzky,googlemail,University of Amsterdam,patrick.putzky@googlemail.com,Invert to Learn to Invert
neurips,2019,1,238,Max,Welling,googlemail,University of Amsterdam / Qualcomm AI Research,welling.max@googlemail.com,Invert to Learn to Invert
neurips,2019,0,5326,Jenelle,Feather,mit,MIT,jfeather@mit.edu,Metamers of neural networks reveal divergence from human perceptual systems
neurips,2019,1,5326,Alex,Durango,mit,MIT,durangoa@mit.edu,Metamers of neural networks reveal divergence from human perceptual systems
neurips,2019,2,5326,Ray,Gonzalez,mit,MIT,raygon@mit.edu,Metamers of neural networks reveal divergence from human perceptual systems
neurips,2019,3,5326,Josh,McDermott,mit,Massachusetts Institute of Technology,jhm@mit.edu,Metamers of neural networks reveal divergence from human perceptual systems
neurips,2019,0,3960,Xiyang,Hu,,Carnegie Mellon University,,Optimal Sparse Decision Trees
neurips,2019,1,3960,Cynthia,Rudin,,Duke,,Optimal Sparse Decision Trees
neurips,2019,2,3960,Margo,Seltzer,,University of British Columbia,,Optimal Sparse Decision Trees
neurips,2019,0,1798,Hanrui,Zhang,duke,Duke University,hrzhang@cs.duke.edu,Distinguishing Distributions When Samples Are Strategically Transformed
neurips,2019,1,1798,Yu,Cheng,duke,Duke University,yucheng@cs.duke.edu,Distinguishing Distributions When Samples Are Strategically Transformed
neurips,2019,2,1798,Vincent,Conitzer,duke,Duke University,conitzer@cs.duke.edu,Distinguishing Distributions When Samples Are Strategically Transformed
neurips,2019,0,1473,Yixing,Xu,huawei,Huawei Noah's Ark Lab,yixing.xu@huawei.com,Positive-Unlabeled Compression on the Cloud
neurips,2019,1,1473,Yunhe,Wang,huawei,Huawei Noah's Ark Lab,yunhe.wang@huawei.com,Positive-Unlabeled Compression on the Cloud
neurips,2019,2,1473,Hanting,Chen,huawei,Huawei Noah's Ark Lab,kai.han@huawei.com,Positive-Unlabeled Compression on the Cloud
neurips,2019,3,1473,Kai,Han,huawei,Huawei Noah's Ark Lab,xuchunjing@huawei.com,Positive-Unlabeled Compression on the Cloud
neurips,2019,4,1473,Chunjing,XU,pku,Huawei Technologies,htchen@pku.edu.cn,Positive-Unlabeled Compression on the Cloud
neurips,2019,5,1473,Dacheng,Tao,sydney,University of Sydney,dacheng.tao@sydney.edu.au,Positive-Unlabeled Compression on the Cloud
neurips,2019,6,1473,Chang,Xu,sydney,University of Sydney,c.xu@sydney.edu.au,Positive-Unlabeled Compression on the Cloud
neurips,2019,0,8294,Nirandika,Wanigasekara,nus,National University of Singapore,nirandiw@comp.nus.edu.sg,Nonparametric Contextual Bandits in Metric Spaces with Unknown Metric
neurips,2019,1,8294,Christina,Yu,cornell,Cornell University,cleeyu@cornell.edu,Nonparametric Contextual Bandits in Metric Spaces with Unknown Metric
neurips,2019,0,315,Andrey,Kolobov,microsoft,Microsoft Research,akolobov@microsoft.com,Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling
neurips,2019,1,315,Yuval,Peres,gmail,N/A,yperes@gmail.com,Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling
neurips,2019,2,315,Cheng,Lu,microsoft,Microsoft,Cheng.Lu@microsoft.com,Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling
neurips,2019,3,315,Eric,Horvitz,microsoft,Microsoft Research,horvitz@microsoft.com,Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling
neurips,2019,0,1393,Alan,Kuhnle,fsu,Florida State University,akuhnle@fsu.edu,Interlaced Greedy Algorithm for Maximization of Submodular Functions in Nearly Linear Time
neurips,2019,0,5518,Guanghui,Lan,gatech,Georgia Tech,george.lan@isye.gatech.edu,A unified variance-reduced accelerated gradient method for convex optimization
neurips,2019,1,5518,Zhize,Li,tsinghua,"Tsinghua University, and KAUST",zz-li14@mails.tsinghua.edu.cn,A unified variance-reduced accelerated gradient method for convex optimization
neurips,2019,2,5518,Yi,Zhou,ibm,IBM Almaden Research Center,yi.zhou@ibm.com,A unified variance-reduced accelerated gradient method for convex optimization
neurips,2019,0,855,Zhize,Li,gmail,"Tsinghua University, and KAUST",zhizeli.thu@gmail.com,SSRGD: Simple Stochastic Recursive Gradient Descent for Escaping Saddle Points
neurips,2019,0,4795,Chaofan,Chen,duke,Duke University,cfchen@cs.duke.edu,This Looks Like That: Deep Learning for Interpretable Image Recognition
neurips,2019,1,4795,Oscar,Li,duke,Carnegie Mellon University,oscarli@alumni.duke.edu,This Looks Like That: Deep Learning for Interpretable Image Recognition
neurips,2019,2,4795,Daniel,Tao,duke,Duke University,chaofan.tao@duke.edu,This Looks Like That: Deep Learning for Interpretable Image Recognition
neurips,2019,3,4795,Alina,Barnett,duke,Duke University,abarnett@cs.duke.edu,This Looks Like That: Deep Learning for Interpretable Image Recognition
neurips,2019,4,4795,Cynthia,Rudin,mit,Duke,su@ll.mit.edu,This Looks Like That: Deep Learning for Interpretable Image Recognition
neurips,2019,5,4795,Jonathan,Su,duke,MIT Lincoln Laboratory,cynthia@cs.duke.edu,This Looks Like That: Deep Learning for Interpretable Image Recognition
neurips,2019,0,6063,Ilai,Bistritz,stanford,Stanford,bistritz@stanford.edu,Online EXP3 Learning in Adversarial Bandits with Delayed Feedback
neurips,2019,1,6063,Zhengyuan,Zhou,stanford,Stanford University,bambos@stanford.edu,Online EXP3 Learning in Adversarial Bandits with Delayed Feedback
neurips,2019,2,6063,Xi,Chen,stanford,New York University,jose.blanchet@stanford.edu,Online EXP3 Learning in Adversarial Bandits with Delayed Feedback
neurips,2019,3,6063,Nicholas,Bambos,nyu,,zzhou@stern.nyu.edu,Online EXP3 Learning in Adversarial Bandits with Delayed Feedback
neurips,2019,4,6063,Jose,Blanchet,nyu,Stanford University,xchen3@stern.nyu.edu,Online EXP3 Learning in Adversarial Bandits with Delayed Feedback
neurips,2019,0,4090,David,Simchi-Levi,mit,MIT,dslevi@mit.edu,Phase Transitions and Cyclic Phenomena in Bandits with Switching Constraints
neurips,2019,1,4090,Yunzong,Xu,mit,MIT,yxu@mit.edu,Phase Transitions and Cyclic Phenomena in Bandits with Switching Constraints
neurips,2019,0,3239,Wonjae,Kim,kakaocorp,Kakao Corporation,dandelin.kim@kakaocorp.com,Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning
neurips,2019,1,3239,Yoonho,Lee,kakaocorp,Kakao Corporation,eddy.l@kakaocorp.com,Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning
neurips,2019,0,7849,Matt,Jordan,utexas,UT Austin,mjordan@cs.utexas.edu,Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes
neurips,2019,1,7849,Justin,Lewis,utexas,University of Texas at Austin,justin94lewis@utexas.edu,Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes
neurips,2019,2,7849,Alexandros,Dimakis,utexas,"University of Texas, Austin",dimakis@austin.utexas.edu,Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes
neurips,2019,0,2781,Sharon,Qian,harvard,Harvard,sharonqian@g.harvard.edu,Fast Parallel Algorithms for Statistical Subset Selection Problems
neurips,2019,1,2781,Yaron,Singer,harvard,Harvard University,yaron@seas.harvard.edu,Fast Parallel Algorithms for Statistical Subset Selection Problems
neurips,2019,0,1688,Lénaïc,Chizat,u-psud,CNRS,lenaic.chizat@u-psud.fr,On Lazy Training in Differentiable Programming
neurips,2019,1,1688,Edouard,Oyallon,centralesupelec,CNRS/LIP6,edouard.oyallon@centralesupelec.fr,On Lazy Training in Differentiable Programming
neurips,2019,2,1688,Francis,Bach,inria,INRIA - Ecole Normale Superieure,francis.bach@inria.fr,On Lazy Training in Differentiable Programming
neurips,2019,0,4021,Niloy,Biswas,harvard,Harvard University,niloy_biswas@g.harvard.edu,Estimating Convergence of Markov chains with L-Lag Couplings
neurips,2019,1,4021,Pierre,Jacob,ox,Harvard University,paul.vanetti@spc.ox.ac.uk,Estimating Convergence of Markov chains with L-Lag Couplings
neurips,2019,2,4021,Paul,Vanetti,harvard,Oxford,pjacob@fas.harvard.edu,Estimating Convergence of Markov chains with L-Lag Couplings
neurips,2019,0,2820,Gabriele,Farina,cmu,Carnegie Mellon University,gfarina@cs.cmu.edu,Efficient Regret Minimization Algorithm for Extensive-Form Correlated Equilibrium
neurips,2019,1,2820,Chun Kai,Ling,cmu,Carnegie Mellon University,chunkail@cs.cmu.edu,Efficient Regret Minimization Algorithm for Extensive-Form Correlated Equilibrium
neurips,2019,2,2820,Fei,Fang,cmu,Carnegie Mellon University,feif@cs.cmu.edu,Efficient Regret Minimization Algorithm for Extensive-Form Correlated Equilibrium
neurips,2019,3,2820,Tuomas,Sandholm,cmu,"CMU, Strategic Machine, Strategy Robot, Optimized Markets",sandholm@cs.cmu.edu,Efficient Regret Minimization Algorithm for Extensive-Form Correlated Equilibrium
neurips,2019,0,7684,Victor,Veitch,,Columbia University,,Using Embeddings to Correct for Unobserved Confounding in Networks
neurips,2019,1,7684,Yixin,Wang,,Columbia University,,Using Embeddings to Correct for Unobserved Confounding in Networks
neurips,2019,2,7684,David,Blei,,Columbia University,,Using Embeddings to Correct for Unobserved Confounding in Networks
neurips,2019,0,8364,Zhiqiang,Xu,baidu,Baidu Inc.,xuzhiqiang04@baidu.com,Towards Practical Alternating Least-Squares for CCA
neurips,2019,1,8364,Ping,Li,baidu,Baidu Research USA,liping11@baidu.com,Towards Practical Alternating Least-Squares for CCA
neurips,2019,0,4822,Jae Hyun,Lim,,"Mila, University of Montreal",,Neural Multisensory Scene Inference
neurips,2019,1,4822,Pedro,O. Pinheiro,,Element AI,,Neural Multisensory Scene Inference
neurips,2019,2,4822,Negar,Rostamzadeh,,Elemenet AI,,Neural Multisensory Scene Inference
neurips,2019,3,4822,Chris,Pal,,"MILA, Polytechnique Montréal, Element AI",,Neural Multisensory Scene Inference
neurips,2019,4,4822,Sungjin,Ahn,,Rutgers University,,Neural Multisensory Scene Inference
neurips,2019,0,3952,Adam,Bielski,unibe,University of Bern,adam.bielski@inf.unibe.ch,Emergence of Object Segmentation in Perturbed Generative Models
neurips,2019,1,3952,Paolo,Favaro,unibe,"Bern University, Switzerland",paolo.favaro@inf.unibe.ch,Emergence of Object Segmentation in Perturbed Generative Models
neurips,2019,0,1444,Hanjun,Dai,,Georgia Tech,,Learning Transferable Graph Exploration
neurips,2019,1,1444,Yujia,Li,,DeepMind,,Learning Transferable Graph Exploration
neurips,2019,2,1444,Chenglong,Wang,,University of Washington,,Learning Transferable Graph Exploration
neurips,2019,3,1444,Rishabh,Singh,,Google Brain,,Learning Transferable Graph Exploration
neurips,2019,4,1444,Po-Sen,Huang,,DeepMind,,Learning Transferable Graph Exploration
neurips,2019,5,1444,Pushmeet,Kohli,,DeepMind,,Learning Transferable Graph Exploration
neurips,2019,0,1546,Baekjin,Kim,umich,University of Michigan,baekjin@umich.edu,On the Optimality of Perturbations in Stochastic and Adversarial Multi-armed Bandit Problems
neurips,2019,1,1546,Ambuj,Tewari,umich,University of Michigan,tewaria@umich.edu,On the Optimality of Perturbations in Stochastic and Adversarial Multi-armed Bandit Problems
neurips,2019,0,2824,Gabriele,Farina,cmu,Carnegie Mellon University,gfarina@cs.cmu.edu,Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions
neurips,2019,1,2824,Christian,Kroer,columbia,Columbia University,christian.kroer@columbia.edu,Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions
neurips,2019,2,2824,Tuomas,Sandholm,cmu,"CMU, Strategic Machine, Strategy Robot, Optimized Markets",sandholm@cs.cmu.edu,Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions
neurips,2019,0,7280,Dong,Yin,berkeley,UC Berkeley,dongyin@berkeley.edu,A Fourier Perspective on Model Robustness in Computer Vision
neurips,2019,1,7280,Raphael,Gontijo Lopes,google,Google Brain,iraphael@google.com,A Fourier Perspective on Model Robustness in Computer Vision
neurips,2019,2,7280,Jon,Shlens,google,Google Research,shlens@google.com,A Fourier Perspective on Model Robustness in Computer Vision
neurips,2019,3,7280,Ekin Dogus,Cubuk,google,Google Brain,cubuk@google.com,A Fourier Perspective on Model Robustness in Computer Vision
neurips,2019,4,7280,Justin,Gilmer,google,Google Brain,gilmer@google.com,A Fourier Perspective on Model Robustness in Computer Vision
neurips,2019,0,6021,Lizhong,Ding,,Inception Institute of Artificial Intelligence,,Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test
neurips,2019,1,6021,Mengyang,Yu,,Inception Institute of Artificial Intelligence,,Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test
neurips,2019,2,6021,Li,Liu,,Inception Institute of Artificial Intelligence,,Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test
neurips,2019,3,6021,Fan,Zhu,,Inception Institute of Artificial Intelligence,,Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test
neurips,2019,4,6021,Yong,Liu,,"Institute of Information Engineering, CAS",,Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test
neurips,2019,5,6021,Yu,Li,,King Abdullah University of Science and Technology,,Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test
neurips,2019,6,6021,Ling,Shao,,Inception Institute of Artificial Intelligence,,Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test
neurips,2019,0,837,Chris,Russell,,The Alan Turing Institute/ The University of Surrey,,Fixing Implicit Derivatives: Trust-Region Based Learning of Continuous Energy Functions
neurips,2019,1,837,Matteo,Toso,,University of Surrey,,Fixing Implicit Derivatives: Trust-Region Based Learning of Continuous Energy Functions
neurips,2019,2,837,Neill,Campbell,,University of Bath,,Fixing Implicit Derivatives: Trust-Region Based Learning of Continuous Energy Functions
neurips,2019,0,6807,Marco,Bressan,,Sapienza University of Rome,,Correlation Clustering with Adaptive Similarity Queries
neurips,2019,1,6807,Nicolò,Cesa-Bianchi,,Università degli Studi di Milano,,Correlation Clustering with Adaptive Similarity Queries
neurips,2019,2,6807,Andrea,Paudice,,University of Milan,,Correlation Clustering with Adaptive Similarity Queries
neurips,2019,3,6807,Fabio,Vitale,,University of Lille - INRIA Lille (France),,Correlation Clustering with Adaptive Similarity Queries
neurips,2019,0,2760,Eric,Jonas,uchicago,University of Chicago,ericj@uchicago.edu,Deep imitation learning for molecular inverse problems
neurips,2019,0,9274,Fushan,Li,ualberta,University of Alberta,fushan@ualberta.ca,Ease-of-Teaching and Language Structure from Emergent Communication
neurips,2019,1,9274,Michael,Bowling,ualberta,University of Alberta / DeepMind,mbowling@ualberta.ca,Ease-of-Teaching and Language Structure from Emergent Communication
neurips,2019,0,1924,David,Durfee,,Georgia Tech,,Practical Differentially Private Top-k Selection with Pay-what-you-get Composition
neurips,2019,1,1924,Ryan,Rogers,,LinkedIn,,Practical Differentially Private Top-k Selection with Pay-what-you-get Composition
neurips,2019,0,4653,Hao,Yu,gmail,Alibaba Group (US) Inc,eeyuhao@gmail.com,A Communication Efficient Stochastic Multi-Block Alternating Direction Method of Multipliers
neurips,2019,0,6079,Michal,Derezinski,berkeley,UC Berkeley,mderezin@berkeley.edu,Distributed estimation of the inverse Hessian by determinantal averaging
neurips,2019,1,6079,Michael,Mahoney,berkeley,UC Berkeley,mmahoney@stat.berkeley.edu,Distributed estimation of the inverse Hessian by determinantal averaging
neurips,2019,0,209,Congchao,Wang,vt,Virginia Polytechnic Institute and State University,ccwang@vt.edu,muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking
neurips,2019,1,209,Yizhi,Wang,vt,Virginia Tech,yzwang@vt.edu,muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking
neurips,2019,2,209,Yinxue,Wang,vt,Virginia Tech,yxwang90@vt.edu,muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking
neurips,2019,3,209,Chiung-Ting,Wu,vt,Virginia Tech,ctwu@vt.edu,muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking
neurips,2019,4,209,Guoqiang,Yu,vt,Virginia Tech,yug@vt.edu,muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking
neurips,2019,0,3011,Mahdi,Karami,,University of Alberta,,Invertible Convolutional Flow
neurips,2019,1,3011,Dale,Schuurmans,,Google,,Invertible Convolutional Flow
neurips,2019,2,3011,Jascha,Sohl-Dickstein,,Google Brain,,Invertible Convolutional Flow
neurips,2019,3,3011,Laurent,Dinh,,Google Brain,,Invertible Convolutional Flow
neurips,2019,4,3011,Daniel,Duckworth,,Google Brain,,Invertible Convolutional Flow
neurips,2019,0,1191,Matan,Atzmon,,Weizmann Institute Of Science,,Controlling Neural Level Sets
neurips,2019,1,1191,Niv,Haim,,Weizmann Institute of Science,,Controlling Neural Level Sets
neurips,2019,2,1191,Lior,Yariv,,Weizmann Institute of Science,,Controlling Neural Level Sets
neurips,2019,3,1191,Ofer,Israelov,,Weizmann Institute of Science,,Controlling Neural Level Sets
neurips,2019,4,1191,Haggai,Maron,,NVIDIA Research,,Controlling Neural Level Sets
neurips,2019,5,1191,Yaron,Lipman,,Weizmann Institute of Science,,Controlling Neural Level Sets
neurips,2019,0,3099,Ben,Adlam,google,Google,adlam@google.com,Learning GANs and Ensembles Using Discrepancy
neurips,2019,1,3099,Corinna,Cortes,google,Google Research,mohri@google.com,Learning GANs and Ensembles Using Discrepancy
neurips,2019,2,3099,Mehryar,Mohri,google,Courant Inst. of Math. Sciences & Google Research,corinna@google.com,Learning GANs and Ensembles Using Discrepancy
neurips,2019,3,3099,Ningshan,Zhang,nyu,New York University,nzhang@stern.nyu.edu,Learning GANs and Ensembles Using Discrepancy
neurips,2019,0,6328,Ferran,Alet,mit,MIT,alet@mit.edu,Neural Relational Inference with Fast Modular Meta-learning
neurips,2019,1,6328,Erica,Weng,mit,MIT,ericaw@mit.edu,Neural Relational Inference with Fast Modular Meta-learning
neurips,2019,2,6328,Tomás,Lozano-Pérez,mit,MIT,tlp@mit.edu,Neural Relational Inference with Fast Modular Meta-learning
neurips,2019,3,6328,Leslie,Kaelbling,mit,MIT,lpk@mit.edu,Neural Relational Inference with Fast Modular Meta-learning
neurips,2019,0,6138,Amin,Jaber,purdue,Purdue University,jaber0@purdue.edu,Identification of Conditional Causal Effects under Markov Equivalence
neurips,2019,1,6138,Jiji,Zhang,ln,Lingnan University,jijizhang@ln.edu.hk,Identification of Conditional Causal Effects under Markov Equivalence
neurips,2019,2,6138,Elias,Bareinboim,columbia,Purdue,eb@cs.columbia.edu,Identification of Conditional Causal Effects under Markov Equivalence
neurips,2019,0,306,Xihui,Liu,cuhk,The Chinese University of Hong Kong,xihuiliu@ee.cuhk.edu.hk,Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis
neurips,2019,1,306,Guojun,Yin,sensetime,University of Science and Technology of China,shaojing@sensetime.com,Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis
neurips,2019,2,306,Jing,Shao,gmail,Sensetime,gjyin91@gmail.com,Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis
neurips,2019,3,306,Xiaogang,Wang,cuhk,The Chinese University of Hong Kong,xgwang@ee.cuhk.edu.hk,Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis
neurips,2019,4,306,hongsheng,Li,cuhk,cuhk,hsli@ee.cuhk.edu.hk,Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis
neurips,2019,0,5348,Zhao,Song,gmail,University of Washington,magic.linuxkde@gmail.com,Average Case Column Subset Selection for Entrywise $\ell_1$-Norm Loss
neurips,2019,1,5348,David,Woodruff,cmu,Carnegie Mellon University,dwoodruf@cs.cmu.edu,Average Case Column Subset Selection for Entrywise $\ell_1$-Norm Loss
neurips,2019,2,5348,Peilin,Zhong,columbia,Columbia University,pz2225@columbia.edu,Average Case Column Subset Selection for Entrywise $\ell_1$-Norm Loss
neurips,2019,0,7101,Tristan,Milne,toronto,University of Toronto,tmilne@math.toronto.edu,Piecewise Strong Convexity of Neural Networks
neurips,2019,0,344,Max,Vladymyrov,google,Google Research,mxv@google.com,No Pressure! Addressing the Problem of Local Minima in Manifold Learning Algorithms
neurips,2019,0,1751,Mohammad Emtiyaz,Khan,riken,RIKEN,emtiyaz.khan@riken.jp,Approximate Inference Turns Deep Networks into Gaussian Processes
neurips,2019,1,1751,Alexander,Immer,epfl,"EPFL, RIKEN",ehsan.abedi@epfl.ch,Approximate Inference Turns Deep Networks into Gaussian Processes
neurips,2019,2,1751,Ehsan,Abedi,epfl,EPFL,alexander.immer@epfl.ch,Approximate Inference Turns Deep Networks into Gaussian Processes
neurips,2019,3,1751,Maciej,Korzepa,dtu,Technical University of Denmark,mjko@dtu.dk,Approximate Inference Turns Deep Networks into Gaussian Processes
neurips,2019,0,5380,Matthew,Reimherr,psu,Pennsylvania State University,mreimherr@psu.edu,Elliptical Perturbations for Differential Privacy
neurips,2019,1,5380,Jordan,Awan,psu,Penn State University,awan@psu.edu,Elliptical Perturbations for Differential Privacy
neurips,2019,0,9114,Han,Zhao,cmu,Carnegie Mellon University,han.zhao@cs.cmu.edu,Inherent Tradeoffs in Learning Fair Representations
neurips,2019,1,9114,Geoff,Gordon,microsoft,Microsoft,geoff.gordon@microsoft.com,Inherent Tradeoffs in Learning Fair Representations
neurips,2019,0,1915,Dimitris,Kalimeris,,Harvard,,SGD on Neural Networks Learns Functions of Increasing Complexity
neurips,2019,1,1915,Gal,Kaplun,,Harvard University,,SGD on Neural Networks Learns Functions of Increasing Complexity
neurips,2019,2,1915,Preetum,Nakkiran,,Harvard,,SGD on Neural Networks Learns Functions of Increasing Complexity
neurips,2019,3,1915,Benjamin,Edelman,,Harvard University,,SGD on Neural Networks Learns Functions of Increasing Complexity
neurips,2019,4,1915,Tristan,Yang,,Harvard University,,SGD on Neural Networks Learns Functions of Increasing Complexity
neurips,2019,5,1915,Boaz,Barak,,Harvard University,,SGD on Neural Networks Learns Functions of Increasing Complexity
neurips,2019,6,1915,Haofeng,Zhang,,Harvard University,,SGD on Neural Networks Learns Functions of Increasing Complexity
neurips,2019,0,4941,Mingrui,Zhang,yale,Yale University,mingrui.zhang@yale.edu,Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback
neurips,2019,1,4941,Lin,Chen,yale,Yale University,lin.chen@yale.edu,Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback
neurips,2019,2,4941,Hamed,Hassani,yale,UPenn,amin.karbasi@yale.edu,Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback
neurips,2019,3,4941,Amin,Karbasi,upenn,Yale,hassani@seas.upenn.edu,Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback
neurips,2019,0,9324,Viet Anh,Nguyen,epfl,EPFL,viet-anh.nguyen@epfl.ch,Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation
neurips,2019,1,9324,Soroosh,Shafieezadeh Abadeh,epfl,EPFL,soroosh.shafiee@epfl.ch,Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation
neurips,2019,2,9324,Man-Chung,Yue,polyu,The Hong Kong Polytechnic University,manchung.yue@polyu.edu.hk,Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation
neurips,2019,3,9324,Daniel,Kuhn,epfl,EPFL,daniel.kuhn@epfl.ch,Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation
neurips,2019,4,9324,Wolfram,Wiesemann,imperial,Imperial College,ww@imperial.ac.uk,Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation
neurips,2019,0,8427,Sidharth,Gupta,illinois,University of Illinois at Urbana-Champaign,gupta67@illinois.edu,Don't take it lightly: Phasing optical random projections with unknown operators
neurips,2019,1,8427,Remi,Gribonval,inria,INRIA,remi.gribonval@inria.fr,Don't take it lightly: Phasing optical random projections with unknown operators
neurips,2019,2,8427,Laurent,Daudet,lighton,LightOn,laurent@lighton.ai,Don't take it lightly: Phasing optical random projections with unknown operators
neurips,2019,3,8427,Ivan,Dokmani,illinois,University of Basel,dokmanic@illinois.edu,Don't take it lightly: Phasing optical random projections with unknown operators
neurips,2019,0,1058,Scott,Gigante,princeton,Yale University,adamsc@princeton.edu,Visualizing the PHATE of Neural Networks
neurips,2019,1,1058,Adam,Charles,yale,Princeton University,scott.gigante@yale.edu,Visualizing the PHATE of Neural Networks
neurips,2019,2,1058,Smita,Krishnaswamy,yale,Yale University,smita.krishnaswamy@yale.edu,Visualizing the PHATE of Neural Networks
neurips,2019,3,1058,Gal,Mishne,ucsd,UC San Diego,gmishne@ucsd.edu,Visualizing the PHATE of Neural Networks
neurips,2019,0,1267,Zhonghui,You,pku,Peking University,zhonghui@pku.edu.cn,Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks
neurips,2019,1,1267,Kun,Yan,pku,Peking University,kyan2018@pku.edu.cn,Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks
neurips,2019,2,1267,Jinmian,Ye,gmail,SMILE Lab,jinmian.y@gmail.com,Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks
neurips,2019,3,1267,Meng,Ma,pku,Peking University,mameng@pku.edu.cn,Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks
neurips,2019,4,1267,Ping,Wang,pku,Peking University,pwang@pku.edu.cn,Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks
neurips,2019,0,7231,Maria,Jahja,cmu,Carnegie Mellon University,maria@stat.cmu.edu,"Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights"
neurips,2019,1,7231,David,Farrow,cmu,Carnegie Mellon University,roni@cs.cmu.edu,"Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights"
neurips,2019,2,7231,Roni,Rosenfeld,gmail,Carnegie Mellon University,dfarrow0@gmail.com,"Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights"
neurips,2019,3,7231,Ryan,Tibshirani,cmu,Carnegie Mellon University,ryantibs@stat.cmu.edu,"Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights"
neurips,2019,0,2405,Kazuki,Osawa,,Tokyo Institute of Technology,,Practical Deep Learning with Bayesian Principles
neurips,2019,1,2405,Siddharth,Swaroop,,University of Cambridge,,Practical Deep Learning with Bayesian Principles
neurips,2019,2,2405,Mohammad Emtiyaz,Khan,,RIKEN,,Practical Deep Learning with Bayesian Principles
neurips,2019,3,2405,Anirudh,Jain,,"Indian Institute of Technology (ISM), Dhanbad",,Practical Deep Learning with Bayesian Principles
neurips,2019,4,2405,Runa,Eschenhagen,,University of Osnabrueck,,Practical Deep Learning with Bayesian Principles
neurips,2019,5,2405,Richard,Turner,,University of Cambridge,,Practical Deep Learning with Bayesian Principles
neurips,2019,6,2405,Rio,Yokota,,"Tokyo Institute of Technology, AIST- Tokyo Tech Real World Big-Data Computation Open Innovation Laboratory (RWBC- OIL), National Institute of Advanced Industrial Science and Technology (AIST)",,Practical Deep Learning with Bayesian Principles
neurips,2019,0,3199,Yonatan,Geifman,technion,Technion,yonatan.g@cs.technion.ac.il,Deep Active Learning with a Neural Architecture Search
neurips,2019,1,3199,Ran,El-Yaniv,technion,Technion,rani@cs.technion.ac.il,Deep Active Learning with a Neural Architecture Search
neurips,2019,0,1693,KANCHARLA,PARIMALA,iith,"Indian Institute of Technology, Hyderabad",ee15m17p100001@iith.ac.in,Quality Aware Generative Adversarial Networks
neurips,2019,1,1693,Sumohana,Channappayya,iith,Indian Institute of Technology Hyderabad,sumohana@iith.ac.in,Quality Aware Generative Adversarial Networks
neurips,2019,0,1536,Chaoyou,Fu,ia,"Institute of Automation, Chinese Academy of Sciences",chaoyou.fu@nlpr.ia.ac.cn,Dual Variational Generation for Low Shot Heterogeneous Face Recognition
neurips,2019,1,1536,Xiang,Wu,ia,"Institue of Automation, Chinese Academy of Science",rhe@nlpr.ia.ac.cn,Dual Variational Generation for Low Shot Heterogeneous Face Recognition
neurips,2019,2,1536,Yibo,Hu,gmail,"Institute of Automation, Chinese Academy of Sciences",alfredxiangwu@gmail.com,Dual Variational Generation for Low Shot Heterogeneous Face Recognition
neurips,2019,3,1536,Huaibo,Huang,ia,"Institute of Automation, Chinese Academy of Science",yibo.hu@cripac.ia.ac.cn,Dual Variational Generation for Low Shot Heterogeneous Face Recognition
neurips,2019,4,1536,Ran,He,ia,"NLPR, CASIA",huaibo.huang@cripac.ia.ac.cn,Dual Variational Generation for Low Shot Heterogeneous Face Recognition
neurips,2019,0,2909,Alexander,Irpan,google,Google Brain,alexirpan@google.com,Off-Policy Evaluation via Off-Policy Classification
neurips,2019,1,2909,Kanishka,Rao,google,Google,kanishkarao@google.com,Off-Policy Evaluation via Off-Policy Classification
neurips,2019,2,2909,Konstantinos,Bousmalis,google,DeepMind,konstantinos@google.com,Off-Policy Evaluation via Off-Policy Classification
neurips,2019,3,2909,Chris,Harris,google,Google,ckharris@google.com,Off-Policy Evaluation via Off-Policy Classification
neurips,2019,4,2909,Julian,Ibarz,google,Google Inc.,julianibarz@google.com,Off-Policy Evaluation via Off-Policy Classification
neurips,2019,5,2909,Sergey,Levine,google,Google,slevine@google.com,Off-Policy Evaluation via Off-Policy Classification
neurips,2019,0,6171,Taesup,Kim,,Mila / Kakao Brain,,Variational Temporal Abstraction
neurips,2019,1,6171,Sungjin,Ahn,,Rutgers University,,Variational Temporal Abstraction
neurips,2019,2,6171,Yoshua,Bengio,,Mila - University of Montreal,,Variational Temporal Abstraction
neurips,2019,0,667,Vincent,Sitzmann,stanford,Stanford University,sitzmann@cs.stanford.edu,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations
neurips,2019,1,667,Michael,Zollhoefer,stanford,Facebook Reality Labs,zollhoefer@cs.stanford.edu,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations
neurips,2019,2,667,Gordon,Wetzstein,stanford,Stanford University,gordon.wetzstein@stanford.edu,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations
neurips,2019,0,6819,Sebastian,Blaes,mpg,"Max-Planck Institute for Intelligent Systems, Tuebingen, Germany",sebastian.blaes@tue.mpg.de,Control What You Can: Intrinsically Motivated Task-Planning Agent
neurips,2019,1,6819,Marin,Vlastelica Pogani,mpg,Max Planck Institute for Intelligent Systems,marin.vlastelica@tue.mpg.de,Control What You Can: Intrinsically Motivated Task-Planning Agent
neurips,2019,2,6819,Jiajie,Zhu,mpg,Max Planck Institute for Intelligent Systems,jzhu@tue.mpg.de,Control What You Can: Intrinsically Motivated Task-Planning Agent
neurips,2019,3,6819,Georg,Martius,mpg,MPI for Intelligent Systems,georg.martius@tue.mpg.de,Control What You Can: Intrinsically Motivated Task-Planning Agent
neurips,2019,0,8746,Ashok,Cutkosky,,Google Research,,Momentum-Based Variance Reduction in Non-Convex SGD
neurips,2019,1,8746,Francesco,Orabona,,Boston University,,Momentum-Based Variance Reduction in Non-Convex SGD
neurips,2019,0,327,Dina,Bashkirova,bu,Boston University,dbash@bu.edu,Adversarial Self-Defense for Cycle-Consistent GANs
neurips,2019,1,327,Ben,Usman,bu,Boston University,usmn@bu.edu,Adversarial Self-Defense for Cycle-Consistent GANs
neurips,2019,2,327,Kate,Saenko,bu,Boston University,saenko@bu.edu,Adversarial Self-Defense for Cycle-Consistent GANs
neurips,2019,0,1783,Giovanni,Chierchia,esiee,ESIEE Paris,giovanni.chierchia@esiee.fr,Ultrametric Fitting by Gradient Descent
neurips,2019,1,1783,Benjamin,Perret,esiee,ESIEE/PARIS,benjamin.perret@esiee.fr,Ultrametric Fitting by Gradient Descent
neurips,2019,0,845,Ivan,Glasser,,Max Planck Institute of Quantum Optics,,Expressive power of tensor-network factorizations for probabilistic modeling
neurips,2019,1,845,Ryan,Sweke,,Freie Universitaet Berlin,,Expressive power of tensor-network factorizations for probabilistic modeling
neurips,2019,2,845,Nicola,Pancotti,,Max Planck Institute of Quantum Optics,,Expressive power of tensor-network factorizations for probabilistic modeling
neurips,2019,3,845,Jens,Eisert,,Freie Universitaet Berlin,,Expressive power of tensor-network factorizations for probabilistic modeling
neurips,2019,4,845,Ignacio,Cirac,,Max-Planck Institute of Quantum Optics,,Expressive power of tensor-network factorizations for probabilistic modeling
neurips,2019,0,4791,Siyuan,Huang,ucla,"University of California, Los Angeles",huangsiyuan@ucla.edu,PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points
neurips,2019,1,4791,Yixin,Chen,ucla,UCLA,ethanchen@ucla.edu,PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points
neurips,2019,2,4791,Tao,Yuan,ucla,UCLA,taoyuan@ucla.edu,PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points
neurips,2019,3,4791,Siyuan,Qi,ucla,UCLA,syqi@cs.ucla.edu,PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points
neurips,2019,4,4791,Yixin,Zhu,ucla,"University of California, Los Angeles",yixin.zhu@ucla.edu,PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points
neurips,2019,5,4791,Song-Chun,Zhu,ucla,UCLA,sczhu@stat.ucla.edu,PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points
neurips,2019,0,6137,Nikhil,Ghosh,berkeley,Caltech,nikhil_ghosh@berkeley.edu,Landmark Ordinal Embedding
neurips,2019,1,6137,Yuxin,Chen,uchicago,UChicago,chenyuxin@uchicago.edu,Landmark Ordinal Embedding
neurips,2019,2,6137,Yisong,Yue,caltech,Caltech,yyue@caltech.edu,Landmark Ordinal Embedding
neurips,2019,0,5227,Steve,Hanneke,gmail,Toyota Technological Institute at Chicago,steve.hanneke@gmail.com,On the Value of Target Data in Transfer Learning
neurips,2019,1,5227,Samory,Kpotufe,columbia,Columbia University,skk2175@columbia.edu,On the Value of Target Data in Transfer Learning
neurips,2019,0,5998,Tomi,Peltola,aalto,Aalto University,tomi.peltola@aalto.fi,Machine Teaching of Active Sequential Learners
neurips,2019,1,5998,Mustafa Mert,Çelikok,aalto,Aalto University,mustafa.celikok@aalto.fi,Machine Teaching of Active Sequential Learners
neurips,2019,2,5998,Pedram,Daee,aalto,Aalto University,pedram.daee@aalto.fi,Machine Teaching of Active Sequential Learners
neurips,2019,3,5998,Samuel,Kaski,aalto,Aalto University,samuel.kaski@aalto.fi,Machine Teaching of Active Sequential Learners
neurips,2019,0,3818,Marek,Petrik,unh,University of New Hampshire,rrussel@cs.unh.edu,Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs
neurips,2019,1,3818,Reazul Hasan,Russel,unh,University of New Hampshire,mpetrik@cs.unh.edu,Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs
neurips,2019,0,4900,Taco,Cohen,qualcomm,Qualcomm AI Research,tacos@qti.qualcomm.com,A General Theory of Equivariant CNNs on Homogeneous Spaces
neurips,2019,1,4900,Mario,Geiger,epfl,EPFL,mario.geiger@epfl.ch,A General Theory of Equivariant CNNs on Homogeneous Spaces
neurips,2019,2,4900,Maurice,Weiler,uva,University of Amsterdam,m.weiler@uva.nl,A General Theory of Equivariant CNNs on Homogeneous Spaces
neurips,2019,0,5336,Yujiao,Shi,,Australian National University,,Spatial-Aware Feature Aggregation for Image based Cross-View Geo-Localization
neurips,2019,1,5336,Liu,Liu,,ANU,,Spatial-Aware Feature Aggregation for Image based Cross-View Geo-Localization
neurips,2019,2,5336,Xin,Yu,,Australian National University,,Spatial-Aware Feature Aggregation for Image based Cross-View Geo-Localization
neurips,2019,3,5336,Hongdong,Li,,Australian National University,,Spatial-Aware Feature Aggregation for Image based Cross-View Geo-Localization
neurips,2019,0,6944,Evgenii,Chzhen,u-psud,Université Paris-Est,evgenii.chzhen@math.u-psud.fr,Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification
neurips,2019,1,6944,Christophe,Denis,u-pem,Universite Paris Est,mohamed.hebiri@u-pem.fr,Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification
neurips,2019,2,6944,Mohamed,Hebiri,u-pem,Université Paris-Est--MLV,christophe.denis@u-pem.fr,Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification
neurips,2019,3,6944,Luca,Oneto,unipi,University of Genoa,luca.oneto@unipi.it,Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification
neurips,2019,4,6944,Massimiliano,Pontil,iit,IIT,massimiliano.pontil@iit.it,Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification
neurips,2019,0,5042,Michela,Meister,gmail,Cornell University,meister.michela@gmail.com,Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels
neurips,2019,1,5042,Tamas,Sarlos,google,Google Research,stamas@google.com,Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels
neurips,2019,2,5042,David,Woodruff,cmu,Carnegie Mellon University,dwoodruf@cs.cmu.edu,Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels
neurips,2019,0,7112,Alessandro,Barp,,Imperial College London,,Minimum Stein Discrepancy Estimators
neurips,2019,1,7112,Francois-Xavier,Briol,,University of Cambridge,,Minimum Stein Discrepancy Estimators
neurips,2019,2,7112,Andrew,Duncan,,Imperial College London,,Minimum Stein Discrepancy Estimators
neurips,2019,3,7112,Mark,Girolami,,University of Cambridge,,Minimum Stein Discrepancy Estimators
neurips,2019,4,7112,Lester,Mackey,,Microsoft Research,,Minimum Stein Discrepancy Estimators
neurips,2019,0,1275,Haggai,Maron,,NVIDIA Research,,Provably Powerful Graph Networks
neurips,2019,1,1275,Heli,Ben-Hamu,,Weizmann Institute of Science,,Provably Powerful Graph Networks
neurips,2019,2,1275,Hadar,Serviansky,,Weizmann Institute of Science,,Provably Powerful Graph Networks
neurips,2019,3,1275,Yaron,Lipman,,Weizmann Institute of Science,,Provably Powerful Graph Networks
neurips,2019,0,5403,Wenjie,Shi,tsinghua,Tsinghua University,shiwj16@mails.tsinghua.edu.cn,Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning
neurips,2019,1,5403,Shiji,Song,tsinghua,"Department of Automation, Tsinghua University",wuhui14@mails.tsinghua.edu.cn,Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning
neurips,2019,2,5403,Hui,Wu,tsinghua,Tsinghua University,xuyz17@mails.tsinghua.edu.cn,Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning
neurips,2019,3,5403,Ya-Chu,Hsu,tsinghua,Tsinghua University,shijis@tsinghua.edu.cn,Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning
neurips,2019,4,5403,Cheng,Wu,tsinghua,Tsinghua,wuc@tsinghua.edu.cn,Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning
neurips,2019,5,5403,Gao,Huang,tsinghua,Tsinghua,gaohuang@tsinghua.edu.cn,Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning
neurips,2019,0,1324,Jen Ning,Lim,mpg,Max Planck Institute for Intelligent Systems,jlim@tuebingen.mpg.de,Kernel Stein Tests for Multiple Model Comparison
neurips,2019,1,1324,Makoto,Yamada,riken,Kyoto University / RIKEN AIP,makoto.yamada@riken.jp,Kernel Stein Tests for Multiple Model Comparison
neurips,2019,2,1324,Bernhard,Schölkopf,mpg,MPI for Intelligent Systems,bs@tuebingen.mpg.de,Kernel Stein Tests for Multiple Model Comparison
neurips,2019,3,1324,Wittawat,Jitkrittum,mpg,Max Planck Institute for Intelligent Systems,wittawat@tuebingen.mpg.de,Kernel Stein Tests for Multiple Model Comparison
neurips,2019,0,7516,Ann-Kathrin,Dombrowski,,TU Berlin,,Explanations can be manipulated and geometry is to blame
neurips,2019,1,7516,Maximillian,Alber,,TU Berlin,,Explanations can be manipulated and geometry is to blame
neurips,2019,2,7516,Christopher,Anders,,Technische Universität Berlin,,Explanations can be manipulated and geometry is to blame
neurips,2019,3,7516,Marcel,Ackermann,,HHI,,Explanations can be manipulated and geometry is to blame
neurips,2019,4,7516,Klaus-Robert,Müller,,TU Berlin,,Explanations can be manipulated and geometry is to blame
neurips,2019,5,7516,Pan,Kessel,,TU Berlin,,Explanations can be manipulated and geometry is to blame
neurips,2019,0,5769,Aya Abdelsalam,Ismail,umd,University of Maryland,asalam@cs.umd.edu,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks
neurips,2019,1,5769,Mohamed,Gunady,umd,University of Maryland,mgunady@cs.umd.edu,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks
neurips,2019,2,5769,Luiz,Pessoa,umd,University of Maryland,pessoa@umd.edu,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks
neurips,2019,3,5769,Hector,Corrada Bravo,umd,University of Maryland,hcorrada@umiacs.umd.edu,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks
neurips,2019,4,5769,Soheil,Feizi,umd,University of Maryland,sfeizi@cs.umd.edu,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks
neurips,2019,0,4519,Paul,Goelz,cmu,Carnegie Mellon University,pgoelz@cs.cmu.edu,Paradoxes in Fair Machine Learning
neurips,2019,1,4519,Anson,Kahng,cmu,Carnegie Mellon University,akahng@cs.cmu.edu,Paradoxes in Fair Machine Learning
neurips,2019,2,4519,Ariel,Procaccia,cmu,Carnegie Mellon University,arielpro@cs.cmu.edu,Paradoxes in Fair Machine Learning
neurips,2019,0,405,Adrian,Dalca,mit,"MIT, HMS",adalca@mit.edu,Learning Conditional Deformable Templates with Convolutional Networks
neurips,2019,1,405,Marianne,Rakic,mit,MIT/ETH Zürich,mrakic@mit.edu,Learning Conditional Deformable Templates with Convolutional Networks
neurips,2019,2,405,John,Guttag,mit,Massachusetts Institute of Technology,guttag@mit.edu,Learning Conditional Deformable Templates with Convolutional Networks
neurips,2019,3,405,Mert,Sabuncu,cornell,Cornell,msabuncu@cornell.edu,Learning Conditional Deformable Templates with Convolutional Networks
neurips,2019,0,403,Gengshan,Yang,cmu,Carnegie Mellon University,gengshay@cs.cmu.edu,Volumetric Correspondence Networks for Optical Flow
neurips,2019,1,403,Deva,Ramanan,cmu,Carnegie Mellon University,deva@cs.cmu.edu,Volumetric Correspondence Networks for Optical Flow
neurips,2019,0,7294,Jean,Pouget-Abadie,,Google,,Variance Reduction in Bipartite Experiments through Correlation Clustering
neurips,2019,1,7294,Kevin,Aydin,,Google,,Variance Reduction in Bipartite Experiments through Correlation Clustering
neurips,2019,2,7294,Warren,Schudy,,Google,,Variance Reduction in Bipartite Experiments through Correlation Clustering
neurips,2019,3,7294,Kay,Brodersen,,Google,,Variance Reduction in Bipartite Experiments through Correlation Clustering
neurips,2019,4,7294,Vahab,Mirrokni,,Google Research NYC,,Variance Reduction in Bipartite Experiments through Correlation Clustering
neurips,2019,0,6347,Susmit,Jha,,SRI,,Attribution-Based Confidence Metric For Deep Neural Networks
neurips,2019,1,6347,Sunny,Raj,,University of Central Florida,,Attribution-Based Confidence Metric For Deep Neural Networks
neurips,2019,2,6347,Steven,Fernandes,,University of Central Florida,,Attribution-Based Confidence Metric For Deep Neural Networks
neurips,2019,3,6347,Sumit,Jha,,University of Central Florida,,Attribution-Based Confidence Metric For Deep Neural Networks
neurips,2019,4,6347,Somesh,Jha,,"University of Wisconsin, Madison",,Attribution-Based Confidence Metric For Deep Neural Networks
neurips,2019,5,6347,Brian,Jalaian,,U.S. Army Research Laboratory,,Attribution-Based Confidence Metric For Deep Neural Networks
neurips,2019,6,6347,Gunjan,Verma,,U.S. Army Research Laboratory,,Attribution-Based Confidence Metric For Deep Neural Networks
neurips,2019,7,6347,Ananthram,Swami,,"Army Research Laboratory, Adelphi",,Attribution-Based Confidence Metric For Deep Neural Networks
neurips,2019,0,8001,Sjoerd,van Steenkiste,,The Swiss AI Lab - IDSIA,,Are Disentangled Representations Helpful for Abstract Visual Reasoning?
neurips,2019,1,8001,Francesco,Locatello,,ETH Zürich - MPI Tübingen,,Are Disentangled Representations Helpful for Abstract Visual Reasoning?
neurips,2019,2,8001,Jürgen,Schmidhuber,,"Swiss AI Lab, IDSIA (USI & SUPSI) - NNAISENSE",,Are Disentangled Representations Helpful for Abstract Visual Reasoning?
neurips,2019,3,8001,Olivier,Bachem,,Google Brain,,Are Disentangled Representations Helpful for Abstract Visual Reasoning?
neurips,2019,0,323,Robert,Gower,gmail,"Institut Polytechnique de Paris, Telecom Paris",gowerrobert@gmail.com,RSN: Randomized Subspace Newton
neurips,2019,1,323,Dmitry,Koralev,kaust,KAUST,dmitry.kovalev@kaust.edu.sa,RSN: Randomized Subspace Newton
neurips,2019,2,323,Felix,Lieder,uni-duesseldorf,Heinrich-Heine-Universität Düsseldorf,lieder@opt.uni-duesseldorf.de,RSN: Randomized Subspace Newton
neurips,2019,3,323,Peter,Richtarik,kaust,KAUST,peter.richtarik@kaust.edu.sa,RSN: Randomized Subspace Newton
neurips,2019,0,2399,Mahesh Chandra,Mukkamala,uni-sb,Saarland University,mukkamala@math.uni-sb.de,Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms
neurips,2019,1,2399,Peter,Ochs,uni-sb,Saarland University,ochs@math.uni-sb.de,Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms
neurips,2019,0,1349,Weishi,Shi,rit,Rochester Institute of Technology,ws7586@rit.edu,Integrating Bayesian and Discriminative Sparse Kernel Machines for  Multi-class Active Learning
neurips,2019,1,1349,Qi,Yu,rit,Rochester Institute of Technology,qi.yu@rit.edu,Integrating Bayesian and Discriminative Sparse Kernel Machines for  Multi-class Active Learning
neurips,2019,0,6229,Yuanzhi,Li,,Princeton,,Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks
neurips,2019,1,6229,Colin,Wei,,Stanford University,,Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks
neurips,2019,2,6229,Tengyu,Ma,,Stanford University,,Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks
neurips,2019,0,8657,Firoozeh,Sepehr,utk,University of Tennessee,dawn@utk.edu,An Algorithm to Learn Polytree Networks with Hidden Nodes
neurips,2019,1,8657,Donatello,Materassi,utk,University of Minnesota,dmateras@utk.edu,An Algorithm to Learn Polytree Networks with Hidden Nodes
neurips,2019,0,161,Justin,Domke,umass,"University of Massachusetts, Amherst",domke@cs.umass.edu,Provable Gradient Variance Guarantees for Black-Box Variational Inference
neurips,2019,0,4216,Zuxuan,Wu,,University of Maryland,,LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition
neurips,2019,1,4216,Caiming,Xiong,,Salesforce,,LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition
neurips,2019,2,4216,Yu-Gang,Jiang,,Fudan University,,LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition
neurips,2019,3,4216,Larry,Davis,,University of Maryland,,LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition
neurips,2019,0,1034,Jiezhang,Cao,scut,South China University of Technology,secaojiezhang@mail.scut.edu.cn,Multi-marginal Wasserstein GAN
neurips,2019,1,1034,Langyuan,Mo,scut,South China University of Technology,selymo@mail.scut.edu.cn,Multi-marginal Wasserstein GAN
neurips,2019,2,1034,Yifan,Zhang,scut,South China University of Technology,sezyifan@mail.scut.edu.cn,Multi-marginal Wasserstein GAN
neurips,2019,3,1034,Kui,Jia,scut,South China University of Technology,mingkuitan@scut.edu.cn,Multi-marginal Wasserstein GAN
neurips,2019,4,1034,Chunhua,Shen,scut,University of Adelaide,kuijia@scut.edu.cn,Multi-marginal Wasserstein GAN
neurips,2019,5,1034,Mingkui,Tan,adelaide,South China University of Technology,chunhua.shen@adelaide.edu.au,Multi-marginal Wasserstein GAN
neurips,2019,0,4399,Adam,Paszke,gmail,University of Warsaw,adam.paszke@gmail.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,1,4399,Sam,Gross,fb,Facebook,sgross@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,2,4399,Francisco,Massa,fb,Facebook AI Research,fmassa@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,3,4399,Adam,Lerer,fb,Facebook AI Research,alerer@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,4,4399,James,Bradbury,gmail,Google Research,jekbradbury@gmail.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,5,4399,Gregory,Chanan,fb,Facebook,gchanan@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,6,4399,Trevor,Killeen,washington,Self Employed,killeent@cs.washington.edu,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,7,4399,Zeming,Lin,fb,Facebook AI Research,zlin@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,8,4399,Natalia,Gimelshein,nvidia,NVIDIA,ngimelshein@nvidia.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,9,4399,Luca,Antiga,orobix,Orobix,luca.antiga@orobix.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,10,4399,Alban,Desmaison,ox,Oxford University,alban@robots.ox.ac.uk,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,11,4399,Andreas,Kopf,xamla,Xamla,andreas.koepf@xamla.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,12,4399,Edward,Yang,fb,Facebook,ezyang@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,13,4399,Zachary,DeVito,stanford,Facebook AI Research,zdevito@cs.stanford.edu,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,14,4399,Martin,Raison,gmail,Nabla,martinraison@gmail.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,15,4399,Alykhan,Tejani,twitter,"Twitter, Inc.",atejani@twitter.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,16,4399,Sasank,Chilamkurthy,gmail,Qure.ai,sasankchilamkurthy@gmail.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,17,4399,Benoit,Steiner,fb,Facebook AI Research,benoitsteiner@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,18,4399,Lu,Fang,fb,Facebook,lufang@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,19,4399,Junjie,Bai,fb,Facebook,jbai@fb.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,20,4399,Soumith,Chintala,gmail,Facebook AI Research,soumith@gmail.com,"PyTorch: An Imperative Style, High-Performance Deep Learning Library"
neurips,2019,0,4496,Shichen,Liu,gmail,University of Southern California (SSO),liushichen95@gmail.com,Learning to Infer Implicit Surfaces without 3D Supervision
neurips,2019,1,4496,Shunsuke,Saito,gmail,University of Southern California,shunsuke.saito16@gmail.com,Learning to Infer Implicit Surfaces without 3D Supervision
neurips,2019,2,4496,Weikai,Chen,gmail,USC Institute for Creative Technology,chenwk891@gmail.com,Learning to Infer Implicit Surfaces without 3D Supervision
neurips,2019,3,4496,Hao,Li,hao-li,Pinscreen/University of Southern California/USC ICT,hao@hao-li.com,Learning to Infer Implicit Surfaces without 3D Supervision
neurips,2019,0,5291,Wenbo,Ren,osu,The Ohio State University,ren.453@osu.edu,On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons
neurips,2019,1,5291,Jia (Kevin),Liu,iastate,Iowa State University,jialiu@iastate.edu,On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons
neurips,2019,2,5291,Ness,Shroff,osu,The Ohio State University,shroff.11@osu.edu,On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons
neurips,2019,0,6609,Jean-Baptiste,Alayrac,,Deepmind,,Are Labels Required for Improving Adversarial Robustness?
neurips,2019,1,6609,Jonathan,Uesato,,DeepMind,,Are Labels Required for Improving Adversarial Robustness?
neurips,2019,2,6609,Po-Sen,Huang,,DeepMind,,Are Labels Required for Improving Adversarial Robustness?
neurips,2019,3,6609,Alhussein,Fawzi,,DeepMind,,Are Labels Required for Improving Adversarial Robustness?
neurips,2019,4,6609,Robert,Stanforth,,DeepMind,,Are Labels Required for Improving Adversarial Robustness?
neurips,2019,5,6609,Pushmeet,Kohli,,DeepMind,,Are Labels Required for Improving Adversarial Robustness?
neurips,2019,0,383,Yong,Guo,scut,South China University of Technology,guo.yong@mail.scut.edu.cn,NAT: Neural Architecture Transformer for Accurate and Compact Architectures
neurips,2019,1,383,Yin,Zheng,scut,"WeiXin Group, Tencent",sechenqi@mail.scut.edu.cn,NAT: Neural Architecture Transformer for Accurate and Compact Architectures
neurips,2019,2,383,Mingkui,Tan,scut,South China University of Technology,mingkuitan@scut.edu.cn,NAT: Neural Architecture Transformer for Accurate and Compact Architectures
neurips,2019,3,383,Qi,Chen,scut,South China University of Technology,ellachen@scut.edu.cn,NAT: Neural Architecture Transformer for Accurate and Compact Architectures
neurips,2019,4,383,Jian,Chen,tencent,"""South China University of Technology, China""",yinzheng@tencent.com,NAT: Neural Architecture Transformer for Accurate and Compact Architectures
neurips,2019,5,383,Peilin,Zhao,tencent,Tencent AI Lab,masonzhao@tencent.com,NAT: Neural Architecture Transformer for Accurate and Compact Architectures
neurips,2019,6,383,Junzhou,Huang,uta,University of Texas at Arlington / Tencent AI Lab,jzhuang@uta.edu,NAT: Neural Architecture Transformer for Accurate and Compact Architectures
neurips,2019,0,5433,Xinzhe,Li,,SJTU,,Learning to Self-Train for Semi-Supervised Few-Shot Classification
neurips,2019,1,5433,Qianru,Sun,,Singapore Management University,,Learning to Self-Train for Semi-Supervised Few-Shot Classification
neurips,2019,2,5433,Yaoyao,Liu,,Tianjin University,,Learning to Self-Train for Semi-Supervised Few-Shot Classification
neurips,2019,3,5433,Qin,Zhou,,Alibaba Group,,Learning to Self-Train for Semi-Supervised Few-Shot Classification
neurips,2019,4,5433,Shibao,Zheng,,SJTU,,Learning to Self-Train for Semi-Supervised Few-Shot Classification
neurips,2019,5,5433,Tat-Seng,Chua,,National Univ. of Singapore,,Learning to Self-Train for Semi-Supervised Few-Shot Classification
neurips,2019,6,5433,Bernt,Schiele,,Max Planck Institute for Informatics,,Learning to Self-Train for Semi-Supervised Few-Shot Classification
neurips,2019,0,8016,Francesco,Locatello,ethz,ETH Zürich - MPI Tübingen,francesco.locatello@inf.ethz.ch,Stochastic Frank-Wolfe for Composite Convex Minimization
neurips,2019,1,8016,Alp,Yurtsever,epfl,EPFL,alp.yurtsever@epfl.ch,Stochastic Frank-Wolfe for Composite Convex Minimization
neurips,2019,2,8016,Olivier,Fercoq,epfl,Telecom ParisTech,volkan.cevher@epfl.ch,Stochastic Frank-Wolfe for Composite Convex Minimization
neurips,2019,3,8016,Volkan,Cevher,telecom-paristech,EPFL,olivier.fercoq@telecom-paristech.fr,Stochastic Frank-Wolfe for Composite Convex Minimization
neurips,2019,0,4485,Lingge,Li,uci,UC Irvine,linggel@uci.edu,Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes
neurips,2019,1,4485,Dustin,Pluta,uci,UC Irvine,dpluta@uci.edu,Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes
neurips,2019,2,4485,Babak,Shahbaba,uci,UCI,babaks@uci.edu,Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes
neurips,2019,3,4485,Norbert,Fortin,uci,UC Irvine,norbert.fortin@uci.edu,Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes
neurips,2019,4,4485,Hernando,Ombao,kaust,KAUST,hernando.ombao@kaust.edu.sa,Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes
neurips,2019,5,4485,Pierre,Baldi,uci,UC Irvine,pfbaldi@ics.uci.edu,Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes
neurips,2019,0,332,Chunjin,Song,gmail,Shenzhen University,songchunjin1990@gmail.com,ETNet: Error Transition Network for Arbitrary Style Transfer
neurips,2019,1,332,Zhijie,Wu,gmail,Shenzhen University,wzj.micker@gmail.com,ETNet: Error Transition Network for Arbitrary Style Transfer
neurips,2019,2,332,Yang,Zhou,gmail,Shenzhen University,zhouyangvcc@gmail.com,ETNet: Error Transition Network for Arbitrary Style Transfer
neurips,2019,3,332,Minglun,Gong,uoguelph,Memorial Univ,minglun@uoguelph.ca,ETNet: Error Transition Network for Arbitrary Style Transfer
neurips,2019,4,332,Hui,Huang,gmail,Shenzhen University,hhzhiyan@gmail.com,ETNet: Error Transition Network for Arbitrary Style Transfer
neurips,2019,0,3821,Alexis,CONNEAU,fb,Facebook,aconneau@fb.com,Cross-lingual Language Model Pretraining
neurips,2019,1,3821,Guillaume,Lample,fb,Facebook AI Research,glample@fb.com,Cross-lingual Language Model Pretraining
neurips,2019,0,8401,Wenbo,Gong,,University of Cambridge,,Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model
neurips,2019,1,8401,Sebastian,Tschiatschek,,Microsoft Research,,Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model
neurips,2019,2,8401,Sebastian,Nowozin,,Microsoft Research Cambridge,,Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model
neurips,2019,3,8401,Richard,Turner,,University of Cambridge,,Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model
neurips,2019,4,8401,José Miguel,Hernández-Lobato,,University of Cambridge,,Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model
neurips,2019,5,8401,Cheng,Zhang,,"Microsoft Research, Cambridge, UK",,Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model
neurips,2019,0,3904,Debmalya,Mandal,columbia,Columbia University,dm3557@columbia.edu,Efficient and Thrifty Voting by Any Means Necessary
neurips,2019,1,3904,Ariel,Procaccia,toronto,Carnegie Mellon University,nisarg@cs.toronto.edu,Efficient and Thrifty Voting by Any Means Necessary
neurips,2019,2,3904,Nisarg,Shah,cmu,University of Toronto,arielpro@cs.cmu.edu,Efficient and Thrifty Voting by Any Means Necessary
neurips,2019,3,3904,David,Woodruff,cmu,Carnegie Mellon University,dwoodruf@cs.cmu.edu,Efficient and Thrifty Voting by Any Means Necessary
neurips,2019,0,4363,Ron,Banner,intel,Intel - Artificial Intelligence Products Group (AIPG),ron.banner@intel.com,Post training 4-bit quantization of convolutional networks for rapid-deployment
neurips,2019,1,4363,Yury,Nahshan,intel,Intel - Artificial Intelligence Products Group (AIPG),yury.nahshan@intel.com,Post training 4-bit quantization of convolutional networks for rapid-deployment
neurips,2019,2,4363,Daniel,Soudry,gmail,Technion,daniel.soudry@gmail.com,Post training 4-bit quantization of convolutional networks for rapid-deployment
neurips,2019,0,4033,Sanjeev,Arora,princeton,Princeton University,arora@cs.princeton.edu,Implicit Regularization in Deep Matrix Factorization
neurips,2019,1,4033,Nadav,Cohen,princeton,Tel Aviv University,huwei@cs.princeton.edu,Implicit Regularization in Deep Matrix Factorization
neurips,2019,2,4033,Wei,Hu,tau,Princeton University,cohennadav@cs.tau.ac.il,Implicit Regularization in Deep Matrix Factorization
neurips,2019,3,4033,Yuping,Luo,princeton,Princeton University,yupingl@cs.princeton.edu,Implicit Regularization in Deep Matrix Factorization
neurips,2019,0,4230,Shahana,Ibrahim,oregonstate,Oregon State University,ibrahish@oregonstate.edu,Crowdsourcing via Pairwise Co-occurrences: Identifiability and Algorithms
neurips,2019,1,4230,Xiao,Fu,umn,Oregon State University,kaga005@umn.edu,Crowdsourcing via Pairwise Co-occurrences: Identifiability and Algorithms
neurips,2019,2,4230,Nikolaos,Kargas,oregonstate,University of Minnesota,xiao.fu@oregonstate.edu,Crowdsourcing via Pairwise Co-occurrences: Identifiability and Algorithms
neurips,2019,3,4230,Kejun,Huang,ufl,University of Florida,kejun.huang@ufl.edu,Crowdsourcing via Pairwise Co-occurrences: Identifiability and Algorithms
neurips,2019,0,2572,Yifan,Sun,cmu,Carnegie Mellon University,yifans@andrew.cmu.edu,Learning low-dimensional state embeddings and metastable clusters from time series data
neurips,2019,1,2572,Yaqi,Duan,princeton,Princeton University,hgong@princeton.edu,Learning low-dimensional state embeddings and metastable clusters from time series data
neurips,2019,2,2572,Hao,Gong,princeton,Princeton University,yaqid@princeton.edu,Learning low-dimensional state embeddings and metastable clusters from time series data
neurips,2019,3,2572,Mengdi,Wang,princeton,Princeton University,mengdiw@princeton.edu,Learning low-dimensional state embeddings and metastable clusters from time series data
neurips,2019,0,6136,Daniel,Levy,stanford,Stanford University,danilevy@stanford.edu,Necessary and Sufficient Geometries for Gradient Methods
neurips,2019,1,6136,John,Duchi,stanford,Stanford,jduchi@stanford.edu,Necessary and Sufficient Geometries for Gradient Methods
neurips,2019,0,4884,Behrooz,Ghorbani,stanford,Stanford University,ghorbani@stanford.edu,Limitations of Lazy Training of Two-layers Neural Network
neurips,2019,1,4884,Song,Mei,stanford,Stanford University,songmei@stanford.edu,Limitations of Lazy Training of Two-layers Neural Network
neurips,2019,2,4884,Theodor,Misiakiewicz,stanford,Stanford University,misiakie@stanford.edu,Limitations of Lazy Training of Two-layers Neural Network
neurips,2019,3,4884,Andrea,Montanari,stanford,Stanford,montanar@stanford.edu,Limitations of Lazy Training of Two-layers Neural Network
neurips,2019,0,6183,Jacob,Abernethy,gatech,Georgia Institute of Technology,prof@gatech.edu,Learning Auctions with Robust Incentive Guarantees
neurips,2019,1,6183,Rachel,Cummings,gatech,Georgia Tech,rachelc@gatech.edu,Learning Auctions with Robust Incentive Guarantees
neurips,2019,2,6183,Bhuvesh,Kumar,gatech,Georgia Tech,bhuvesh@gatech.edu,Learning Auctions with Robust Incentive Guarantees
neurips,2019,3,6183,Sam,Taggart,gatech,Oberlin College,jamiemmt.cs@gatech.edu,Learning Auctions with Robust Incentive Guarantees
neurips,2019,4,6183,Jamie,Morgenstern,oberlin,University of Washington,sam.taggart@oberlin.edu,Learning Auctions with Robust Incentive Guarantees
neurips,2019,0,5940,Farzin,Haddadpour,psu,Pennsylvania State university,fxh18@psu.edu,Local SGD with  Periodic Averaging: Tighter Analysis  and Adaptive Synchronization
neurips,2019,1,5940,Mohammad Mahdi,Kamani,psu,Pennsylvania State University,mqk5591@psu.edu,Local SGD with  Periodic Averaging: Tighter Analysis  and Adaptive Synchronization
neurips,2019,2,5940,Mehrdad,Mahdavi,psu,Pennsylvania State University,mzm616@psu.edu,Local SGD with  Periodic Averaging: Tighter Analysis  and Adaptive Synchronization
neurips,2019,3,5940,Viveck,Cadambe,psu,Penn State,vxc12@psu.edu,Local SGD with  Periodic Averaging: Tighter Analysis  and Adaptive Synchronization
neurips,2019,0,5367,Ruoxi,Sun,,Columbia University,,Scalable Bayesian inference of dendritic voltage via spatiotemporal recurrent state space models
neurips,2019,1,5367,Scott,Linderman,,Columbia University,,Scalable Bayesian inference of dendritic voltage via spatiotemporal recurrent state space models
neurips,2019,2,5367,Ian,Kinsella,,Columbia University,,Scalable Bayesian inference of dendritic voltage via spatiotemporal recurrent state space models
neurips,2019,3,5367,Liam,Paninski,,Columbia University,,Scalable Bayesian inference of dendritic voltage via spatiotemporal recurrent state space models
neurips,2019,0,4117,Santiago,Paternain,upenn,University of Pennsylvania,spater@seas.upenn.edu,Constrained Reinforcement Learning Has Zero Duality Gap
neurips,2019,1,4117,Luiz,Chamon,upenn,University of Pennsylvania,luizf@seas.upenn.edu,Constrained Reinforcement Learning Has Zero Duality Gap
neurips,2019,2,4117,Miguel,Calvo-Fullana,upenn,University of Pennsylvania,cfullana@seas.upenn.edu,Constrained Reinforcement Learning Has Zero Duality Gap
neurips,2019,3,4117,Alejandro,Ribeiro,upenn,University of Pennsylvania,aribeiro@seas.upenn.edu,Constrained Reinforcement Learning Has Zero Duality Gap
neurips,2019,0,3052,Francisco,Garcia,umass,University of Massachusetts - Amherst,fmgarcia@cs.umass.edu,A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning
neurips,2019,1,3052,Philip,Thomas,umass,University of Massachusetts Amherst,pthomas@cs.umass.edu,A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning
neurips,2019,0,6282,Aviral,Kumar,,UC Berkeley,,Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction
neurips,2019,1,6282,Justin,Fu,,UC Berkeley,,Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction
neurips,2019,2,6282,Matthew,Soh,,UC Berkeley,,Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction
neurips,2019,3,6282,George,Tucker,,Google Brain,,Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction
neurips,2019,4,6282,Sergey,Levine,,UC Berkeley,,Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction
neurips,2019,0,3180,Drew,Hudson,,Stanford,,Learning by Abstraction: The Neural State Machine
neurips,2019,1,3180,Christopher,Manning,,Stanford University,,Learning by Abstraction: The Neural State Machine
neurips,2019,0,7150,Li,Dong,microsoft,Microsoft Research,lidong1@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,1,7150,Nan,Yang,microsoft,Microsoft Research Asia,nanya@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,2,7150,Wenhui,Wang,microsoft,Microsoft Research,wenwan@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,3,7150,Furu,Wei,microsoft,Microsoft Research Asia,fuwei@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,4,7150,Xiaodong,Liu,microsoft,Microsoft,xiaodl@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,5,7150,Yu,Wang,microsoft,Microsoft Research,yuwan@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,6,7150,Jianfeng,Gao,microsoft,"Microsoft Research, Redmond, WA",jfgao@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,7,7150,Ming,Zhou,microsoft,Microsoft Research,mingzhou@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,8,7150,Hsiao-Wuen,Hon,microsoft,Microsoft Research,hon@microsoft.com,Unified Language Model Pre-training for Natural Language Understanding and Generation
neurips,2019,0,1963,Lingyu,Liang,gmail,South China University of Technology,lianglysky@gmail.com,Adaptive GNN for Image Analysis and Editing
neurips,2019,1,1963,LianWen,Jin,gmail,South China University of Technology,lianwen.jin@gmail.com,Adaptive GNN for Image Analysis and Editing
neurips,2019,2,1963,Yong,Xu,scut,South China University of Technology,yxu@scut.edu.cn,Adaptive GNN for Image Analysis and Editing
neurips,2019,0,253,Chengzhi,Mao,columbia,Columbia University,cm3797@columbia.edu,Metric Learning for Adversarial Robustness
neurips,2019,1,253,Ziyuan,Zhong,columbia,Columbia University,ziyuan.zhong@columbia.edu,Metric Learning for Adversarial Robustness
neurips,2019,2,253,Junfeng,Yang,columbia,Columbia University,junfeng@cs.columbia.edu,Metric Learning for Adversarial Robustness
neurips,2019,3,253,Carl,Vondrick,columbia,Columbia University,vondrick@cs.columbia.edu,Metric Learning for Adversarial Robustness
neurips,2019,4,253,Baishakhi,Ray,columbia,Columbia University,rayb@cs.columbia.edu,Metric Learning for Adversarial Robustness
neurips,2019,0,828,Mete,Ozay,,Independent Researcher (N/A),,Fine-grained Optimization of Deep Neural Networks
neurips,2019,0,1357,Deepak,Pathak,,"UC Berkeley, FAIR, CMU",,Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity
neurips,2019,1,1357,Christopher,Lu,,UC Berkeley and Covariant.ai,,Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity
neurips,2019,2,1357,Trevor,Darrell,,UC Berkeley,,Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity
neurips,2019,3,1357,Phillip,Isola,,Massachusetts Institute of Technology,,Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity
neurips,2019,4,1357,Alexei,Efros,,UC Berkeley,,Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity
neurips,2019,0,4578,Kimon,Antonakopoulos,inria,Inria,kimon.antonakopoulos@inria.fr,An adaptive Mirror-Prox method for variational inequalities with singular operators
neurips,2019,1,4578,Veronica,Belmega,ensea,ENSEA,belmega@ensea.fr,An adaptive Mirror-Prox method for variational inequalities with singular operators
neurips,2019,2,4578,Panayotis,Mertikopoulos,imag,CNRS (French National Center for Scientific Research),panayotis.mertikopoulos@imag.fr,An adaptive Mirror-Prox method for variational inequalities with singular operators
neurips,2019,0,7574,Pierre,Monteiller,ens,ENS Ulm,pierre.monteiller@ens.fr,Alleviating Label Switching with Optimal Transport
neurips,2019,1,7574,Sebastian,Claici,mit,MIT,sclaici@mit.edu,Alleviating Label Switching with Optimal Transport
neurips,2019,2,7574,Edward,Chien,mit,Massachusetts Institute of Technology,edchien@mit.edu,Alleviating Label Switching with Optimal Transport
neurips,2019,3,7574,Farzaneh,Mirzazadeh,ibm,"MIT-IBM Watson AI Lab, IBM Research",farzaneh@ibm.com,Alleviating Label Switching with Optimal Transport
neurips,2019,4,7574,Justin,Solomon,mit,MIT,jsolomon@mit.edu,Alleviating Label Switching with Optimal Transport
neurips,2019,5,7574,Mikhail,Yurochkin,ibm,"IBM Research, MIT-IBM Watson AI Lab",mikhail.yurochkin@ibm.com,Alleviating Label Switching with Optimal Transport
neurips,2019,0,4738,Song,Liu,bristol,University of Bristol,song.liu@bristol.ac.uk,Fisher Efficient Inference of Intractable Models
neurips,2019,1,4738,Takafumi,Kanamori,titech,Tokyo Institute of Technology/RIKEN,kanamori@c.titech.ac.jp,Fisher Efficient Inference of Intractable Models
neurips,2019,2,4738,Wittawat,Jitkrittum,mpg,Max Planck Institute for Intelligent Systems,wittawat@tuebingen.mpg.de,Fisher Efficient Inference of Intractable Models
neurips,2019,3,4738,Yu,Chen,bristol,University of Bristol,yc14600@bristol.ac.uk,Fisher Efficient Inference of Intractable Models
neurips,2019,0,2110,Difan,Zou,ucla,"University of California, Los Angeles",knowzou@cs.ucla.edu,Stochastic Gradient Hamiltonian Monte Carlo Methods with Recursive Variance Reduction
neurips,2019,1,2110,Pan,Xu,ucla,"University of California, Los Angeles",panxu@cs.ucla.edu,Stochastic Gradient Hamiltonian Monte Carlo Methods with Recursive Variance Reduction
neurips,2019,2,2110,Quanquan,Gu,ucla,UCLA,qgu@cs.ucla.edu,Stochastic Gradient Hamiltonian Monte Carlo Methods with Recursive Variance Reduction
neurips,2019,0,4777,Jacob,Abernethy,gatech,Georgia Institute of Technology,prof@gatech.edu,Online Learning via the Differential Privacy Lens
neurips,2019,1,4777,Young Hun,Jung,umich,University of Michigan,yhjung@umich.edu,Online Learning via the Differential Privacy Lens
neurips,2019,2,4777,Chansoo,Lee,google,Google,chansoo@google.com,Online Learning via the Differential Privacy Lens
neurips,2019,3,4777,Audra,McMillan,gmail,Northeastern/Boston University,audramarymcmillan@gmail.com,Online Learning via the Differential Privacy Lens
neurips,2019,4,4777,Ambuj,Tewari,umich,University of Michigan,tewaria@umich.edu,Online Learning via the Differential Privacy Lens
neurips,2019,0,8128,Murat,Kocaoglu,ibm,MIT-IBM Watson AI Lab,murat@ibm.com,Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions
neurips,2019,1,8128,Amin,Jaber,purdue,Purdue University,jaber0@purdue.edu,Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions
neurips,2019,2,8128,Karthikeyan,Shanmugam,ibm,"IBM Research, NY",karthikeyan.shanmugam2@ibm.com,Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions
neurips,2019,3,8128,Elias,Bareinboim,columbia,Purdue,eb@cs.columbia.edu,Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions
neurips,2019,0,2164,Aleksis,Pirinen,,Lund University,,Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction
neurips,2019,1,2164,Erik,Gärtner,,Lund University,,Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction
neurips,2019,2,2164,Cristian,Sminchisescu,,Google Research,,Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction
neurips,2019,0,6497,Etienne,Boursier,ens-paris-saclay,ENS Paris Saclay,etienne.boursier@ens-paris-saclay.fr,SIC-MMAB: Synchronisation Involves Communication in Multiplayer Multi-Armed Bandits
neurips,2019,1,6497,Vianney,Perchet,normalesup,ENSAE & Criteo AI Lab,vianney.perchet@normalesup.org,SIC-MMAB: Synchronisation Involves Communication in Multiplayer Multi-Armed Bandits
neurips,2019,0,2932,Edward,Raff,,Booz Allen Hamilton,,A Step Toward Quantifying Independently Reproducible Machine Learning Research
neurips,2019,0,4700,Ernesto,Araya Valdivia,u-psud,Université Paris-Sud,ernesto.araya-valdivia@u-psud.fr,Latent distance estimation for random geometric graphs
neurips,2019,1,4700,De Castro,Yohann,ec-lyon,École centrale de Lyon,yohann.de-castro@ec-lyon.fr,Latent distance estimation for random geometric graphs
neurips,2019,0,3314,Jian,Ni,ustc,University of Science and Technology of China,nj1@mail.ustc.edu.cn,Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning
neurips,2019,1,3314,Shanghang,Zhang,cmu,Carnegie Mellon University,shanghaz@andrew.cmu.edu,Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning
neurips,2019,2,3314,Haiyong,Xie,ieee,University of Science and Technology of China,haiyong.xie@ieee.org,Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning
neurips,2019,0,4488,Jiarui,Gan,ox,University of Oxford,jiarui.gan@cs.ox.ac.uk,Manipulating a Learning Defender and Ways to Counteract
neurips,2019,1,4488,Qingyu,Guo,ntu,Nanyang Technological University,qguo005@e.ntu.edu.sg,Manipulating a Learning Defender and Ways to Counteract
neurips,2019,2,4488,Long,Tran-Thanh,soton,University of Southampton,l.tran-thanh@soton.ac.uk,Manipulating a Learning Defender and Ways to Counteract
neurips,2019,3,4488,Bo,An,ntu,Nanyang Technological University,boan@ntu.edu.sg,Manipulating a Learning Defender and Ways to Counteract
neurips,2019,4,4488,Michael,Wooldridge,ox,Univ of Oxford,mjw@cs.ox.ac.uk,Manipulating a Learning Defender and Ways to Counteract
neurips,2019,0,7289,Borja,Balle,,Amazon,,Privacy Amplification by Mixing and Diffusion Mechanisms
neurips,2019,1,7289,Gilles,Barthe,,Max Planck Institute,,Privacy Amplification by Mixing and Diffusion Mechanisms
neurips,2019,2,7289,Marco,Gaboardi,,Univeristy at Buffalo,,Privacy Amplification by Mixing and Diffusion Mechanisms
neurips,2019,3,7289,Joseph,Geumlek,,"University of California, San Diego",,Privacy Amplification by Mixing and Diffusion Mechanisms
neurips,2019,0,1972,Tavor,Baharav,stanford,Stanford University,tavorb@stanford.edu,Ultra Fast Medoid Identification via Correlated Sequential Halving
neurips,2019,1,1972,David,Tse,stanford,Stanford University,dntse@stanford.edu,Ultra Fast Medoid Identification via Correlated Sequential Halving
neurips,2019,0,7047,Alberto,Bietti,inria,Inria,alberto.bietti@inria.fr,On the Inductive Bias of Neural Tangent Kernels
neurips,2019,1,7047,Julien,Mairal,inria,Inria,julien.mairal@inria.fr,On the Inductive Bias of Neural Tangent Kernels
neurips,2019,0,9357,Hosein,Hasani,sharif,Sharif University of Technology,hasani.hosein@ee.sharif.edu,Surround Modulation: A Bio-inspired Connectivity Structure for Convolutional Neural Networks
neurips,2019,1,9357,Mahdieh,Soleymani,sharif,Sharif University of Technology,soleymani@sharif.edu,Surround Modulation: A Bio-inspired Connectivity Structure for Convolutional Neural Networks
neurips,2019,2,9357,Hamid,Aghajan,sharif,Sharif University of Technology,aghajan@ee.sharif.edu,Surround Modulation: A Bio-inspired Connectivity Structure for Convolutional Neural Networks
neurips,2019,0,6235,Yu,Tian,rutgers,Rutgers,yt219@cs.rutgers.edu,Rethinking Kernel Methods for Node Representation Learning on Graphs
neurips,2019,1,6235,Long,Zhao,rutgers,Rutgers University,lz311@cs.rutgers.edu,Rethinking Kernel Methods for Node Representation Learning on Graphs
neurips,2019,2,6235,Xi,Peng,udel,University of Delaware,xipeng@udel.edu,Rethinking Kernel Methods for Node Representation Learning on Graphs
neurips,2019,3,6235,Dimitris,Metaxas,rutgers,Rutgers University,dnm@cs.rutgers.edu,Rethinking Kernel Methods for Node Representation Learning on Graphs
neurips,2019,0,6133,Moshe,Shenfeld,huji,Hebrew University of Jerusalem,katrina@cs.huji.ac.il,A Necessary and Sufficient Stability Notion for Adaptive Generalization
neurips,2019,1,6133,Katrina,Ligett,huji,Hebrew University,moshe.shenfeld@cs.huji.ac.il,A Necessary and Sufficient Stability Notion for Adaptive Generalization
neurips,2019,0,8201,Nicolò,Pagliana,unige,Università degli studi di Genova (DIMA),pagliana@dima.unige.it,Implicit Regularization of Accelerated Methods in Hilbert Spaces
neurips,2019,1,8201,Lorenzo,Rosasco,mit,University of Genova- MIT - IIT,lrosasco@mit.edu,Implicit Regularization of Accelerated Methods in Hilbert Spaces
neurips,2019,0,2880,Guillaume,Charpiat,,INRIA,,Input Similarity from the Neural Network Perspective
neurips,2019,1,2880,Nicolas,Girard,,Inria Sophia-Antipolis,,Input Similarity from the Neural Network Perspective
neurips,2019,2,2880,Loris,Felardos,,INRIA,,Input Similarity from the Neural Network Perspective
neurips,2019,3,2880,Yuliya,Tarabalka,,Inria Sophia-Antipolis,,Input Similarity from the Neural Network Perspective
neurips,2019,0,5673,Boyu,Wang,uwo,University of Western Ontario,bwang@csd.uwo.ca,Transfer Learning via Minimizing the Performance Gap Between Domains
neurips,2019,1,5673,Jorge,Mendez,upenn,University of Pennsylvania,mendezme@seas.upenn.edu,Transfer Learning via Minimizing the Performance Gap Between Domains
neurips,2019,2,5673,Mingbo,Cai,princeton,Princeton University,mcai@princeton.edu,Transfer Learning via Minimizing the Performance Gap Between Domains
neurips,2019,3,5673,Eric,Eaton,upenn,University of Pennsylvania,eeaton@seas.upenn.edu,Transfer Learning via Minimizing the Performance Gap Between Domains
neurips,2019,0,1101,Xinyang,Chen,,Tsinghua University,,Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning
neurips,2019,1,1101,Sinan,Wang,,Tsinghua University,,Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning
neurips,2019,2,1101,Bo,Fu,,Tsinghua University,,Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning
neurips,2019,3,1101,Mingsheng,Long,,Tsinghua University,,Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning
neurips,2019,4,1101,Jianmin,Wang,,Tsinghua University,,Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning
neurips,2019,0,16,Jiasen,Lu,,Georgia Tech,,ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
neurips,2019,1,16,Dhruv,Batra,,Georgia Tech / Facebook AI Research (FAIR),,ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
neurips,2019,2,16,Devi,Parikh,,Georgia Tech / Facebook AI Research (FAIR),,ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
neurips,2019,3,16,Stefan,Lee,,Georgia Institute of Technology,,ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
neurips,2019,0,8659,Andisheh,Amrollahi,ethz,ETH Zurich,amrollaa@ethz.ch,Efficiently Learning Fourier Sparse Set Functions
neurips,2019,1,8659,Amir,Zandieh,epfl,epfl,amir.zandieh@epfl.ch,Efficiently Learning Fourier Sparse Set Functions
neurips,2019,2,8659,Michael,Kapralov,epfl,EPFL,michael.kapralov@epfl.ch,Efficiently Learning Fourier Sparse Set Functions
neurips,2019,3,8659,Andreas,Krause,ethz,ETH Zurich,krausea@ethz.ch,Efficiently Learning Fourier Sparse Set Functions
neurips,2019,0,7492,Amirmohammad,Rooshenas,umass,University of Massachusetts Amherst,pedram@cs.umass.edu,"Search-Guided, Lightly-Supervised Training of Structured Prediction Energy Networks"
neurips,2019,1,7492,Dongxu,Zhang,umass,University of Massachusetts Amherst,dongxuzhang@cs.umass.edu,"Search-Guided, Lightly-Supervised Training of Structured Prediction Energy Networks"
neurips,2019,2,7492,Gopal,Sharma,umass,University of Massachusetts Amherst,gopalsharma@cs.umass.edu,"Search-Guided, Lightly-Supervised Training of Structured Prediction Energy Networks"
neurips,2019,3,7492,Andrew,McCallum,umass,UMass Amherst,mccallum@cs.umass.edu,"Search-Guided, Lightly-Supervised Training of Structured Prediction Energy Networks"
neurips,2019,0,8417,Soroush,Nasiriany,berkeley,UC Berkeley,snasiriany@berkeley.edu,Planning with Goal-Conditioned Policies
neurips,2019,1,8417,Vitchyr,Pong,berkeley,UC Berkeley,vitchyr@berkeley.edu,Planning with Goal-Conditioned Policies
neurips,2019,2,8417,Steven,Lin,berkeley,UC Berkeley,stevenlin598@berkeley.edu,Planning with Goal-Conditioned Policies
neurips,2019,3,8417,Sergey,Levine,berkeley,UC Berkeley,svlevine@berkeley.edu,Planning with Goal-Conditioned Policies
neurips,2019,0,8807,Yiming,Ding,berkeley,"University of California, Berkeley",dingyiming0427@berkeley.edu,Goal-conditioned Imitation Learning
neurips,2019,1,8807,Carlos,Florensa,berkeley,UC Berkeley,florensa@berkeley.edu,Goal-conditioned Imitation Learning
neurips,2019,2,8807,Pieter,Abbeel,intel,UC Berkeley & covariant.ai,mariano.j.phielipp@intel.com,Goal-conditioned Imitation Learning
neurips,2019,3,8807,Mariano,Phielipp,berkeley,Intel AI Labs,pabbeel@berkeley.edu,Goal-conditioned Imitation Learning
neurips,2019,0,5490,Larkin,Flodin,umass,University of Massachusetts Amherst,lflodin@cs.umass.edu,Superset Technique for Approximate Recovery in One-Bit Compressed Sensing
neurips,2019,1,5490,Venkata,Gandikota,gmail,"University of Massachusetts, Amherst",gandikota.venkata@gmail.com,Superset Technique for Approximate Recovery in One-Bit Compressed Sensing
neurips,2019,2,5490,Arya,Mazumdar,umass,University of Massachusetts Amherst,arya@cs.umass.edu,Superset Technique for Approximate Recovery in One-Bit Compressed Sensing
neurips,2019,0,3269,Yanyao,Shen,utexas,UT Austin,shenyanyao@utexas.edu,Iterative Least Trimmed Squares for Mixed Linear Regression
neurips,2019,1,3269,Sujay,Sanghavi,utexas,UT-Austin,sanghavi@mail.utexas.edu,Iterative Least Trimmed Squares for Mixed Linear Regression
neurips,2019,0,104,Kimia,Nadjahi,telecom-paris,Télécom ParisTech,kimia.nadjahi@telecom-paris.fr,Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance
neurips,2019,1,104,Alain,Durmus,telecom-paris,ENS Paris Saclay,umut.simsekli@telecom-paris.fr,Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance
neurips,2019,2,104,Umut,Simsekli,telecom-paris,Institut Polytechnique de Paris/ University of Oxford,roland.badeau@telecom-paris.fr,Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance
neurips,2019,3,104,Roland,Badeau,ens-cachan,Télécom ParisTech,alain.durmus@cmla.ens-cachan.fr,Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance
neurips,2019,0,2946,Jinsung,Yoon,,"University of California, Los Angeles",,Time-series Generative Adversarial Networks
neurips,2019,1,2946,Daniel,Jarrett,,University of Cambridge,,Time-series Generative Adversarial Networks
neurips,2019,2,2946,Mihaela,van der Schaar,,"University of California, Los Angeles",,Time-series Generative Adversarial Networks
neurips,2019,0,3778,Sebastian,Goldt,,"Institut de Physique Théorique, CNRS, Paris",,Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup
neurips,2019,1,3778,Madhu,Advani,,Apple,,Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup
neurips,2019,2,3778,Andrew,Saxe,,University of Oxford,,Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup
neurips,2019,3,3778,Florent,Krzakala,,École Normale Supérieure,,Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup
neurips,2019,4,3778,Lenka,Zdeborová,,CEA Saclay,,Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup
neurips,2019,0,3637,Mike,Gartrell,criteo,Criteo AI Lab,m.gartrell@criteo.com,Learning Nonsymmetric Determinantal Point Processes
neurips,2019,1,3637,Victor-Emmanuel,Brunel,criteo,ENSAE ParisTech,e.dohmatob@criteo.com,Learning Nonsymmetric Determinantal Point Processes
neurips,2019,2,3637,Elvis,Dohmatob,ensae,Criteo,victor.emmanuel.brunel@ensae.fr,Learning Nonsymmetric Determinantal Point Processes
neurips,2019,3,3637,Syrine,Krichene,google,Google,syrinekrichene@google.com,Learning Nonsymmetric Determinantal Point Processes
neurips,2019,0,2995,Dinesh,Garg,ibm,"IBM Research AI, India",garg.dinesh@in.ibm.com,Quantum Embedding of Knowledge for Reasoning
neurips,2019,1,2995,Shajith,Ikbal,ibm,"IBM Research AI, India",shajmoha@in.ibm.com,Quantum Embedding of Knowledge for Reasoning
neurips,2019,2,2995,Santosh,Srivastava,ibm,IBM Research AI,sasriva5@in.ibm.com,Quantum Embedding of Knowledge for Reasoning
neurips,2019,3,2995,Harit,Vishwakarma,wisc,University of Wisconsin Madison,hvishwakarma@cs.wisc.edu,Quantum Embedding of Knowledge for Reasoning
neurips,2019,4,2995,Hima,Karanam,ibm,IBM Research AI,hkaranam@in.ibm.com,Quantum Embedding of Knowledge for Reasoning
neurips,2019,5,2995,L Venkata,Subramaniam,ibm,IBM Research AI - India,lvsubram@in.ibm.com,Quantum Embedding of Knowledge for Reasoning
neurips,2019,0,4561,Vitaliy,Chiley,,Cerebras Systems,,Online Normalization for Training Neural Networks
neurips,2019,1,4561,Ilya,Sharapov,,Cerebras Systems,,Online Normalization for Training Neural Networks
neurips,2019,2,4561,Atli,Kosson,,Cerebras Systems,,Online Normalization for Training Neural Networks
neurips,2019,3,4561,Urs,Koster,,Cerebras Systems,,Online Normalization for Training Neural Networks
neurips,2019,4,4561,Ryan,Reece,,Cerebras Systems,,Online Normalization for Training Neural Networks
neurips,2019,5,4561,Sofia,Samaniego de la Fuente,,Cerebras Systems,,Online Normalization for Training Neural Networks
neurips,2019,6,4561,Vishal,Subbiah,,Cerebras Systems,,Online Normalization for Training Neural Networks
neurips,2019,7,4561,Michael,James,,Cerebras,,Online Normalization for Training Neural Networks
neurips,2019,0,243,Nikolaos,Tziavelis,,Northeastern University,,Equitable Stable Matchings in Quadratic Time
neurips,2019,1,243,Ioannis,Giannakopoulos,,National Technical University of Athens,,Equitable Stable Matchings in Quadratic Time
neurips,2019,2,243,Katerina,Doka,,NTUA,,Equitable Stable Matchings in Quadratic Time
neurips,2019,3,243,Nectarios,Koziris,,NTUA,,Equitable Stable Matchings in Quadratic Time
neurips,2019,4,243,Panagiotis,Karras,,Aarhus University,,Equitable Stable Matchings in Quadratic Time
neurips,2019,0,1917,Antonio,Ginart,stanford,Stanford University,tginart@stanford.edu,Making AI Forget You: Data Deletion in Machine Learning
neurips,2019,1,1917,Melody,Guan,stanford,Stanford University,mguan@stanford.edu,Making AI Forget You: Data Deletion in Machine Learning
neurips,2019,2,1917,Gregory,Valiant,stanford,Stanford University,valiant@stanford.edu,Making AI Forget You: Data Deletion in Machine Learning
neurips,2019,3,1917,James,Zou,stanford,Stanford,jamesz@stanford.edu,Making AI Forget You: Data Deletion in Machine Learning
neurips,2019,0,926,Shengyuan,Hu,,Cornell University,,A New Defense Against Adversarial Images: Turning a Weakness into a Strength
neurips,2019,1,926,Tao,Yu,,Cornell University,,A New Defense Against Adversarial Images: Turning a Weakness into a Strength
neurips,2019,2,926,Chuan,Guo,,Cornell University,,A New Defense Against Adversarial Images: Turning a Weakness into a Strength
neurips,2019,3,926,Wei-Lun,Chao,,Cornell University          Ohio State University (OSU),,A New Defense Against Adversarial Images: Turning a Weakness into a Strength
neurips,2019,4,926,Kilian,Weinberger,,Cornell University / ASAPP Research,,A New Defense Against Adversarial Images: Turning a Weakness into a Strength
neurips,2019,0,8196,Brendan,O'Donoghue,,DeepMind,,Hamiltonian descent for composite objectives
neurips,2019,1,8196,Chris,Maddison,,"Institute for Advanced Study, Princeton",,Hamiltonian descent for composite objectives
neurips,2019,0,2621,Fan,Yang,cmu,Carnegie Mellon University,fanyang1@cs.cmu.edu,Game Design for Eliciting Distinguishable Behavior
neurips,2019,1,2621,Liu,Leqi,cmu,Carnegie Mellon University,leqil@cs.cmu.edu,Game Design for Eliciting Distinguishable Behavior
neurips,2019,2,2621,Yifan,Wu,cmu,Carnegie Mellon University,yw4@cs.cmu.edu,Game Design for Eliciting Distinguishable Behavior
neurips,2019,3,2621,Zachary,Lipton,cmu,Carnegie Mellon University,pradeepr@cs.cmu.edu,Game Design for Eliciting Distinguishable Behavior
neurips,2019,4,2621,Pradeep,Ravikumar,cmu,Carnegie Mellon University,wcohen@cs.cmu.edu,Game Design for Eliciting Distinguishable Behavior
neurips,2019,5,2621,Tom,Mitchell,cmu,Carnegie Mellon University,zlipton@cmu.edu,Game Design for Eliciting Distinguishable Behavior
neurips,2019,6,2621,William,Cohen,cmu,Google AI,tom.mitchell@cmu.edu,Game Design for Eliciting Distinguishable Behavior
neurips,2019,0,3280,Qing,Wang,,Huya AI,,Divergence-Augmented Policy Optimization
neurips,2019,1,3280,Yingru,Li,,"The Chinese University of Hong Kong, Shenzhen, China",,Divergence-Augmented Policy Optimization
neurips,2019,2,3280,Jiechao,Xiong,,Tencent AI Lab,,Divergence-Augmented Policy Optimization
neurips,2019,3,3280,Tong,Zhang,,Tencent AI Lab,,Divergence-Augmented Policy Optimization
neurips,2019,0,5999,Takumi,Kobayashi,go,National Institute of Advanced Industrial Science and Technology,takumi.kobayashi@aist.go.jp,Gaussian-Based Pooling for Convolutional Neural Networks
neurips,2019,0,6935,Felipe,Tobar,uchile,Universidad de Chile,ftobar@dim.uchile.cl,Band-Limited Gaussian Processes: The Sinc Kernel
neurips,2019,0,5861,Sitao,Luan,,"McGill University, Mila",sitao.luan@mail,Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks
neurips,2019,1,5861,Mingde,Zhao,,Mila & McGill University,mingde.zhao@mail,Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks
neurips,2019,2,5861,Xiao-Wen,Chang,,McGill University,chang@cs,Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks
neurips,2019,3,5861,Doina,Precup,mcgill,McGill University / Mila / DeepMind Montreal,dprecup@cs.mcgill.ca,Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks
neurips,2019,0,6304,Huong,Ha,deakin,Deakin University,huong.ha@deakin.edu.au,Bayesian Optimization with Unknown Search Space
neurips,2019,1,6304,Santu,Rana,deakin,Deakin University,santu.rana@deakin.edu.au,Bayesian Optimization with Unknown Search Space
neurips,2019,2,6304,Sunil,Gupta,deakin,Deakin University,sunil.gupta@deakin.edu.au,Bayesian Optimization with Unknown Search Space
neurips,2019,3,6304,Thanh,Nguyen,deakin,Deakin University,thanhnt@deakin.edu.au,Bayesian Optimization with Unknown Search Space
neurips,2019,4,6304,Hung,Tran-The,deakin,Deakin University,hung.tranthe@deakin.edu.au,Bayesian Optimization with Unknown Search Space
neurips,2019,5,6304,Svetha,Venkatesh,deakin,Deakin University,svetha.venkatesh@deakin.edu.au,Bayesian Optimization with Unknown Search Space
neurips,2019,0,328,Othmane,Sebbouh,gmail,Télécom ParisTech,othmane.sebbouh@gmail.com,Towards closing the gap between the theory and practice of SVRG
neurips,2019,1,328,Nidham,Gazagnadou,telecom-paris,Télécom Paris,nidham.gazagnadou@telecom-paris.fr,Towards closing the gap between the theory and practice of SVRG
neurips,2019,2,328,Samy,Jelassi,princeton,Princeton University,sjelassi@princeton.edu,Towards closing the gap between the theory and practice of SVRG
neurips,2019,3,328,Francis,Bach,inria,INRIA - Ecole Normale Superieure,francis.bach@inria.fr,Towards closing the gap between the theory and practice of SVRG
neurips,2019,4,328,Robert,Gower,telecom-paris,"Institut Polytechnique de Paris, Telecom Paris",robert.gower@telecom-paris.fr,Towards closing the gap between the theory and practice of SVRG
neurips,2019,0,4192,Gecia,Bravo Hermsdorff,princeton,Princeton University,geciah@princeton.edu,A Unifying Framework for Spectrum-Preserving Graph Sparsification and Coarsening
neurips,2019,1,4192,Lee,Gunderson,princeton,Princeton University,leeg@princeton.edu,A Unifying Framework for Spectrum-Preserving Graph Sparsification and Coarsening
neurips,2019,0,4658,Gunjan,Verma,mail,ARL,gunjan.verma.civ@mail.mil,Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks
neurips,2019,1,4658,Ananthram,Swami,mail,"Army Research Laboratory, Adelphi",ananthram.swami.civ@mail.mil,Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks
neurips,2019,0,1849,Zhen,Zhang,wustl,WASHINGTON UNIVERSITY IN ST.LOUIS,1zhen.zhang@wustl.edu,KerGM: Kernelized Graph Matching
neurips,2019,1,1849,Yijian,Xiang,wustl,Washington University in St. Louis,yijian.xiang@wustl.edu,KerGM: Kernelized Graph Matching
neurips,2019,2,1849,Lingfei,Wu,wustl,IBM Research AI,xuebing@wustl.edu,KerGM: Kernelized Graph Matching
neurips,2019,3,1849,Bing,Xue,wustl,Washington University in St. Louis,nehorai@wustl.edu,KerGM: Kernelized Graph Matching
neurips,2019,4,1849,Arye,Nehorai,wm,WASHINGTON UNIVERSITY IN ST.LOUIS,2lwu@email.wm.edu,KerGM: Kernelized Graph Matching
neurips,2019,0,8591,Liu,Leqi,cmu,Carnegie Mellon University,leqil@cs.cmu.edu,On Human-Aligned Risk Minimization
neurips,2019,1,8591,Adarsh,Prasad,cmu,Carnegie Mellon University,adarshp@cs.cmu.edu,On Human-Aligned Risk Minimization
neurips,2019,2,8591,Pradeep,Ravikumar,cmu,Carnegie Mellon University,pradeepr@cs.cmu.edu,On Human-Aligned Risk Minimization
neurips,2019,0,6665,Hongge,Chen,mit,MIT,chenhg@mit.edu,Robustness Verification of Tree-based Models
neurips,2019,1,6665,Huan,Zhang,huan-zhang,UCLA,huan@huan-zhang.com,Robustness Verification of Tree-based Models
neurips,2019,2,6665,Si,Si,google,Google Research,sisidaisy@google.com,Robustness Verification of Tree-based Models
neurips,2019,3,6665,Yang,Li,google,Google,liyang@google.com,Robustness Verification of Tree-based Models
neurips,2019,4,6665,Duane,Boning,mit,Massachusetts Institute of Technology,boning@mtl.mit.edu,Robustness Verification of Tree-based Models
neurips,2019,5,6665,Cho-Jui,Hsieh,ucla,UCLA,chohsieh@cs.ucla.edu,Robustness Verification of Tree-based Models
neurips,2019,0,6094,Kai,Zhong,amazon,Amazon,kaizhong@amazon.com,Provable Non-linear Inductive Matrix Completion
neurips,2019,1,6094,Zhao,Song,microsoft,UT-Austin,prajain@microsoft.com,Provable Non-linear Inductive Matrix Completion
neurips,2019,2,6094,Prateek,Jain,gmail,Microsoft Research,magic.linuxkde@gmail.com,Provable Non-linear Inductive Matrix Completion
neurips,2019,3,6094,Inderjit,Dhillon,amazon,UT Austin & Amazon,isd@amazon.com,Provable Non-linear Inductive Matrix Completion
neurips,2019,0,4875,Karim,Ahmed,dartmouth,Cornell; Darmouth,LT@dartmouth.edu,STAR-Caps: Capsule Networks with Straight-Through Attentive Routing
neurips,2019,1,4875,Lorenzo,Torresani,dartmouth,Facebook,karim@cs.dartmouth.edu,STAR-Caps: Capsule Networks with Straight-Through Attentive Routing
neurips,2019,0,9358,Da,Xu,walmartlabs,Walmart Labs,Da.Xu@walmartlabs.com,Self-attention with Functional Time Representation Learning
neurips,2019,1,9358,Chuanwei,Ruan,walmartlabs,Walmart Labs,Chuanwei.Ruan@walmartlabs.com,Self-attention with Functional Time Representation Learning
neurips,2019,2,9358,Evren,Korpeoglu,walmartlabs,Walmart Labs,EKorpeoglu@walmartlabs.com,Self-attention with Functional Time Representation Learning
neurips,2019,3,9358,Sushant,Kumar,walmartlabs,Walmart Labs,SKumar4@walmartlabs.com,Self-attention with Functional Time Representation Learning
neurips,2019,4,9358,Kannan,Achan,walmartlabs,Walmart Labs,KAchan@walmartlabs.com,Self-attention with Functional Time Representation Learning
neurips,2019,0,520,Xuesong,Niu,ict,"Institute of Computing Technology, CAS",xuesong.niu@vipl.ict.ac.cn,Multi-label Co-regularization for Semi-supervised Facial Action Unit Recognition
neurips,2019,1,520,Hu,Han,ict,"ICT, CAS",hanhu@ict.ac.cn,Multi-label Co-regularization for Semi-supervised Facial Action Unit Recognition
neurips,2019,2,520,Shiguang,Shan,ict,Chinese Academy of Sciences,sgshan@ict.ac.cn,Multi-label Co-regularization for Semi-supervised Facial Action Unit Recognition
neurips,2019,3,520,Xilin,Chen,ict,"Institute of Computing Technology, Chinese Academy of Sciences",xlchen@ict.ac.cn,Multi-label Co-regularization for Semi-supervised Facial Action Unit Recognition
neurips,2019,0,6594,Yatin,Nandwani,iitd,Indian Institute Of Technology Delhi,yatin.nandwani@cse.iitd.ac.in,A Primal Dual Formulation For Deep Learning With Constraints
neurips,2019,1,6594,Abhishek,Pathak,iitd,"Indian Institute Of Technology, Delhi",abhishek.pathak.cs115@cse.iitd.ac.in,A Primal Dual Formulation For Deep Learning With Constraints
neurips,2019,2,6594,Mausam,,iitd,IIT Dehli,mausam@cse.iitd.ac.in,A Primal Dual Formulation For Deep Learning With Constraints
neurips,2019,3,6594,Parag,Singla,iitd,Indian Institute of Technology Delhi,parags@cse.iitd.ac.in,A Primal Dual Formulation For Deep Learning With Constraints
neurips,2019,0,1361,Ofir,Nachum,google,Google Brain,ofirnachum@google.com,DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections
neurips,2019,1,1361,Yinlam,Chow,google,Google Research,yinlamchow@google.com,DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections
neurips,2019,2,1361,Bo,Dai,google,Google Brain,bodai@google.com,DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections
neurips,2019,3,1361,Lihong,Li,google,Google Brain,lihong@google.com,DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections
neurips,2019,0,5782,Yuan,Cao,ucla,UCLA,yuancao@cs.ucla.edu,Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks
neurips,2019,1,5782,Quanquan,Gu,ucla,UCLA,qgu@cs.ucla.edu,Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks
neurips,2019,0,3292,Alessio,Ansuini,gmail,International School for Advanced Studies (SISSA),alessioansuini@gmail.com,Intrinsic dimension of data representations in deep neural networks
neurips,2019,1,3292,Alessandro,Laio,tum,International School for Advanced Studies (SISSA),macke@tum.de,Intrinsic dimension of data representations in deep neural networks
neurips,2019,2,3292,Jakob,Macke,sissa,"Technical University of Munich, Munich, Germany",laio@sissa.it,Intrinsic dimension of data representations in deep neural networks
neurips,2019,3,3292,Davide,Zoccolan,sissa,"Visual Neuroscience Lab, International School for Advanced Studies (SISSA)",zoccolan@sissa.it,Intrinsic dimension of data representations in deep neural networks
neurips,2019,0,5774,Eui Chul,Shin,berkeley,UC Berkeley,ricshin@berkeley.edu,Program Synthesis and Semantic Parsing with Learned Code Idioms
neurips,2019,1,5774,Miltiadis,Allamanis,microsoft,Microsoft Research,miallama@microsoft.com,Program Synthesis and Semantic Parsing with Learned Code Idioms
neurips,2019,2,5774,Marc,Brockschmidt,microsoft,Microsoft Research,mabrocks@microsoft.com,Program Synthesis and Semantic Parsing with Learned Code Idioms
neurips,2019,3,5774,Alex,Polozov,microsoft,Microsoft Research,polozov@microsoft.com,Program Synthesis and Semantic Parsing with Learned Code Idioms
neurips,2019,0,2808,Gautier,Izacard,polytechnique,Ecole Polytechnique,gautier.izacard@polytechnique.edu,Data-driven Estimation of Sinusoid Frequencies
neurips,2019,1,2808,Sreyas,Mohan,nyu,NYU,sm7582@nyu.edu,Data-driven Estimation of Sinusoid Frequencies
neurips,2019,2,2808,Carlos,Fernandez-Granda,nyu,NYU,cfgranda@cims.nyu.edu,Data-driven Estimation of Sinusoid Frequencies
neurips,2019,0,1542,Mitchell,Wortsman,washington,"University of Washington, Allen Institute for Artificial Intelligence",mitchnw@cs.washington.edu,Discovering Neural Wirings
neurips,2019,1,1542,Ali,Farhadi,xnor,"University of Washington, Allen Institute for Artificial Intelligence",ali@xnor.ai,Discovering Neural Wirings
neurips,2019,2,1542,Mohammad,Rastegari,xnor,XNOR.AI- AI2,mohammad@xnor.ai,Discovering Neural Wirings
neurips,2019,0,8557,Amit,Daniely,,Hebrew University and Google Research,,Locally Private Learning without Interaction Requires Separation
neurips,2019,1,8557,Vitaly,Feldman,,Google Brain,,Locally Private Learning without Interaction Requires Separation
neurips,2019,0,4477,Hugo,Touvron,,Facebook AI Research,,Fixing the train-test resolution discrepancy
neurips,2019,1,4477,Andrea,Vedaldi,,Facebook AI Research and University of Oxford,,Fixing the train-test resolution discrepancy
neurips,2019,2,4477,Matthijs,Douze,,Facebook AI Research,,Fixing the train-test resolution discrepancy
neurips,2019,3,4477,Herve,Jegou,,Facebook AI Research,,Fixing the train-test resolution discrepancy
neurips,2019,0,928,Xiangyu,Xu,gmail,Carnegie Mellon University,xuxiangyu2014@gmail.com,Quadratic Video Interpolation
neurips,2019,1,928,Li,Siyao,sensetime,SenseTime Research,lisiyao1@sensetime.com,Quadratic Video Interpolation
neurips,2019,2,928,Wenxiu,Sun,sensetime,SenseTime Research,sunwenxiu@sensetime.com,Quadratic Video Interpolation
neurips,2019,3,928,Qian,Yin,bnu,Beijing Normal University,yinqian@bnu.edu.cn,Quadratic Video Interpolation
neurips,2019,4,928,Ming-Hsuan,Yang,ucmerced,Google / UC Merced,mhyang@ucmerced.edu,Quadratic Video Interpolation
neurips,2019,0,7259,Ngoc-Trung,Tran,,Singapore University of Technology and Design,,Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game
neurips,2019,1,7259,Viet-Hung,Tran,,Singapore University of Technology and Design,,Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game
neurips,2019,2,7259,Bao-Ngoc,Nguyen,,Singapore University of Technology and Design,,Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game
neurips,2019,3,7259,Linxiao,Yang,,University of Electronic Science  and Technology of China; Singapore University of Technology and Design,,Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game
neurips,2019,4,7259,Ngai-Man (Man),Cheung,,Singapore University of Technology and Design,,Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game
neurips,2019,0,7188,Pierre,Ablin,inria,Inria,pierre.ablin@inria.fr,Learning step sizes for unfolded sparse coding
neurips,2019,1,7188,Thomas,Moreau,inria,Inria,thomas.moreau@inria.fr,Learning step sizes for unfolded sparse coding
neurips,2019,2,7188,Mathurin,Massias,inria,Inria,mathurin.massias@inria.fr,Learning step sizes for unfolded sparse coding
neurips,2019,3,7188,Alexandre,Gramfort,inria,INRIA,alexandre.gramfort@inria.fr,Learning step sizes for unfolded sparse coding
neurips,2019,0,2394,Renjie,Liao,,University of Toronto,,Efficient Graph Generation with Graph Recurrent Attention Networks
neurips,2019,1,2394,Yujia,Li,,DeepMind,,Efficient Graph Generation with Graph Recurrent Attention Networks
neurips,2019,2,2394,Yang,Song,,Stanford University,,Efficient Graph Generation with Graph Recurrent Attention Networks
neurips,2019,3,2394,Shenlong,Wang,,University of Toronto,,Efficient Graph Generation with Graph Recurrent Attention Networks
neurips,2019,4,2394,Will,Hamilton,,McGill,,Efficient Graph Generation with Graph Recurrent Attention Networks
neurips,2019,5,2394,David,Duvenaud,,University of Toronto,,Efficient Graph Generation with Graph Recurrent Attention Networks
neurips,2019,6,2394,Raquel,Urtasun,,Uber ATG,,Efficient Graph Generation with Graph Recurrent Attention Networks
neurips,2019,7,2394,Richard,Zemel,,Vector Institute/University of Toronto,,Efficient Graph Generation with Graph Recurrent Attention Networks
neurips,2019,0,75,Vineet,Kosaraju,,Stanford University,,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks
neurips,2019,1,75,Amir,Sadeghian,,Stanford University,,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks
neurips,2019,2,75,Roberto,Martín-Martín,,Stanford University,,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks
neurips,2019,3,75,Ian,Reid,,University of Adelaide,,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks
neurips,2019,4,75,Hamid,Rezatofighi,,Stanford University // University of Adelaide,,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks
neurips,2019,5,75,Silvio,Savarese,,Stanford University,,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks
neurips,2019,0,3645,Bo,Yang,,University of Oxford,,Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds
neurips,2019,1,3645,Jianan,Wang,,DeepMind,,Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds
neurips,2019,2,3645,Ronald,Clark,,Imperial College London,,Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds
neurips,2019,3,3645,Qingyong,Hu,,University of Oxford,,Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds
neurips,2019,4,3645,Sen,Wang,,Heriot-Watt University,,Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds
neurips,2019,5,3645,Andrew,Markham,,University of Oxford,,Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds
neurips,2019,6,3645,Niki,Trigoni,,University of Oxford,,Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds
neurips,2019,0,4225,Guokun,Lai,cmu,Carnegie Mellon University,1guokun@cs.cmu.edu,Re-examination of the Role of Latent Variables in Sequence Modeling
neurips,2019,1,4225,Zihang,Dai,cmu,Carnegie Mellon University,dzihang@cs.cmu.edu,Re-examination of the Role of Latent Variables in Sequence Modeling
neurips,2019,2,4225,Yiming,Yang,cmu,CMU,yiming@cs.cmu.edu,Re-examination of the Role of Latent Variables in Sequence Modeling
neurips,2019,3,4225,Shinjae,Yoo,bnl,Brookhaven National Lab,2sjyoo@bnl.gov,Re-examination of the Role of Latent Variables in Sequence Modeling
neurips,2019,0,5738,Jisoo,Jeong,snu,Seoul National University,soo3553@snu.ac.kr,Consistency-based Semi-supervised Learning for Object detection
neurips,2019,1,5738,Seungeui,Lee,snu,Seoul National University,seungeui.lee@snu.ac.kr,Consistency-based Semi-supervised Learning for Object detection
neurips,2019,2,5738,Jeesoo,Kim,snu,Seoul National University,kimjiss0305@snu.ac.kr,Consistency-based Semi-supervised Learning for Object detection
neurips,2019,3,5738,Nojun,Kwak,snu,Seoul National University,nojunk@snu.ac.kr,Consistency-based Semi-supervised Learning for Object detection
neurips,2019,0,8838,Kwang-Sung,Jun,arizona,U of Arizona,kjun@cs.arizona.edu,Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration
neurips,2019,1,8838,Ashok,Cutkosky,cutkosky,Google Research,ashok@cutkosky.com,Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration
neurips,2019,2,8838,Francesco,Orabona,orabona,Boston University,francesco@orabona.com,Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration
neurips,2019,0,5499,Raman,Arora,jhu,Johns Hopkins University,arora@cs.jhu.edu,Bandits with Feedback Graphs and Switching Costs
neurips,2019,1,5499,Teodor Vanislavov,Marinov,jhu,Johns Hopkins University,tmarino2@jhu.edu,Bandits with Feedback Graphs and Switching Costs
neurips,2019,2,5499,Mehryar,Mohri,google,Courant Inst. of Math. Sciences & Google Research,mohri@google.com,Bandits with Feedback Graphs and Switching Costs
neurips,2019,0,9029,Maxime,Gasse,polymtl,Polytechnique Montréal,maxime.gasse@polymtl.ca,Exact Combinatorial Optimization with Graph Convolutional Neural Networks
neurips,2019,1,9029,Didier,Chetelat,polymtl,Polytechnique Montreal,didier.chetelat@polymtl.ca,Exact Combinatorial Optimization with Graph Convolutional Neural Networks
neurips,2019,2,9029,Nicola,Ferroni,specialvideo,University of Bologna,n.ferroni@specialvideo.it,Exact Combinatorial Optimization with Graph Convolutional Neural Networks
neurips,2019,3,9029,Laurent,Charlin,hec,MILA / U.Montreal,laurent.charlin@hec.ca,Exact Combinatorial Optimization with Graph Convolutional Neural Networks
neurips,2019,4,9029,Andrea,Lodi,polymtl,École Polytechnique Montréal,andrea.lodi@polymtl.ca,Exact Combinatorial Optimization with Graph Convolutional Neural Networks
neurips,2019,0,3240,Mareike,Hartmann,ku,University of Copenhagen,hartmann@di.ku.dk,Comparing Unsupervised Word Translation Methods Step by Step
neurips,2019,1,3240,Yova,Kementchedjhieva,ku,University of Copenhagen,yova@di.ku.dk,Comparing Unsupervised Word Translation Methods Step by Step
neurips,2019,2,3240,Anders,Søgaard,ku,University of Copenhagen,soegaard@di.ku.dk,Comparing Unsupervised Word Translation Methods Step by Step
neurips,2019,0,479,Tingting,Qiao,zju,Zhejiang University,qiaott@zju.edu.cn,"Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge"
neurips,2019,1,479,Jing,Zhang,zju,The University of Sydney,xdq@zju.edu.cn,"Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge"
neurips,2019,2,479,Duanqing,Xu,sydney,Zhejiang University,jing.zhang1@sydney.edu.au,"Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge"
neurips,2019,3,479,Dacheng,Tao,sydney,University of Sydney,dacheng.tao@sydney.edu.au,"Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge"
neurips,2019,0,8271,Charith,Mendis,mit,MIT,charithm@mit.edu,Compiler Auto-Vectorization with Imitation Learning
neurips,2019,1,8271,Cambridge,Yang,mit,MIT,camyang@csail.mit.edu,Compiler Auto-Vectorization with Imitation Learning
neurips,2019,2,8271,Yewen,Pu,mit,MIT,yewenpu@mit.edu,Compiler Auto-Vectorization with Imitation Learning
neurips,2019,3,8271,Dr.Saman,Amarasinghe,mit,Massachusetts institute of technology,saman@csail.mit.edu,Compiler Auto-Vectorization with Imitation Learning
neurips,2019,4,8271,Michael,Carbin,mit,MIT,mcarbin@csail.mit.edu,Compiler Auto-Vectorization with Imitation Learning
neurips,2019,0,8295,Debraj,Basu,adobe,Adobe Inc.,dbasu@adobe.com,"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations"
neurips,2019,1,8295,Deepesh,Data,amazon,UCLA,cakarak@amazon.com,"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations"
neurips,2019,2,8295,Can,Karakus,ucla,Amazon Web Services,deepeshdata@ucla.edu,"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations"
neurips,2019,3,8295,Suhas,Diggavi,ucla,UCLA,suhasdiggavi@ucla.edu,"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations"
neurips,2019,0,958,Yasutoshi,Ida,ieee,NTT,yasutoshi.ida@ieee.org,Fast Sparse Group Lasso
neurips,2019,1,958,Yasuhiro,Fujiwara,ntt,NTT Communication Science Laboratories,yasuhiro.fujiwara.kh@hco.ntt.co.jp,Fast Sparse Group Lasso
neurips,2019,2,958,Hisashi,Kashima,kyoto-u,Kyoto University/RIKEN Center for AIP,kashima@i.kyoto-u.ac.jp,Fast Sparse Group Lasso
neurips,2019,0,7339,Gabriel,Loaiza-Ganem,columbia,Columbia University,gl2480@columbia.edu,Deep Random Splines for Point Process Intensity Estimation of Neural Population Data
neurips,2019,1,7339,Sean,Perkins,columbia,Columbia University,sp3222@columbia.edu,Deep Random Splines for Point Process Intensity Estimation of Neural Population Data
neurips,2019,2,7339,Karen,Schroeder,columbia,Columbia University,ks3381@columbia.edu,Deep Random Splines for Point Process Intensity Estimation of Neural Population Data
neurips,2019,3,7339,Mark,Churchland,columbia,Columbia University,mc3502@columbia.edu,Deep Random Splines for Point Process Intensity Estimation of Neural Population Data
neurips,2019,4,7339,John,Cunningham,columbia,University of Columbia,jpc2181@columbia.edu,Deep Random Splines for Point Process Intensity Estimation of Neural Population Data
neurips,2019,0,4455,Senanayak Sesh Kumar,Karri,imperial,Imperial College London,s.karri@imperial.ac.uk,Fast Decomposable Submodular Function Minimization using Constrained Total Variation
neurips,2019,1,4455,Francis,Bach,inria,INRIA - Ecole Normale Superieure,francis.bach@inria.fr,Fast Decomposable Submodular Function Minimization using Constrained Total Variation
neurips,2019,2,4455,Thomas,Pock,tugraz,Graz University of Technology,pock@icg.tugraz.at,Fast Decomposable Submodular Function Minimization using Constrained Total Variation
neurips,2019,0,1757,Patrick,Kidger,,University of Oxford,,Deep Signature Transforms
neurips,2019,1,1757,Patric,Bonnier,,University of Oxford,,Deep Signature Transforms
neurips,2019,2,1757,Imanol,Perez Arribas,,University of Oxford,,Deep Signature Transforms
neurips,2019,3,1757,Cristopher,Salvi,,University of Oxford,,Deep Signature Transforms
neurips,2019,4,1757,Terry,Lyons,,University of Oxford,,Deep Signature Transforms
neurips,2019,0,932,Bao,Wang,,UCLA,,ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies
neurips,2019,1,932,Zuoqiang,Shi,,zqshi@mail.tsinghua.edu.cn,,ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies
neurips,2019,2,932,Stanley,Osher,,UCLA,,ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies
neurips,2019,0,5115,Russell,Mendonca,berkeley,UC Berkeley,russellm@berkeley.edu,Guided Meta-Policy Search
neurips,2019,1,5115,Abhishek,Gupta,berkeley,"University of California, Berkeley",cbfinn@berkeley.edu,Guided Meta-Policy Search
neurips,2019,2,5115,Rosen,Kralev,berkeley,UC Berkeley,abhigupta@eecs.berkeley.edu,Guided Meta-Policy Search
neurips,2019,3,5115,Pieter,Abbeel,berkeley,UC Berkeley & covariant.ai,pabbeel@eecs.berkeley.edu,Guided Meta-Policy Search
neurips,2019,4,5115,Sergey,Levine,berkeley,UC Berkeley,svlevine@eecs.berkeley.edu,Guided Meta-Policy Search
neurips,2019,5,5115,Chelsea,Finn,gmail,Stanford University,rdkralev@gmail.com,Guided Meta-Policy Search
neurips,2019,0,4037,Theo,Deprelle,,École des ponts ParisTech,,Learning elementary structures for 3D shape generation and matching
neurips,2019,1,4037,Thibault,Groueix,,École des ponts ParisTech,,Learning elementary structures for 3D shape generation and matching
neurips,2019,2,4037,Matthew,Fisher,,Adobe Research,,Learning elementary structures for 3D shape generation and matching
neurips,2019,3,4037,Vladimir,Kim,,Adobe,,Learning elementary structures for 3D shape generation and matching
neurips,2019,4,4037,Bryan,Russell,,Adobe,,Learning elementary structures for 3D shape generation and matching
neurips,2019,5,4037,Mathieu,Aubry,,École des ponts ParisTech,,Learning elementary structures for 3D shape generation and matching
neurips,2019,0,5762,CHAO,LI,gmail,Xidian University,chaolee.xd@gmail.com,Cross-Modal Learning with Adversarial Samples
neurips,2019,1,5762,Shangqian,Gao,gmail,University of Pittsburgh,chdeng.xd@gmail.com,Cross-Modal Learning with Adversarial Samples
neurips,2019,2,5762,Cheng,Deng,gmail,Xidian University,xiede.xd@gmail.com,Cross-Modal Learning with Adversarial Samples
neurips,2019,3,5762,De,Xie,pitt,XiDian University,shg84@pitt.edu,Cross-Modal Learning with Adversarial Samples
neurips,2019,4,5762,Wei,Liu,columbia,Tencent AI Lab,wl2223@columbia.edu,Cross-Modal Learning with Adversarial Samples
neurips,2019,0,2853,Chanho,Eom,yonsei,Yonsei University,cheom@yonsei.ac.kr,Learning Disentangled Representation for Robust Person Re-identification
neurips,2019,1,2853,Bumsub,Ham,yonsei,Yonsei University,bumsub.ham@yonsei.ac.kr,Learning Disentangled Representation for Robust Person Re-identification
neurips,2019,0,2851,Ivan,Stelmakh,cmu,Carnegie Mellon University,stiv@cs.cmu.edu,On Testing for Biases in Peer Review
neurips,2019,1,2851,Nihar,Shah,cmu,CMU,nihars@cs.cmu.edu,On Testing for Biases in Peer Review
neurips,2019,2,2851,Aarti,Singh,cmu,CMU,aarti@cs.cmu.edu,On Testing for Biases in Peer Review
neurips,2019,0,4626,Gail,Weiss,technion,Technion,sgailw@cs.technion.ac.il,Learning Deterministic Weighted Automata with Queries and Counterexamples
neurips,2019,1,4626,Yoav,Goldberg,biu,Bar Ilan University,yogo@cs.biu.ac.il,Learning Deterministic Weighted Automata with Queries and Counterexamples
neurips,2019,2,4626,Eran,Yahav,technion,Technion,yahave@cs.technion.ac.il,Learning Deterministic Weighted Automata with Queries and Counterexamples
neurips,2019,0,2605,Candice,Schumann,umd,University of Maryland,schumann@cs.umd.edu,Making the Cut: A Bandit-based Approach to Tiered Interviewing
neurips,2019,1,2605,Zhi,Lang,umd,"University of Maryland, College Park",zlang@cs.umd.edu,Making the Cut: A Bandit-based Approach to Tiered Interviewing
neurips,2019,2,2605,Jeffrey,Foster,tufts,Tufts University,jfoster@cs.tufts.edu,Making the Cut: A Bandit-based Approach to Tiered Interviewing
neurips,2019,3,2605,John,Dickerson,umd,University of Maryland,john@cs.umd.edu,Making the Cut: A Bandit-based Approach to Tiered Interviewing
neurips,2019,0,3995,David,Sabbagh,,INRIA,,Manifold-regression to predict from MEG/EEG brain signals without source modeling
neurips,2019,1,3995,Pierre,Ablin,,Inria,,Manifold-regression to predict from MEG/EEG brain signals without source modeling
neurips,2019,2,3995,Gael,Varoquaux,,"Parietal Team, INRIA",,Manifold-regression to predict from MEG/EEG brain signals without source modeling
neurips,2019,3,3995,Alexandre,Gramfort,,INRIA,,Manifold-regression to predict from MEG/EEG brain signals without source modeling
neurips,2019,4,3995,Denis,Engemann,,INRIA Saclay,,Manifold-regression to predict from MEG/EEG brain signals without source modeling
neurips,2019,0,8242,Youwei,Lyu,gmail,Beijing University of Posts and Telecommunications,youweilv@gmail.com,Reflection Separation using a Pair of Unpolarized and Polarized Images
neurips,2019,1,8242,Zhaopeng,Cui,gmail,ETH Zurich,zhpcui@gmail.com,Reflection Separation using a Pair of Unpolarized and Polarized Images
neurips,2019,2,8242,Si,Li,bupt,Beijing University of Posts and Telecommunications,lisi@bupt.edu.cn,Reflection Separation using a Pair of Unpolarized and Polarized Images
neurips,2019,3,8242,Marc,Pollefeys,ethz,ETH Zurich,marc.pollefeys@inf.ethz.ch,Reflection Separation using a Pair of Unpolarized and Polarized Images
neurips,2019,4,8242,Boxin,Shi,pku,Peking University,shiboxin@pku.edu.cn,Reflection Separation using a Pair of Unpolarized and Polarized Images
neurips,2019,0,3109,Tiantian,Fang,illinois,University of Illinois Urbana-Champaign,tf6@illinois.edu,Co-Generation with GANs using AIS based HMC
neurips,2019,1,3109,Alexander,Schwing,illinois,University of Illinois at Urbana-Champaign,aschwing@illinois.edu,Co-Generation with GANs using AIS based HMC
neurips,2019,0,7092,Carl,Doersch,,DeepMind,,Sim2real transfer learning for 3D human pose estimation: motion to the rescue
neurips,2019,1,7092,Andrew,Zisserman,,DeepMind & University of Oxford,,Sim2real transfer learning for 3D human pose estimation: motion to the rescue
neurips,2019,0,6257,Zheng,Li,gmail,Tsinghua University,lzlz19971997@gmail.com,Dimension-Free Bounds for Low-Precision Training
neurips,2019,1,6257,Christopher,De Sa,cornell,Cornell,cdesa@cs.cornell.edu,Dimension-Free Bounds for Low-Precision Training
neurips,2019,0,1901,Nathan,Kallus,cornell,Cornell University,kallus@cornell.edu,Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds
neurips,2019,1,1901,Angela,Zhou,cornell,Cornell University,az434@cornell.edu,Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds
neurips,2019,0,825,Thang,Vu,kaist,KAIST,thangvubk@kaist.ac.kr,Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution
neurips,2019,1,825,Hyunjun,Jang,kaist,KAIST,wiseholi@kaist.ac.kr,Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution
neurips,2019,2,825,Trung,Pham,kaist,KAIST,trungpx@kaist.ac.kr,Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution
neurips,2019,3,825,Chang,Yoo,kaist,KAIST,cd_yoo@kaist.ac.kr,Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution
neurips,2019,0,7847,Adam,Foster,,University of Oxford,,Variational Bayesian Optimal Experimental Design
neurips,2019,1,7847,Martin,Jankowiak,,Uber AI Labs,,Variational Bayesian Optimal Experimental Design
neurips,2019,2,7847,Elias,Bingham,,Uber AI Labs,,Variational Bayesian Optimal Experimental Design
neurips,2019,3,7847,Paul,Horsfall,,Uber AI Labs,,Variational Bayesian Optimal Experimental Design
neurips,2019,4,7847,Yee Whye,Teh,,"University of Oxford, DeepMind",,Variational Bayesian Optimal Experimental Design
neurips,2019,5,7847,Thomas,Rainforth,,University of Oxford,,Variational Bayesian Optimal Experimental Design
neurips,2019,6,7847,Noah,Goodman,,Stanford University,,Variational Bayesian Optimal Experimental Design
neurips,2019,0,8744,Joshua,Robinson,mit,MIT,joshrob@mit.edu,Flexible Modeling of Diversity with Strongly Log-Concave Distributions
neurips,2019,1,8744,Suvrit,Sra,mit,MIT,suvrit@mit.edu,Flexible Modeling of Diversity with Strongly Log-Concave Distributions
neurips,2019,2,8744,Stefanie,Jegelka,mit,MIT,stefje@csail.mit.edu,Flexible Modeling of Diversity with Strongly Log-Concave Distributions
neurips,2019,0,3418,Yiren,Wang,illinois,University of Illinois at Urbana-Champaign,1yiren@illinois.edu,Neural Machine Translation with Soft Prototype
neurips,2019,1,3418,Yingce,Xia,illinois,Microsoft Research Asia,czhai@illinois.edu,Neural Machine Translation with Soft Prototype
neurips,2019,2,3418,Fei,Tian,microsoft,Facebook,2yingce.xia@microsoft.com,Neural Machine Translation with Soft Prototype
neurips,2019,3,3418,Fei,Gao,microsoft,University of Chinese Academy of Sciences,taoqin@microsoft.com,Neural Machine Translation with Soft Prototype
neurips,2019,4,3418,Tao,Qin,microsoft,Microsoft Research,tie-yan.liu@microsoft.com,Neural Machine Translation with Soft Prototype
neurips,2019,5,3418,Cheng Xiang,Zhai,fb,University of Illinois at Urbana-Champaign,3feitia@fb.com,Neural Machine Translation with Soft Prototype
neurips,2019,6,3418,Tie-Yan,Liu,ict,Microsoft Research,4gaofei17n@ict.ac.cn,Neural Machine Translation with Soft Prototype
neurips,2019,0,5531,Allan,Jabri,,UC Berkeley,,Unsupervised Curricula for Visual Meta-Reinforcement Learning
neurips,2019,1,5531,Kyle,Hsu,,University of Toronto,,Unsupervised Curricula for Visual Meta-Reinforcement Learning
neurips,2019,2,5531,Abhishek,Gupta,,"University of California, Berkeley",,Unsupervised Curricula for Visual Meta-Reinforcement Learning
neurips,2019,3,5531,Ben,Eysenbach,,Carnegie Mellon University,,Unsupervised Curricula for Visual Meta-Reinforcement Learning
neurips,2019,4,5531,Sergey,Levine,,UC Berkeley,,Unsupervised Curricula for Visual Meta-Reinforcement Learning
neurips,2019,5,5531,Chelsea,Finn,,Stanford University,,Unsupervised Curricula for Visual Meta-Reinforcement Learning
neurips,2019,0,6487,Shinji,Ito,nec,"NEC Corporation,      University of Tokyo",i-shinji@nec.com,Improved Regret Bounds for Bandit Combinatorial Optimization
neurips,2019,1,6487,Daisuke,Hatano,tmu,RIKEN AIP,sumita@tmu.ac.jp,Improved Regret Bounds for Bandit Combinatorial Optimization
neurips,2019,2,6487,Hanna,Sumita,riken,Tokyo Metropolitan University,daisuke.hatano@riken.jp,Improved Regret Bounds for Bandit Combinatorial Optimization
neurips,2019,3,6487,Kei,Takemura,nec,NEC Corporation,kei_takemura@nec.com,Improved Regret Bounds for Bandit Combinatorial Optimization
neurips,2019,4,6487,Takuro,Fukunaga,chuo-u,"Chuo University, JST PRESTO, RIKEN AIP",fukunaga.07s@g.chuo-u.ac.jp,Improved Regret Bounds for Bandit Combinatorial Optimization
neurips,2019,5,6487,Naonori,Kakimura,keio,Keio University,kakimura@math.keio.ac.jp,Improved Regret Bounds for Bandit Combinatorial Optimization
neurips,2019,6,6487,Ken-Ichi,Kawarabayashi,nii,National Institute of Informatics,k-keniti@nii.ac.jp,Improved Regret Bounds for Bandit Combinatorial Optimization
neurips,2019,0,3162,Gi-Soo,Kim,snu,Seoul National University,gisoo1989@snu.ac.kr,Doubly-Robust Lasso Bandit
neurips,2019,1,3162,Myunghee Cho,Paik,snu,Seoul National University,myungheechopaik@snu.ac.kr,Doubly-Robust Lasso Bandit
neurips,2019,0,7426,Dexiong,Chen,inria,Inria,dexiong.chen@inria.fr,Recurrent Kernel Networks
neurips,2019,1,7426,Laurent,Jacob,univ-lyon1,CNRS,laurent.jacob@univ-lyon1.fr,Recurrent Kernel Networks
neurips,2019,2,7426,Julien,Mairal,inria,Inria,julien.mairal@inria.fr,Recurrent Kernel Networks
neurips,2019,0,2257,Tianbo,Li,ntu,Nanyang Technological University,tianbo001@e.ntu.edu.sg,Thinning for Accelerating the Learning of Point Processes
neurips,2019,1,2257,Yiping,Ke,ntu,Nanyang Technological University,ypke@ntu.edu.sg,Thinning for Accelerating the Learning of Point Processes
neurips,2019,0,4604,Necdet Serhat,Aybat,psu,Penn State University,nsa10@psu.edu,A Universally Optimal Multistage Accelerated Stochastic Gradient Method
neurips,2019,1,4604,Alireza,Fallah,rutgers,MIT,mg1366@rutgers.edu,A Universally Optimal Multistage Accelerated Stochastic Gradient Method
neurips,2019,2,4604,Mert,Gurbuzbalaban,mit,Rutgers,afallah@mit.edu,A Universally Optimal Multistage Accelerated Stochastic Gradient Method
neurips,2019,3,4604,Asuman,Ozdaglar,mit,Massachusetts Institute of Technology,asuman@mit.edu,A Universally Optimal Multistage Accelerated Stochastic Gradient Method
neurips,2019,0,39,Brian,Lubars,colorado,University of Colorado Boulder,brian.lubars@colorado.edu,"Ask not what AI can do, but what AI should do: Towards a framework of task delegability"
neurips,2019,1,39,Chenhao,Tan,colorado,University of Colorado Boulder,chenhao.tan@colorado.edu,"Ask not what AI can do, but what AI should do: Towards a framework of task delegability"
neurips,2019,0,8496,Blossom,Metevier,,"University of Massachusetts, Amherst",,Offline Contextual Bandits with High Probability Fairness Guarantees
neurips,2019,1,8496,Stephen,Giguere,,"University of Massachusetts, Amherst",,Offline Contextual Bandits with High Probability Fairness Guarantees
neurips,2019,2,8496,Sarah,Brockman,,University of Massachusetts Amherst,,Offline Contextual Bandits with High Probability Fairness Guarantees
neurips,2019,3,8496,Ari,Kobren,,UMass Amherst,,Offline Contextual Bandits with High Probability Fairness Guarantees
neurips,2019,4,8496,Yuriy,Brun,,University of Massachusetts Amherst,,Offline Contextual Bandits with High Probability Fairness Guarantees
neurips,2019,5,8496,Emma,Brunskill,,Stanford University,,Offline Contextual Bandits with High Probability Fairness Guarantees
neurips,2019,6,8496,Philip,Thomas,,University of Massachusetts Amherst,,Offline Contextual Bandits with High Probability Fairness Guarantees
neurips,2019,0,5922,Aditya,Grover,,Stanford University,,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting
neurips,2019,1,5922,Jiaming,Song,,Stanford University,,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting
neurips,2019,2,5922,Ashish,Kapoor,,Microsoft,,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting
neurips,2019,3,5922,Kenneth,Tran,,Microsoft Research,,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting
neurips,2019,4,5922,Alekh,Agarwal,,Microsoft Research,,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting
neurips,2019,5,5922,Eric,Horvitz,,Microsoft Research,,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting
neurips,2019,6,5922,Stefano,Ermon,,Stanford,,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting
neurips,2019,0,1958,Janice,Lan,uber,Uber AI,janlan@uber.com,LCA: Loss Change Allocation for Neural Network Training
neurips,2019,1,1958,Rosanne,Liu,uber,Uber AI Labs,rosanne@uber.com,LCA: Loss Change Allocation for Neural Network Training
neurips,2019,2,1958,Hattie,Zhou,uber,Uber,hattie@uber.com,LCA: Loss Change Allocation for Neural Network Training
neurips,2019,3,1958,Jason,Yosinski,uber,Uber AI; Recursion,yosinski@uber.com,LCA: Loss Change Allocation for Neural Network Training
neurips,2019,0,2692,Chen,Xing,,Montreal Institute of Learning Algorithms,,Adaptive Cross-Modal Few-shot Learning
neurips,2019,1,2692,Negar,Rostamzadeh,,Elemenet AI,,Adaptive Cross-Modal Few-shot Learning
neurips,2019,2,2692,Boris,Oreshkin,,Element AI,,Adaptive Cross-Modal Few-shot Learning
neurips,2019,3,2692,Pedro,O. Pinheiro,,Element AI,,Adaptive Cross-Modal Few-shot Learning
neurips,2019,0,609,Hedi,Hadiji,u-psud,"Laboratoire de Mathematiques dOrsay, Univ. Paris-Sud,",hedi.hadiji@math.u-psud.fr,Polynomial Cost of Adaptation for X-Armed Bandits
neurips,2019,0,4757,Axel,Brando,,BBVA DATA & ANALYTICS SL UNIVERSITAT DE BARCELONA,,Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians
neurips,2019,1,4757,Jose,Rodriguez,,BBVA Data & Analytics,,Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians
neurips,2019,2,4757,Jordi,Vitria,,Universitat de Barcelona,,Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians
neurips,2019,3,4757,Alberto,Rubio Muñoz,,BBVA Data & Analytics,,Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians
neurips,2019,0,4956,Zhitao,Ying,stanford,Stanford University,rexying@cs.stanford.edu,GNNExplainer: Generating Explanations for Graph Neural Networks
neurips,2019,1,4956,Dylan,Bourgeois,stanford,EPFL,dtsbourg@cs.stanford.edu,GNNExplainer: Generating Explanations for Graph Neural Networks
neurips,2019,2,4956,Jiaxuan,You,stanford,Stanford University,jiaxuan@cs.stanford.edu,GNNExplainer: Generating Explanations for Graph Neural Networks
neurips,2019,3,4956,Marinka,Zitnik,stanford,Stanford University,marinka@cs.stanford.edu,GNNExplainer: Generating Explanations for Graph Neural Networks
neurips,2019,4,4956,Jure,Leskovec,stanford,Stanford University and Pinterest,jure@cs.stanford.edu,GNNExplainer: Generating Explanations for Graph Neural Networks
neurips,2019,0,8478,Wei,Ma,cmu,Carnegie Mellon University,weima@cmu.edu,Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption
neurips,2019,1,8478,George,Chen,cmu,Carnegie Mellon University,georgechen@cmu.edu,Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption
neurips,2019,0,53,Matthias,Minderer,google,Google Research,mjlm@google.com,Unsupervised learning of object structure and dynamics from videos
neurips,2019,1,53,Chen,Sun,google,Google Research,chensun@google.com,Unsupervised learning of object structure and dynamics from videos
neurips,2019,2,53,Ruben,Villegas,google,Adobe Research / U. Michigan,rubville@google.com,Unsupervised learning of object structure and dynamics from videos
neurips,2019,3,53,Forrester,Cole,google,Google Research,fcole@google.com,Unsupervised learning of object structure and dynamics from videos
neurips,2019,4,53,Kevin,Murphy,google,Google,kpmurphy@google.com,Unsupervised learning of object structure and dynamics from videos
neurips,2019,5,53,Honglak,Lee,google,Google Brain,honglak@google.com,Unsupervised learning of object structure and dynamics from videos
neurips,2019,0,2044,Dominik,Linzner,tu-darmstadt,Technische Universität Darmstadt,dominik.linzner@bcs.tu-darmstadt.de,Scalable Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data
neurips,2019,1,2044,Michael,Schmidt,tu-darmstadt,TU Darmstadt,michael.schmidt@bcs.tu-darmstadt.de,Scalable Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data
neurips,2019,2,2044,Heinz,Koeppl,tu-darmstadt,Technische Universität Darmstadt,heinz.koeppl@bcs.tu-darmstadt.de,Scalable Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data
neurips,2019,0,765,Jianwei,Yang,,Georgia Tech,,Cross-channel Communication Networks
neurips,2019,1,765,Zhile,Ren,,Georgia Tech,,Cross-channel Communication Networks
neurips,2019,2,765,Chuang,Gan,,MIT-IBM Watson AI Lab,,Cross-channel Communication Networks
neurips,2019,3,765,Hongyuan,Zhu,,Astar,,Cross-channel Communication Networks
neurips,2019,4,765,Devi,Parikh,,Georgia Tech / Facebook AI Research (FAIR),,Cross-channel Communication Networks
neurips,2019,0,1057,Haichao,Zhang,gmail,Horizon Robotics,hczhang1@gmail.com,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training
neurips,2019,1,1057,Jianyu,Wang,gmail,Baidu USA,wjyouch@gmail.com,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training
neurips,2019,0,1595,Santtu,Tikka,jyu,University of Jyväskylä,santtu.tikka@jyu.fi,Identifying Causal Effects via Context-specific Independence Relations
neurips,2019,1,1595,Antti,Hyttinen,helsinki,University of Helsinki,antti.hyttinen@helsinki.fi,Identifying Causal Effects via Context-specific Independence Relations
neurips,2019,2,1595,Juha,Karvanen,jyu,University of Jyvaskyla,juha.t.karvanen@jyu.fi,Identifying Causal Effects via Context-specific Independence Relations
neurips,2019,0,3730,Marco,Cuturi,google,Google Brain  &  CREST - ENSAE,cuturi@google.com,Differentiable Ranking and Sorting using Optimal Transport
neurips,2019,1,3730,Olivier,Teboul,google,Google Brain,oliviert@google.com,Differentiable Ranking and Sorting using Optimal Transport
neurips,2019,2,3730,Jean-Philippe,Vert,google,,jpvert@google.com,Differentiable Ranking and Sorting using Optimal Transport
neurips,2019,0,2771,Yikang,Shen,,"Mila, University of Montreal, MSR Montreal",,Ordered Memory
neurips,2019,1,2771,Shawn,Tan,,Mila,,Ordered Memory
neurips,2019,2,2771,Arian,Hosseini,,"Mila, University of Montreal, MSR Montreal",,Ordered Memory
neurips,2019,3,2771,Zhouhan,Lin,,MILA,,Ordered Memory
neurips,2019,4,2771,Alessandro,Sordoni,,Microsoft Research,,Ordered Memory
neurips,2019,5,2771,Aaron,Courville,,U. Montreal,,Ordered Memory
neurips,2019,0,4759,Jonathan,Kuck,stanford,Stanford,kuck@stanford.edu,Approximating the Permanent by Sampling from Adaptive Partitions
neurips,2019,1,4759,Tri,Dao,stanford,Stanford University,trid@stanford.edu,Approximating the Permanent by Sampling from Adaptive Partitions
neurips,2019,2,4759,Hamid,Rezatofighi,stanford,Stanford University // University of Adelaide,hamidrt@stanford.edu,Approximating the Permanent by Sampling from Adaptive Partitions
neurips,2019,3,4759,Ashish,Sabharwal,stanford,Allen Institute for AI,ermon@stanford.edu,Approximating the Permanent by Sampling from Adaptive Partitions
neurips,2019,4,4759,Stefano,Ermon,allenai,Stanford,ashishs@allenai.org,Approximating the Permanent by Sampling from Adaptive Partitions
neurips,2019,0,9119,Niru,Maheswaranathan,google,Google Brain,nirum@google.com,Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics
neurips,2019,1,9119,Alex,Williams,stanford,Stanford University,ahwillia@stanford.edu,Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics
neurips,2019,2,9119,Matthew,Golub,stanford,Stanford University,mgolub@stanford.edu,Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics
neurips,2019,3,9119,Surya,Ganguli,stanford,Stanford,sganguli@stanford.edu,Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics
neurips,2019,4,9119,David,Sussillo,google,Google Inc.,sussillo@google.com,Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics
neurips,2019,0,1565,SHUAI,ZHANG,,University of New South Wales,,Quaternion Knowledge Graph Embeddings
neurips,2019,1,1565,Yi,Tay,,Nanyang Technological University,,Quaternion Knowledge Graph Embeddings
neurips,2019,2,1565,Lina,Yao,,UNSW,,Quaternion Knowledge Graph Embeddings
neurips,2019,3,1565,Qi,Liu,,Facebook AI Research,,Quaternion Knowledge Graph Embeddings
neurips,2019,0,1398,Rebekka,Burkholz,harvard,Harvard University,rburkholz@hsph.harvard.edu,Initialization of ReLUs for Dynamical Isometry
neurips,2019,1,1398,Alina,Dubatovka,ethz,ETH Zurich,alina.dubatovka@inf.ethz.ch,Initialization of ReLUs for Dynamical Isometry
neurips,2019,0,9203,Muhammad Waleed,Gondal,,Max Planck Institute for Intelligent Systems,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,1,9203,Manuel,Wuthrich,,Max Planck Institute for Intelligent Systems,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,2,9203,Djordje,Miladinovic,,ETH Zurich,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,3,9203,Francesco,Locatello,,ETH Zürich - MPI Tübingen,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,4,9203,Martin,Breidt,,MPI for Biological Cybernetics,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,5,9203,Valentin,Volchkov,,Max Planck Institut for Intelligent Systems,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,6,9203,Joel,Akpo,,Max Planck Institute for Intelligent Systems,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,7,9203,Olivier,Bachem,,Google Brain,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,8,9203,Bernhard,Schölkopf,,MPI for Intelligent Systems,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,9,9203,Stefan,Bauer,,MPI for Intelligent Systems,,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset
neurips,2019,0,6177,Amir,Abboud,gmail,IBM research,amir.abboud@gmail.com,Subquadratic High-Dimensional Hierarchical Clustering
neurips,2019,1,6177,Vincent,Cohen-Addad,gmail,CNRS & Sorbonne Université,vcohenad@gmail.com,Subquadratic High-Dimensional Hierarchical Clustering
neurips,2019,2,6177,Hussein,Houdrouge,polytechnique,Ecole Polytechnique,hussein.houdrouge@polytechnique.edu,Subquadratic High-Dimensional Hierarchical Clustering
neurips,2019,0,8002,Thijs,Vogels,epfl,EPFL,thijs.vogels@epfl.ch,PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization
neurips,2019,1,8002,Sai Praneeth,Karimireddy,epfl,EPFL,sai.karimrieddy@epfl.ch,PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization
neurips,2019,2,8002,Martin,Jaggi,epfl,EPFL,martin.jaggi@epfl.ch,PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization
neurips,2019,0,6022,Anmol,Kagrecha,,Indian Institute of Technology Bombay,,"Distribution oblivious, risk-aware algorithms for multi-armed   bandits with unbounded rewards"
neurips,2019,1,6022,Jayakrishnan,Nair,,"""Assist. Prof, EE, IIT Bombay""",,"Distribution oblivious, risk-aware algorithms for multi-armed   bandits with unbounded rewards"
neurips,2019,2,6022,Krishna,Jagannathan,,IIT Madras,,"Distribution oblivious, risk-aware algorithms for multi-armed   bandits with unbounded rewards"
neurips,2019,0,5615,Aditya,Menon,,Google,,Multilabel reductions: what is my loss optimising?
neurips,2019,1,5615,Ankit Singh,Rawat,,Google,,Multilabel reductions: what is my loss optimising?
neurips,2019,2,5615,Sashank,Reddi,,Google,,Multilabel reductions: what is my loss optimising?
neurips,2019,3,5615,Sanjiv,Kumar,,Google,,Multilabel reductions: what is my loss optimising?
neurips,2019,0,7967,Yanis,Bahroun,rutgers,Flatiron institute,anirvans@physics.rutgers.edu,A Similarity-preserving Network Trained on Transformed Images Recapitulates Salient Features of the Fly Motion Detection Circuit
neurips,2019,1,7967,Dmitri,Chklovskii,atironinstitute,"Flatiron Institute, Simons Foundation",ybahroun@atironinstitute.org,A Similarity-preserving Network Trained on Transformed Images Recapitulates Salient Features of the Fly Motion Detection Circuit
neurips,2019,2,7967,Anirvan,Sengupta,atironinstitute,Rutgers University,dchklovskii@atironinstitute.org,A Similarity-preserving Network Trained on Transformed Images Recapitulates Salient Features of the Fly Motion Detection Circuit
neurips,2019,0,1179,Wei-Da,Chen,nthu,National Tsing Hua University,wdchen@datalab.cs.nthu.edu.tw,CNN^{2}: Viewpoint Generalization via a Binocular Vision
neurips,2019,1,1179,Shan-Hung (Brandon),Wu,nthu,National Tsing Hua University,shwu@cs.nthu.edu.tw,CNN^{2}: Viewpoint Generalization via a Binocular Vision
neurips,2019,0,5717,Tejas,Kulkarni,google,DeepMind,tkulkarni@google.com,Unsupervised Learning of Object Keypoints for Perception and Control
neurips,2019,1,5717,Ankush,Gupta,google,DeepMind,ankushgupta@google.com,Unsupervised Learning of Object Keypoints for Perception and Control
neurips,2019,2,5717,Catalin,Ionescu,google,Deepmind,cdi@google.com,Unsupervised Learning of Object Keypoints for Perception and Control
neurips,2019,3,5717,Sebastian,Borgeaud,google,DeepMind,sborgeaud@google.com,Unsupervised Learning of Object Keypoints for Perception and Control
neurips,2019,4,5717,Malcolm,Reynolds,google,DeepMind,mareynolds@google.com,Unsupervised Learning of Object Keypoints for Perception and Control
neurips,2019,5,5717,Andrew,Zisserman,google,DeepMind & University of Oxford,zisserman@google.com,Unsupervised Learning of Object Keypoints for Perception and Control
neurips,2019,6,5717,Volodymyr,Mnih,google,DeepMind,vmnih@google.com,Unsupervised Learning of Object Keypoints for Perception and Control
neurips,2019,0,5575,Jiaxuan,You,stanford,Stanford University,jiaxuan@cs.stanford.edu,G2SAT: Learning to Generate SAT Formulas
neurips,2019,1,5575,Haoze,Wu,stanford,Stanford University,haozewu@stanford.edu,G2SAT: Learning to Generate SAT Formulas
neurips,2019,2,5575,Clark,Barrett,stanford,Stanford University,barrett@cs.stanford.edu,G2SAT: Learning to Generate SAT Formulas
neurips,2019,3,5575,Raghuram,Ramanujan,davidson,Davidson College,raramanujan@davidson.edu,G2SAT: Learning to Generate SAT Formulas
neurips,2019,4,5575,Jure,Leskovec,stanford,Stanford University and Pinterest,jure@cs.stanford.edu,G2SAT: Learning to Generate SAT Formulas
neurips,2019,0,4708,Christos,Louizos,uva,University of Amsterdam,c.louizos@uva.nl,The Functional Neural Process
neurips,2019,1,4708,Xiahan,Shi,tno,Bosch Center for Artificial Intelligence,klamer.schutter@tno.nl,The Functional Neural Process
neurips,2019,2,4708,Klamer,Schutte,bosch,TNO,xiahan.shi@de.bosch.com,The Functional Neural Process
neurips,2019,3,4708,Max,Welling,uva,University of Amsterdam / Qualcomm AI Research,m.welling@uva.nl,The Functional Neural Process
neurips,2019,0,1768,Ming,Yu,,"The University of Chicago, Booth School of Business",,Convergent Policy Optimization for Safe Reinforcement Learning
neurips,2019,1,1768,Zhuoran,Yang,,Princeton University,,Convergent Policy Optimization for Safe Reinforcement Learning
neurips,2019,2,1768,Mladen,Kolar,,University of Chicago,,Convergent Policy Optimization for Safe Reinforcement Learning
neurips,2019,3,1768,Zhaoran,Wang,,Northwestern University,,Convergent Policy Optimization for Safe Reinforcement Learning
neurips,2019,0,2956,Shen-Huan,Lyu,nju,Nanjing University,lvsh@lamda.nju.edu.cn,A Refined Margin Distribution Analysis for Forest Representation Learning
neurips,2019,1,2956,Liang,Yang,nju,Nanjing University,yangl@lamda.nju.edu.cn,A Refined Margin Distribution Analysis for Forest Representation Learning
neurips,2019,2,2956,Zhi-Hua,Zhou,nju,Nanjing University,zhouzh@lamda.nju.edu.cn,A Refined Margin Distribution Analysis for Forest Representation Learning
neurips,2019,0,3549,Ron,Shapira Weber,bgu,Ben-Gurion University,ronsha@post.bgu.ac.il,Diffeomorphic Temporal Alignment Nets
neurips,2019,1,3549,Matan,Eyal,bgu,Ben Gurion University,mataney@post.bgu.ac.il,Diffeomorphic Temporal Alignment Nets
neurips,2019,2,3549,Nicki,Skafte,dtu,Technical University of Denmark,nsde@dtu.dk,Diffeomorphic Temporal Alignment Nets
neurips,2019,3,3549,Oren,Shriki,bgu,Ben-Gurion University of the Negev,orenfr@cs.bgu.ac.il,Diffeomorphic Temporal Alignment Nets
neurips,2019,4,3549,Oren,Freifeld,bgu,Ben-Gurion University,shrikio@bgu.ac.il,Diffeomorphic Temporal Alignment Nets
neurips,2019,0,3965,Sicheng,Zhao,gmail,University of California Berkeley,schzhao@gmail.com,Multi-source Domain Adaptation for Semantic Segmentation
neurips,2019,1,3965,Bo,Li,gmail,Harbin Institute of Technology,drluodian@gmail.com,Multi-source Domain Adaptation for Semantic Segmentation
neurips,2019,2,3965,Xiangyu,Yue,berkeley,UC Berkeley,xyyue@berkeley.edu,Multi-source Domain Adaptation for Semantic Segmentation
neurips,2019,3,3965,Yang,Gu,berkeley,Didi chuxing,keutzer@berkeley.edu,Multi-source Domain Adaptation for Semantic Segmentation
neurips,2019,4,3965,Pengfei,Xu,didiglobal,Didi Chuxing,guyangdavid@didiglobal.com,Multi-source Domain Adaptation for Semantic Segmentation
neurips,2019,5,3965,Runbo,Hu,didiglobal,DiDi Chuxing,xupengfeipf@didiglobal.com,Multi-source Domain Adaptation for Semantic Segmentation
neurips,2019,6,3965,Hua,Chai,didiglobal,Didi Chuxing,hurunbo@didiglobal.com,Multi-source Domain Adaptation for Semantic Segmentation
neurips,2019,7,3965,Kurt,Keutzer,didiglobal,"EECS, UC Berkeley",chaihua@didiglobal.com,Multi-source Domain Adaptation for Semantic Segmentation
neurips,2019,0,2695,Ioannis,Koutis,njit,New Jersey Institute of Technology,ikoutis@njit.edu,Spectral Modification of Graphs for Improved Spectral Clustering
neurips,2019,1,2695,Huong,Le,njit,NJIT,hyl4@njit.edu,Spectral Modification of Graphs for Improved Spectral Clustering
neurips,2019,0,4444,Sanjeev,Arora,,Princeton University,,On Exact Computation with an Infinitely Wide Neural Net
neurips,2019,1,4444,Simon,Du,,Institute for Advanced Study,,On Exact Computation with an Infinitely Wide Neural Net
neurips,2019,2,4444,Wei,Hu,,Princeton University,,On Exact Computation with an Infinitely Wide Neural Net
neurips,2019,3,4444,Zhiyuan,Li,,Princeton University,,On Exact Computation with an Infinitely Wide Neural Net
neurips,2019,4,4444,Russ,Salakhutdinov,,Carnegie Mellon University,,On Exact Computation with an Infinitely Wide Neural Net
neurips,2019,5,4444,Ruosong,Wang,,Carnegie Mellon University,,On Exact Computation with an Infinitely Wide Neural Net
neurips,2019,0,9023,Chulhee,Yun,mit,MIT,chulheey@mit.edu,Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity
neurips,2019,1,9023,Suvrit,Sra,mit,MIT,suvrit@mit.edu,Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity
neurips,2019,2,9023,Ali,Jadbabaie,mit,MIT,jadbabai@mit.edu,Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity
neurips,2019,0,9014,Sam,Wiseman,ttic,TTIC,swiseman@ttic.edu,Amortized Bethe Free Energy Minimization for Learning MRFs
neurips,2019,1,9014,Yoon,Kim,harvard,Harvard University,yoonkim@seas.harvard.edu,Amortized Bethe Free Energy Minimization for Learning MRFs
neurips,2019,0,676,Fengxiang,He,sydney,The University of Sydney,fengxiang.he@sydney.edu.au,Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence
neurips,2019,1,676,Tongliang,Liu,sydney,The University of Sydney,tongliang.liu@sydney.edu.au,Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence
neurips,2019,2,676,Dacheng,Tao,sydney,University of Sydney,dacheng.tao@sydney.edu.au,Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence
neurips,2019,0,3088,Zhilin,Yang,cmu,Recurrent AI,zhiliny@cs.cmu.edu,XLNet: Generalized Autoregressive Pretraining for Language Understanding
neurips,2019,1,3088,Zihang,Dai,cmu,Carnegie Mellon University,dzihang@cs.cmu.edu,XLNet: Generalized Autoregressive Pretraining for Language Understanding
neurips,2019,2,3088,Yiming,Yang,cmu,CMU,yiming@cs.cmu.edu,XLNet: Generalized Autoregressive Pretraining for Language Understanding
neurips,2019,3,3088,Jaime,Carbonell,cmu,CMU,jgc@cs.cmu.edu,XLNet: Generalized Autoregressive Pretraining for Language Understanding
neurips,2019,4,3088,Russ,Salakhutdinov,cmu,Carnegie Mellon University,rsalakhu@cs.cmu.edu,XLNet: Generalized Autoregressive Pretraining for Language Understanding
neurips,2019,5,3088,Quoc,Le,google,Google,qvl@google.com,XLNet: Generalized Autoregressive Pretraining for Language Understanding
neurips,2019,0,1295,Alexis,Bellot,,University of Cambridge / Alan Turing Institute,,Conditional Independence Testing using Generative Adversarial Networks
neurips,2019,1,1295,Mihaela,van der Schaar,,"University of Cambridge, Alan Turing Institute and UCLA",,Conditional Independence Testing using Generative Adversarial Networks
neurips,2019,0,1322,Xindian,Ma,tju,Tianjin University,xindianma@tju.edu.cn,A Tensorized Transformer for Language Modeling
neurips,2019,1,1322,Peng,Zhang,tju,Tianjin University,pzhang@tju.edu.cn,A Tensorized Transformer for Language Modeling
neurips,2019,2,1322,Shuai,Zhang,tju,Tianjin University,szhang96@tju.edu.cn,A Tensorized Transformer for Language Modeling
neurips,2019,3,1322,Nan,Duan,tju,Microsoft Research Asia,yxhou@tju.edu.cn,A Tensorized Transformer for Language Modeling
neurips,2019,4,1322,Yuexian,Hou,microsoft,Tianjin University,nanduan@microsoft.com,A Tensorized Transformer for Language Modeling
neurips,2019,5,1322,Ming,Zhou,microsoft,Microsoft Research,mingzhou@microsoft.com,A Tensorized Transformer for Language Modeling
neurips,2019,6,1322,Dawei,Song,bit,Beijing Institute of Technology,dwsong@bit.edu.cn,A Tensorized Transformer for Language Modeling
neurips,2019,0,1589,Sascha,Saralajew,,Dr. Ing. h.c. F. Porsche AG,,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components
neurips,2019,1,1589,Lars,Holdijk,,Radboud University Nijmegen,,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components
neurips,2019,2,1589,Maike,Rees,,Dr. Ing. h.c. F. Porsche AG,,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components
neurips,2019,3,1589,Ebubekir,Asan,,Porsche AG,,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components
neurips,2019,4,1589,Thomas,Villmann,,University of Applied Sciences Mittweida,,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components
neurips,2019,0,4724,Robin,Sandkühler,unibas,University of Basel,robin.sandkuehler@unibas.ch,Recurrent Registration Neural Networks for Deformable Image Registration
neurips,2019,1,4724,Simon,Andermatt,unibas,Center for medical Image Analysis and Navigation,simon.andermatt@unibas.ch,Recurrent Registration Neural Networks for Deformable Image Registration
neurips,2019,2,4724,Grzegorz,Bauman,usb,University of Basel Hospital,grzegorz.bauman@usb.ch,Recurrent Registration Neural Networks for Deformable Image Registration
neurips,2019,3,4724,Sylvia,Nyilas,insel,Bern University Hospital,sylvia.nyilas@insel.ch,Recurrent Registration Neural Networks for Deformable Image Registration
neurips,2019,4,4724,Christoph,Jud,unibas,University of Basel,christoph.jud@unibas.ch,Recurrent Registration Neural Networks for Deformable Image Registration
neurips,2019,5,4724,Philippe C.,Cattin,unibas,University of Basel,philippe.cattin@unibas.ch,Recurrent Registration Neural Networks for Deformable Image Registration
neurips,2019,0,7860,Dirk,van der Hoeven,gmail,Leiden University,dirkvderhoeven@gmail.com,User-Specified Local Differential Privacy in Unconstrained Adaptive Online Learning
neurips,2019,0,9009,Philip,Bachman,gmail,Microsoft Research,phil.bachman@gmail.com,Learning Representations by Maximizing Mutual Information Across Views
neurips,2019,1,9009,R Devon,Hjelm,microsoft,Microsoft Research,devon.hjelm@microsoft.com,Learning Representations by Maximizing Mutual Information Across Views
neurips,2019,2,9009,William,Buchwalter,microsoft,Microsoft,wibuch@microsoft.com,Learning Representations by Maximizing Mutual Information Across Views
neurips,2019,0,2710,Jian,QIAN,ens,INRIA Lille - Sequel Team,jian.qian@ens.fr,Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs
neurips,2019,1,2710,Ronan,Fruit,inria,Inria Lille,ronan.fruit@inria.fr,Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs
neurips,2019,2,2710,Matteo,Pirotta,fb,Facebook AI Research,pirotta@fb.com,Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs
neurips,2019,3,2710,Alessandro,Lazaric,fb,Facebook Artificial Intelligence Research,lazaric@fb.com,Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs
neurips,2019,0,7610,Eszter,Vértes,ucl,"Gatsby Unit, UCL",eszter@gatsby.ucl.ac.uk,A neurally plausible model learns successor representations in partially observable environments
neurips,2019,1,7610,Maneesh,Sahani,ucl,"Gatsby Unit, UCL",maneesh@gatsby.ucl.ac.uk,A neurally plausible model learns successor representations in partially observable environments
neurips,2019,0,1973,PHUONG_HA,NGUYEN,gmail,University of Connecticut (UCONN),phuongha.ntu@gmail.com,Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD
neurips,2019,1,1973,Lam,Nguyen,ibm,"IBM Research, Thomas J. Watson Research Center",LamNguyen.MLTD@ibm.com,Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD
neurips,2019,2,1973,Marten,van Dijk,uconn,University of Connecticut,marten.van_dijk@uconn.edu,Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD
neurips,2019,0,2701,Shali,Jiang,wustl,Washington University in St. Louis,jiang.s@wustl.edu,Cost Effective Active Search
neurips,2019,1,2701,Roman,Garnett,cmu,Washington University in St. Louis,moseleyb@andrew.cmu.edu,Cost Effective Active Search
neurips,2019,2,2701,Benjamin,Moseley,wustl,Carnegie Mellon University,garnett@wustl.edu,Cost Effective Active Search
neurips,2019,0,739,Dominic,Richards,ox,University of Oxford,dominic.richards@spc.ox.ac.uk,Optimal Statistical Rates for Decentralised Non-Parametric Regression with Linear Speed-Up
neurips,2019,1,739,Patrick,Rebeschini,ox,University of Oxford,patrick.rebeschini@stats.ox.ac.uk,Optimal Statistical Rates for Decentralised Non-Parametric Regression with Linear Speed-Up
neurips,2019,0,7210,Rodolfo,Corona Rodriguez,,UC Berkeley,,Modeling Conceptual Understanding in Image Reference Games
neurips,2019,1,7210,Stephan,Alaniz,,Max Planck Institute for Informatics,,Modeling Conceptual Understanding in Image Reference Games
neurips,2019,2,7210,Zeynep,Akata,,University of Amsterdam,,Modeling Conceptual Understanding in Image Reference Games
neurips,2019,0,1835,Georgios,Detorakis,uci,"University of California, Irvine",gdetorak@uci.edu,Inherent Weight Normalization in Stochastic Neural Networks
neurips,2019,1,1835,Sourav,Dutta,nd,Univ. Notre Dame,sdutta4@nd.edu,Inherent Weight Normalization in Stochastic Neural Networks
neurips,2019,2,1835,Abhishek,Khanna,nd,Univ. Notre Dame,akhanna@nd.edu,Inherent Weight Normalization in Stochastic Neural Networks
neurips,2019,3,1835,Matthew,Jerry,nd,Univ. Notre Dame,mjerry@alumni.nd.edu,Inherent Weight Normalization in Stochastic Neural Networks
neurips,2019,4,1835,Suman,Datta,nd,Univ. Notre Dame,sdatta@nd.edu,Inherent Weight Normalization in Stochastic Neural Networks
neurips,2019,5,1835,Emre,Neftci,uci,UC Irvine,eneftci@uci.edu,Inherent Weight Normalization in Stochastic Neural Networks
neurips,2019,0,6710,Ehsan,Abbasi,caltech,Caltech,eabbasi@caltech.edu,Universality in Learning from Linear Measurements
neurips,2019,1,6710,Fariborz,Salehi,caltech,California Institute of Technology,fsalehi@caltech.edu,Universality in Learning from Linear Measurements
neurips,2019,2,6710,Babak,Hassibi,caltech,Caltech,hassibi@caltech.edu,Universality in Learning from Linear Measurements
neurips,2019,0,1269,Faidra Georgia,Monachou,,Stanford University,,Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design
neurips,2019,1,1269,Itai,Ashlagi,,Stanford,,Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design
neurips,2019,0,8139,Saurabh,Sihag,,Rensselaer Polytechnic Institute,,Structure Learning with Side Information: Sample Complexity
neurips,2019,1,8139,Ali,Tajer,,Rensselaer Polytechnic Institute,,Structure Learning with Side Information: Sample Complexity
neurips,2019,0,8312,Dustin,Tran,,Google Brain,,Discrete Flows: Invertible Generative Models of Discrete Data
neurips,2019,1,8312,Keyon,Vafa,,Columbia University,,Discrete Flows: Invertible Generative Models of Discrete Data
neurips,2019,2,8312,Kumar,Agrawal,,Google AI Resident,,Discrete Flows: Invertible Generative Models of Discrete Data
neurips,2019,3,8312,Laurent,Dinh,,Google Brain,,Discrete Flows: Invertible Generative Models of Discrete Data
neurips,2019,4,8312,Ben,Poole,,Google Brain,,Discrete Flows: Invertible Generative Models of Discrete Data
neurips,2019,0,1331,Amir,Dezfouli,,"Data61, CSIRO",,Disentangled behavioural representations
neurips,2019,1,1331,Hassan,Ashtiani,,McMaster University,,Disentangled behavioural representations
neurips,2019,2,1331,Omar,Ghattas,,University of Chicago,,Disentangled behavioural representations
neurips,2019,3,1331,Richard,Nock,,"Data61, the Australian National University and the University of Sydney",,Disentangled behavioural representations
neurips,2019,4,1331,Peter,Dayan,,Max Planck Institute for Biological Cybernetics,,Disentangled behavioural representations
neurips,2019,5,1331,Cheng Soon,Ong,,Data61 and Australian National University,,Disentangled behavioural representations
neurips,2019,0,1831,Jiaqi,Ma,umich,University of Michigan,jiaqima@umich.edu,A Flexible Generative Framework for Graph-based Semi-supervised Learning
neurips,2019,1,1831,Weijing,Tang,umich,University of Michigan,weijtang@umich.edu,A Flexible Generative Framework for Graph-based Semi-supervised Learning
neurips,2019,2,1831,Ji,Zhu,umich,University of Michigan,jizhu@umich.edu,A Flexible Generative Framework for Graph-based Semi-supervised Learning
neurips,2019,3,1831,Qiaozhu,Mei,umich,University of Michigan,qmei@umich.edu,A Flexible Generative Framework for Graph-based Semi-supervised Learning
neurips,2019,0,7813,Lalit,Jain,washington,University of Washington,lalitj@cs.washington.edu,A New Perspective on Pool-Based Active Classification and False-Discovery Control
neurips,2019,1,7813,Kevin,Jamieson,washington,U Washington,jamieson@cs.washington.edu,A New Perspective on Pool-Based Active Classification and False-Discovery Control
neurips,2019,0,7179,Giulia,Denevi,iit,IIT & UNIGE,giulia.denevi@iit.it,Online-Within-Online Meta-Learning
neurips,2019,1,7179,Dimitris,Stamos,imperial,University College London,c.ciliberto@imperial.ac.uk,Online-Within-Online Meta-Learning
neurips,2019,2,7179,Carlo,Ciliberto,ucl,Imperial College London,d.stamos.12@ucl.ac.uk,Online-Within-Online Meta-Learning
neurips,2019,3,7179,Massimiliano,Pontil,ucl,IIT & UCL,m.pontil@ucl.ac.uk,Online-Within-Online Meta-Learning
neurips,2019,0,4461,Guodong,Zhang,,University of Toronto,,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model
neurips,2019,1,4461,Lala,Li,,Google,,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model
neurips,2019,2,4461,Zachary,Nado,,Google Inc.,,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model
neurips,2019,3,4461,James,Martens,,DeepMind,,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model
neurips,2019,4,4461,Sushant,Sachdeva,,University of Toronto,,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model
neurips,2019,5,4461,George,Dahl,,Google Brain,,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model
neurips,2019,6,4461,Chris,Shallue,,Google Brain,,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model
neurips,2019,7,4461,Roger,Grosse,,University of Toronto,,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model
neurips,2019,0,5078,Hunter,Lang,microsoft,Microsoft Research,hunter.lang@microsoft.com,Using Statistics to Automate Stochastic Optimization
neurips,2019,1,5078,Lin,Xiao,microsoft,Microsoft Research,penzhan@microsoft.com,Using Statistics to Automate Stochastic Optimization
neurips,2019,2,5078,Pengchuan,Zhang,microsoft,Microsoft Research,lin.xiao@microsoft.com,Using Statistics to Automate Stochastic Optimization
neurips,2019,0,6431,Allan,Grønlund,,"Aarhus University, MADALGO",,Margin-Based Generalization Lower Bounds for Boosted Classifiers
neurips,2019,1,6431,Lior,Kamma,,Aarhus University,,Margin-Based Generalization Lower Bounds for Boosted Classifiers
neurips,2019,2,6431,Kasper,Green Larsen,,Aarhus University,,Margin-Based Generalization Lower Bounds for Boosted Classifiers
neurips,2019,3,6431,Alexander,Mathiasen,,Aarhus University,,Margin-Based Generalization Lower Bounds for Boosted Classifiers
neurips,2019,4,6431,Jelani,Nelson,,UC Berkeley,,Margin-Based Generalization Lower Bounds for Boosted Classifiers
neurips,2019,0,910,Muhan,Zhang,wustl,Washington University; Facebook (now),muhan@wustl.edu,D-VAE: A Variational Autoencoder for Directed Acyclic Graphs
neurips,2019,1,910,Shali,Jiang,wustl,Washington University in St. Louis,jiang.s@wustl.edu,D-VAE: A Variational Autoencoder for Directed Acyclic Graphs
neurips,2019,2,910,Zhicheng,Cui,wustl,Washington University in St. Louis,z.cui@wustl.edu,D-VAE: A Variational Autoencoder for Directed Acyclic Graphs
neurips,2019,3,910,Roman,Garnett,wustl,Washington University in St. Louis,garnett@wustl.edu,D-VAE: A Variational Autoencoder for Directed Acyclic Graphs
neurips,2019,4,910,Yixin,Chen,wustl,Washington University in St. Louis,chen@cse.wustl.edu,D-VAE: A Variational Autoencoder for Directed Acyclic Graphs
neurips,2019,0,71,Andrew,Ilyas,mit,MIT,ailyas@mit.edu,"Adversarial Examples Are Not Bugs, They Are Features"
neurips,2019,1,71,Shibani,Santurkar,mit,MIT,shibani@mit.edu,"Adversarial Examples Are Not Bugs, They Are Features"
neurips,2019,2,71,Dimitris,Tsipras,mit,MIT,tsipras@mit.edu,"Adversarial Examples Are Not Bugs, They Are Features"
neurips,2019,3,71,Logan,Engstrom,mit,MIT,engstrom@mit.edu,"Adversarial Examples Are Not Bugs, They Are Features"
neurips,2019,4,71,Brandon,Tran,mit,Massachusetts Institute of Technology,btran115@mit.edu,"Adversarial Examples Are Not Bugs, They Are Features"
neurips,2019,5,71,Aleksander,Madry,mit,MIT,madry@mit.edu,"Adversarial Examples Are Not Bugs, They Are Features"
neurips,2019,0,4586,Bin,Hu,,University of Illinois at Urbana-Champaign,,Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory
neurips,2019,1,4586,Usman,Syed,,University of Illinois Urbana Champaign,,Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory
neurips,2019,0,2869,Yiqi,Zhong,usc,University of Southern California,choyingw@usc.edu,Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion
neurips,2019,1,2869,Cho-Ying,Wu,usc,University of Southern California,yiqizhon@usc.edu,Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion
neurips,2019,2,2869,Suya,You,mail,US Army Research Laboratory,suya.you.civ@mail.mil,Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion
neurips,2019,3,2869,Ulrich,Neumann,usc,USC,uneumann@usc.edu,Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion
neurips,2019,0,7793,Maximilian,Igl,,University of Oxford,,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
neurips,2019,1,7793,Kamil,Ciosek,,Microsoft,,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
neurips,2019,2,7793,Yingzhen,Li,,Microsoft Research Cambridge,,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
neurips,2019,3,7793,Sebastian,Tschiatschek,,Microsoft Research,,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
neurips,2019,4,7793,Cheng,Zhang,,"Microsoft Research, Cambridge, UK",,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
neurips,2019,5,7793,Sam,Devlin,,Microsoft Research,,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
neurips,2019,6,7793,Katja,Hofmann,,Microsoft Research,,Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
neurips,2019,0,8140,Cory,Stephenson,intel,Intel,cory.stephenson@intel.com,Untangling in Invariant Speech Recognition
neurips,2019,1,8140,Jenelle,Feather,mit,MIT,jfeather@mit.edu,Untangling in Invariant Speech Recognition
neurips,2019,2,8140,Suchismita,Padhy,intel,Intel AI Lab,suchismita.padhy@intel.com,Untangling in Invariant Speech Recognition
neurips,2019,3,8140,Oguz,Elibol,intel,Intel AI Lab,oguz.h.elibol@intel.com,Untangling in Invariant Speech Recognition
neurips,2019,4,8140,Hanlin,Tang,intel,Intel AI Products Group,hanlin.tang@intel.com,Untangling in Invariant Speech Recognition
neurips,2019,5,8140,Josh,McDermott,mit,Massachusetts Institute of Technology,jhm@mit.edu,Untangling in Invariant Speech Recognition
neurips,2019,6,8140,SueYeon,Chung,mit,Columbia/MIT,sueyeon@mit.edu,Untangling in Invariant Speech Recognition
neurips,2019,0,9031,Greg,Ver Steeg,isi,USC Information Sciences Institute,gregv@isi.edu,Fast structure learning with modular regularization
neurips,2019,1,9031,Hrayr,Harutyunyan,usc,USC Information Sciences Institute,moyerd@usc.edu,Fast structure learning with modular regularization
neurips,2019,2,9031,Daniel,Moyer,isi,USC Information Sciences Institute,hrayrh@isi.edu,Fast structure learning with modular regularization
neurips,2019,3,9031,Aram,Galstyan,isi,USC Information Sciences Institute,galstyan@isi.edu,Fast structure learning with modular regularization
neurips,2019,0,3634,Roi,Livni,tau,Tel Aviv University,rlivni@tauex.tau.ac.il,Graph-based Discriminators: Sample Complexity and Expressiveness
neurips,2019,1,3634,Yishay,Mansour,gmail,Tel Aviv University / Google,mansour.yishay@gmail.com,Graph-based Discriminators: Sample Complexity and Expressiveness
neurips,2019,0,4514,Aleksandar,Bojchevski,tum,Technical University of Munich,a.bojchevski@in.tum.de,Certifiable Robustness to Graph Perturbations
neurips,2019,1,4514,Stephan,Günnemann,tum,Technical University of Munich,guennemann@in.tum.de,Certifiable Robustness to Graph Perturbations
neurips,2019,0,8574,Ganlin,Song,yale,Yale University,ganlin.song@yale.edu,Surfing: Iterative Optimization Over Incrementally Trained Deep Networks
neurips,2019,1,8574,Zhou,Fan,yale,Yale Univ,zhou.fan@yale.edu,Surfing: Iterative Optimization Over Incrementally Trained Deep Networks
neurips,2019,2,8574,John,Lafferty,yale,Yale University,john.lafferty@yale.edu,Surfing: Iterative Optimization Over Incrementally Trained Deep Networks
neurips,2019,0,5750,Xingye,Qiao,binghamton,Binghamton University,qiao@math.binghamton.edu,Rates of Convergence for Large-scale Nearest Neighbor Classification
neurips,2019,1,5750,Jiexin,Duan,purdue,Purdue University,duan32@purdue.edu,Rates of Convergence for Large-scale Nearest Neighbor Classification
neurips,2019,2,5750,Guang,Cheng,purdue,Purdue University,chengg@purdue.edu,Rates of Convergence for Large-scale Nearest Neighbor Classification
neurips,2019,0,2626,Harsh,Gupta,illinois,University of Illinois at Urbana-Champaign,hgupta10@illinois.edu,Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning
neurips,2019,1,2626,R.,Srikant,illinois,University of Illinois at Urbana-Champaign,rsrikant@illinois.edu,Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning
neurips,2019,2,2626,Lei,Ying,umich,ASU,leiying@umich.edu,Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning
neurips,2019,0,2415,Christopher,Nemeth,lancaster,Lancaster University,c.nemeth@lancaster.ac.uk,Pseudo-Extended Markov chain Monte Carlo
neurips,2019,1,2415,Fredrik,Lindsten,liu,Linköping University,fredrik.lindsten@liu.se,Pseudo-Extended Markov chain Monte Carlo
neurips,2019,2,2415,Maurizio,Filippone,eurecom,EURECOM,maurizio.filippone@eurecom.fr,Pseudo-Extended Markov chain Monte Carlo
neurips,2019,3,2415,James,Hensman,prowler,PROWLER.io,james@prowler.io,Pseudo-Extended Markov chain Monte Carlo
neurips,2019,0,7442,John,Lee,gatech,Georgia Institute of Technology,john.lee@gatech.edu,Hierarchical Optimal Transport for Multimodal Distribution Alignment
neurips,2019,1,7442,Max,Dabagia,gatech,Georgia Institute of Technology,maxdabagia@gatech.edu,Hierarchical Optimal Transport for Multimodal Distribution Alignment
neurips,2019,2,7442,Eva,Dyer,gatech,Georgia Institute of Technology,evadyer@gatech.edu,Hierarchical Optimal Transport for Multimodal Distribution Alignment
neurips,2019,3,7442,Christopher,Rozell,gatech,Georgia Institute of Technology,crozell@gatech.edu,Hierarchical Optimal Transport for Multimodal Distribution Alignment
neurips,2019,0,7742,Ankit Singh,Rawat,google,Google Research,ankitsrawat@google.com,Sampled Softmax with Random Fourier Features
neurips,2019,1,7742,Jiecao,Chen,google,Google Research,chenjiecao@google.com,Sampled Softmax with Random Fourier Features
neurips,2019,2,7742,Felix Xinnan,Yu,google,Google Research,felixyu@google.com,Sampled Softmax with Random Fourier Features
neurips,2019,3,7742,Ananda Theertha,Suresh,google,Google,theertha@google.com,Sampled Softmax with Random Fourier Features
neurips,2019,4,7742,Sanjiv,Kumar,google,Google Research,sanjivk@google.com,Sampled Softmax with Random Fourier Features
neurips,2019,0,1663,Sivan,Sabato,bgu,Ben-Gurion University of the Negev,sabatos@cs.bgu.ac.il,Epsilon-Best-Arm Identification in Pay-Per-Reward Multi-Armed Bandits
neurips,2019,0,7360,Raman,Arora,jhu,Johns Hopkins University,arora@cs.jhu.edu,On Differentially Private Graph Sparsification and Applications
neurips,2019,1,7360,Jalaj,Upadhyay,gmail,Apple,jalaj.kumar.upadhyay@gmail.com,On Differentially Private Graph Sparsification and Applications
neurips,2019,0,2794,Ji,Xu,columbia,Columbia University,jixu@cs.columbia.edu,On the number of variables to use in principal component regression
neurips,2019,1,2794,Daniel,Hsu,columbia,Columbia University,djhsu@cs.columbia.edu,On the number of variables to use in principal component regression
neurips,2019,0,4168,Taeyoung,Hahn,snu,SNUVL,taeyounghahn@snu.ac.kr,Self-Routing Capsule Networks
neurips,2019,1,4168,Myeongjang,Pyeon,snu,Seoul National University,mjpyeon@snu.ac.kr,Self-Routing Capsule Networks
neurips,2019,2,4168,Gunhee,Kim,snu,Seoul National University,gunhee@snu.ac.kr,Self-Routing Capsule Networks
neurips,2019,0,5724,Xueying,Bai,stonybrook,Stony Brook University,xubai@cs.stonybrook.edu,A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation
neurips,2019,1,5724,Jian,Guan,tsinghua,Tsinghua University,j-guan19@mails.tsinghua.edu.cn,A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation
neurips,2019,2,5724,Hongning,Wang,virginia,University of Virginia,hw5x@virginia.edu,A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation
neurips,2019,0,5,Risto,Vuorio,gmail,University of Michigan,vuoristo@gmail.com,Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation
neurips,2019,1,5,Shao-Hua,Sun,usc,University of Southern California,shaohuas@usc.edu,Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation
neurips,2019,2,5,Hexiang,Hu,usc,University of Southern California,hexiangh@usc.edu,Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation
neurips,2019,3,5,Joseph,Lim,usc,University of Southern California,limjj@usc.edu,Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation
neurips,2019,0,1959,Christopher,Thomas,pitt,University of Pittsburgh,chris@cs.pitt.edu,Predicting the Politics of an Image Using Webly Supervised Data
neurips,2019,1,1959,Adriana,Kovashka,pitt,University of Pittsburgh,kovashka@cs.pitt.edu,Predicting the Politics of an Image Using Webly Supervised Data
neurips,2019,0,1030,Aaron,Defazio,,Facebook AI Research,,On the Curved Geometry of Accelerated Optimization
neurips,2019,0,5830,Devansh,Arpit,,Salesforce/MILA,,How to Initialize your Network? Robust Initialization for WeightNorm & ResNets
neurips,2019,1,5830,Víctor,Campos,,Barcelona Supercomputing Center,,How to Initialize your Network? Robust Initialization for WeightNorm & ResNets
neurips,2019,2,5830,Yoshua,Bengio,,Mila - University of Montreal,,How to Initialize your Network? Robust Initialization for WeightNorm & ResNets
neurips,2019,0,3531,Bolin,Wei,gmail,Peking University,bolin.wbl@gmail.com,Code Generation as a Dual Task of Code Summarization
neurips,2019,1,3531,Ge,Li,pku,Peking University,lige@pku.edu.cn,Code Generation as a Dual Task of Code Summarization
neurips,2019,2,3531,Xin,Xia,pku,Monash University,ypfzy@pku.edu.cn,Code Generation as a Dual Task of Code Summarization
neurips,2019,3,3531,Zhiyi,Fu,pku,"Key Lab of High Confidence Software Technologies (Peking University), Ministry of Education",zhijin@pku.edu.cn,Code Generation as a Dual Task of Code Summarization
neurips,2019,4,3531,Zhi,Jin,monash,"Key Lab of High Confidence Software Technologies (Peking University), Ministry o",xin.xia@monash.edu,Code Generation as a Dual Task of Code Summarization
neurips,2019,0,692,Mitsuru,Kusumoto,preferred,"Preferred Networks, Inc.",mkusumoto@preferred.jp,A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation
neurips,2019,1,692,Takuya,Inoue,u-tokyo,University of Tokyo,inoue-takuya57@g.ecc.u-tokyo.ac.jp,A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation
neurips,2019,2,692,Gentaro,Watanabe,preferred,"Preferred Networks, Inc.",g.wtnb@preferred.jp,A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation
neurips,2019,3,692,Takuya,Akiba,preferred,"Preferred Networks, Inc.",akiba@preferred.jp,A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation
neurips,2019,4,692,Masanori,Koyama,preferred,Preferred Networks Inc.,masomatics@preferred.jp,A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation
neurips,2019,0,6333,Rahaf,Aljundi,gmail,"KU Leuven, Belgium",rahaf.aljundi@gmail.com,Gradient based sample selection for online continual learning
neurips,2019,1,6333,Min,Lin,gmail,MILA,mavenlin@gmail.com,Gradient based sample selection for online continual learning
neurips,2019,2,6333,Baptiste,Goujaud,gmail,MILA,baptiste.goujaud@gmail.com,Gradient based sample selection for online continual learning
neurips,2019,3,6333,Yoshua,Bengio,mila,Mila,yoshua.bengio@mila.quebec,Gradient based sample selection for online continual learning
neurips,2019,0,772,Carl,Yang,illinois,"University of Illinois, Urbana Champaign",jiyang3@illinois.edu,Conditional Structure Generation through Graph Variational Generative Adversarial Nets
neurips,2019,1,772,Peiye,Zhuang,illinois,UIUC,peiye@illinois.edu,Conditional Structure Generation through Graph Variational Generative Adversarial Nets
neurips,2019,2,772,Wenhan,Shi,illinois,UIUC,wenhans2@illinois.edu,Conditional Structure Generation through Graph Variational Generative Adversarial Nets
neurips,2019,3,772,Alan,Luu,illinois,UIUC,alanluu2@illinois.edu,Conditional Structure Generation through Graph Variational Generative Adversarial Nets
neurips,2019,4,772,Pan,Li,illinois,Stanford,panli2@illinois.edu,Conditional Structure Generation through Graph Variational Generative Adversarial Nets
neurips,2019,0,1107,Jun,Shu,,Xi'an Jiaotong University,,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting
neurips,2019,1,1107,Qi,Xie,,Xi'an Jiaotong University,,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting
neurips,2019,2,1107,Lixuan,Yi,,Xi'an Jiaotong University,,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting
neurips,2019,3,1107,Qian,Zhao,,Xi'an Jiaotong University,,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting
neurips,2019,4,1107,Sanping,Zhou,,Xi'an Jiaotong University,,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting
neurips,2019,5,1107,Zongben,Xu,,Xi'an Jiaotong University,,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting
neurips,2019,6,1107,Deyu,Meng,,Xi'an Jiaotong University,,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting
neurips,2019,0,1927,Seungki,Min,,Columbia Business School,,Thompson Sampling with Information Relaxation Penalties
neurips,2019,1,1927,Costis,Maglaras,,Columbia Business School,,Thompson Sampling with Information Relaxation Penalties
neurips,2019,2,1927,Ciamac,Moallemi,,Columbia University,,Thompson Sampling with Information Relaxation Penalties
neurips,2019,0,5606,Shinji,Ito,nec,"NEC Corporation,      University of Tokyo",i-shinji@nec.com,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback
neurips,2019,1,5606,Daisuke,Hatano,tmu,RIKEN AIP,sumita@tmu.ac.jp,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback
neurips,2019,2,5606,Hanna,Sumita,chuo-u,Tokyo Metropolitan University,fukunaga.07s@g.chuo-u.ac.jp,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback
neurips,2019,3,5606,Kei,Takemura,riken,NEC Corporation,daisuke.hatano@riken.jp,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback
neurips,2019,4,5606,Takuro,Fukunaga,nec,"Chuo University, JST PRESTO, RIKEN AIP",kei_takemura@nec.com,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback
neurips,2019,5,5606,Naonori,Kakimura,keio,Keio University,kakimura@math.keio.ac.jp,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback
neurips,2019,6,5606,Ken-Ichi,Kawarabayashi,nii,National Institute of Informatics,k-keniti@nii.ac.jp,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback
neurips,2019,0,8026,Honghao,Li,curie,Institut Curie,honghao.li@curie.fr,Constraint-based Causal Structure Learning with Consistent Separating Sets
neurips,2019,1,8026,Vincent,Cabeli,curie,Institut Curie,vincent.cabeli@curie.fr,Constraint-based Causal Structure Learning with Consistent Separating Sets
neurips,2019,2,8026,Nadir,Sella,curie,Institut Curie,nadir.sella@curie.fr,Constraint-based Causal Structure Learning with Consistent Separating Sets
neurips,2019,3,8026,Herve,Isambert,curie,Institut Curie,herve.isambert@curie.fr,Constraint-based Causal Structure Learning with Consistent Separating Sets
neurips,2019,0,2565,Shirin,Jalali,nokia-bell-labs,Nokia Bell Labs,shirin.jalali@nokia-bell-labs.com,Efficient Deep Approximation of GMMs
neurips,2019,1,2565,Carl,Nuzman,nokia-bell-labs,Nokia Bell Labs,carl.nuzman@nokia-bell-labs.com,Efficient Deep Approximation of GMMs
neurips,2019,2,2565,Iraj,Saniee,nokia-bell-labs,Nokia Bell Labs,iraj.saniee@nokia-bell-labs.com,Efficient Deep Approximation of GMMs
neurips,2019,0,6822,Atalanti,Mastakouri,mpg,Max Planck Institute for Intelligent Systems,amastakouri@tue.mpg.de,Selecting causal brain features with a single conditional independence test per feature
neurips,2019,1,6822,Bernhard,Schölkopf,mpg,MPI for Intelligent Systems,bs@tue.mpg.de,Selecting causal brain features with a single conditional independence test per feature
neurips,2019,2,6822,Dominik,Janzing,amazon,Amazon,janzind@amazon.com,Selecting causal brain features with a single conditional independence test per feature
neurips,2019,0,1247,Su Young,Lee,kaist,KAIST,suyoung.l@kaist.ac.kr,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update
neurips,2019,1,1247,Choi,Sungik,kaist,KAIST,si_choi@kaist.ac.kr,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update
neurips,2019,2,1247,Sae-Young,Chung,kaist,KAIST,schung@kaist.ac.kr,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update
neurips,2019,0,3565,Cheng-Chun,Hsu,sinica,Academia Sinica,hsu06118@citi.sinica.edu.tw,Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior
neurips,2019,1,3565,Kuang-Jui,Hsu,qualcomm,Qualcomm,kuangjui@qti.qualcomm.com,Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior
neurips,2019,2,3565,Chung-Chi,Tsai,qualcomm,Qualcomm,chuntsai@qti.qualcomm.com,Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior
neurips,2019,3,3565,Yen-Yu,Lin,nctu,National Chiao Tung University,lin@cs.nctu.edu.tw,Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior
neurips,2019,4,3565,Yung-Yu,Chuang,ntu,National Taiwan University,cyy@csie.ntu.edu.tw,Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior
neurips,2019,0,1695,Marcel,Hirt,,University College London,,Copula-like Variational Inference
neurips,2019,1,1695,Petros,Dellaportas,,"University College London, Athens University of Economics and Alan Turing Institute",,Copula-like Variational Inference
neurips,2019,2,1695,Alain,Durmus,,ENS Paris Saclay,,Copula-like Variational Inference
neurips,2019,0,7658,Laura,Galindez Olascoaga,kuleuven,KU Leuven,laura.galindez@esat.kuleuven.be,Towards Hardware-Aware Tractable Learning of Probabilistic Models
neurips,2019,1,7658,Wannes,Meert,kuleuven,K.U.Leuven,nimish.shah@esat.kuleuven.be,Towards Hardware-Aware Tractable Learning of Probabilistic Models
neurips,2019,2,7658,Nimish,Shah,kuleuven,KU Leuven,marian.verhelst@esat.kuleuven.be,Towards Hardware-Aware Tractable Learning of Probabilistic Models
neurips,2019,3,7658,Marian,Verhelst,kuleuven,KU Leuven,wannes.meert@cs.kuleuven.be,Towards Hardware-Aware Tractable Learning of Probabilistic Models
neurips,2019,4,7658,Guy,Van den Broeck,ucla,UCLA,guyvdb@cs.ucla.edu,Towards Hardware-Aware Tractable Learning of Probabilistic Models
neurips,2019,0,5672,Tengyu,Xu,osu,The Ohio State University,xu.3260@osu.edu,Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples
neurips,2019,1,5672,Shaofeng,Zou,buffalo,"University at Buffalo, the State University of New York",szou3@buffalo.edu,Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples
neurips,2019,2,5672,Yingbin,Liang,osu,The Ohio State University,liang.889@osu.edu,Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples
neurips,2019,0,2850,Mengye,Ren,toronto,University of Toronto / Uber ATG,mren@cs.toronto.edu,Incremental Few-Shot Learning with Attention Attractor Networks
neurips,2019,1,2850,Renjie,Liao,toronto,University of Toronto,rjliao@cs.toronto.edu,Incremental Few-Shot Learning with Attention Attractor Networks
neurips,2019,2,2850,Ethan,Fetaya,toronto,Bar Ilan University,ethanf@cs.toronto.edu,Incremental Few-Shot Learning with Attention Attractor Networks
neurips,2019,3,2850,Richard,Zemel,toronto,Vector Institute/University of Toronto,zemel@cs.toronto.edu,Incremental Few-Shot Learning with Attention Attractor Networks
neurips,2019,0,4818,Kevin,Smith,,MIT,,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations
neurips,2019,1,4818,Lingjie,Mei,,MIT,,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations
neurips,2019,2,4818,Shunyu,Yao,,Princeton University,,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations
neurips,2019,3,4818,Jiajun,Wu,,MIT,,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations
neurips,2019,4,4818,Elizabeth,Spelke,,Harvard University,,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations
neurips,2019,5,4818,Josh,Tenenbaum,,MIT,,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations
neurips,2019,6,4818,Tomer,Ullman,,Harvard,,Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations
neurips,2019,0,7987,Mostafa,Rahmani,baidu,Baidu Research,mostafarahmani@baidu.com,Outlier Detection and Robust PCA Using a Convex Measure of Innovation
neurips,2019,1,7987,Ping,Li,baidu,Baidu Research USA,liping11@baidu.com,Outlier Detection and Robust PCA Using a Convex Measure of Innovation
neurips,2019,0,5523,Raman,Arora,jhu,Johns Hopkins University,arora@cs.jhu.edu,Efficient Convex Relaxations for Streaming PCA
neurips,2019,1,5523,Teodor Vanislavov,Marinov,jhu,Johns Hopkins University,tmarino2@jhu.edu,Efficient Convex Relaxations for Streaming PCA
neurips,2019,0,742,Maria-Florina,Balcan,cmu,Carnegie Mellon University,ninamf@cs.cmu.edu,Envy-Free Classification
neurips,2019,1,742,Travis,Dick,cmu,TTIC,riteshn@cmu.edu,Envy-Free Classification
neurips,2019,2,742,Ritesh,Noothigattu,cmu,Carnegie Mellon University,tdick@cs.cmu.edu,Envy-Free Classification
neurips,2019,3,742,Ariel,Procaccia,cmu,Carnegie Mellon University,arielpro@cs.cmu.edu,Envy-Free Classification
neurips,2019,0,3328,Jie,Song,zju,Zhejiang University,sjie@zju.edu.cn,Deep Model Transferability from Attribution Maps
neurips,2019,1,3328,Yixin,Chen,zju,Zhejiang University,chenyix@zju.edu.cn,Deep Model Transferability from Attribution Maps
neurips,2019,2,3328,Xinchao,Wang,zju,Stevens Institute of Technology,chengchaoshen@zju.edu.cn,Deep Model Transferability from Attribution Maps
neurips,2019,3,3328,Chengchao,Shen,zju,Zhejiang University,brooksong@zju.edu.cn,Deep Model Transferability from Attribution Maps
neurips,2019,4,3328,Mingli,Song,stevens,Zhejiang University,xinchao.wang@stevens.edu,Deep Model Transferability from Attribution Maps
neurips,2019,0,6666,Alexander,Mott,google,DeepMind,alexmott@google.com,Towards Interpretable Reinforcement Learning Using Attention Augmented Agents
neurips,2019,1,6666,Daniel,Zoran,google,DeepMind,danielzoran@google.com,Towards Interpretable Reinforcement Learning Using Attention Augmented Agents
neurips,2019,2,6666,Mike,Chrzanowski,google,Google Brain,chrzanowski@google.com,Towards Interpretable Reinforcement Learning Using Attention Augmented Agents
neurips,2019,3,6666,Daan,Wierstra,google,DeepMind Technologies,wierstra@google.com,Towards Interpretable Reinforcement Learning Using Attention Augmented Agents
neurips,2019,4,6666,Danilo,Jimenez Rezende,google,Google DeepMind,danilor@google.com,Towards Interpretable Reinforcement Learning Using Attention Augmented Agents
neurips,2019,0,2885,Adam,Gaier,h-brs,Google / Inria / H-BRS,adam.gaier@h-brs.de,Weight Agnostic Neural Networks
neurips,2019,1,2885,David,Ha,google,Google Brain,hadavid@google.com,Weight Agnostic Neural Networks
neurips,2019,0,8774,Matthieu,SIMEONI,ibm,IBM Research / EPFL,meo@zurich.ibm.com,DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging
neurips,2019,1,8774,Sepand,Kashani,westernsydney,EPFL,paul.hurley@westernsydney.edu.au,DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging
neurips,2019,2,8774,Paul,Hurley,epfl,Western Sydney University,sepand.kashani@epfl.ch,DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging
neurips,2019,3,8774,Martin,Vetterli,epfl,EPFL,martin.vetterli@epfl.ch,DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging
neurips,2019,0,1614,Sulaiman,Alghunaim,ucla,UCLA,salghunaim@ucla.edu,A Linearly Convergent Proximal Gradient Algorithm for Decentralized  Optimization
neurips,2019,1,1614,Kun,Yuan,ucla,Alibaba Inc.,kunyuan@ucla.edu,A Linearly Convergent Proximal Gradient Algorithm for Decentralized  Optimization
neurips,2019,2,1614,Ali,Sayed,epfl,Ecole Polytechnique Fédérale de Lausanne,ali.sayed@epfl.ch,A Linearly Convergent Proximal Gradient Algorithm for Decentralized  Optimization
neurips,2019,0,6001,Albert,Shaw,,Tesla,,Meta Architecture Search
neurips,2019,1,6001,Wei,Wei,,Google AI,,Meta Architecture Search
neurips,2019,2,6001,Weiyang,Liu,,Georgia Institute of Technology,,Meta Architecture Search
neurips,2019,3,6001,Le,Song,,Georgia Institute of Technology,,Meta Architecture Search
neurips,2019,4,6001,Bo,Dai,,Google Brain,,Meta Architecture Search
neurips,2019,0,2485,Yue,Yu,tsinghua,Tsinghua University,yu-y14@mails.tsinghua.edu.cn,Double Quantization for Communication-Efficient Distributed Optimization
neurips,2019,1,2485,Jiaxiang,Wu,tencent,Tencent AI Lab,jonathanwu@tencent.com,Double Quantization for Communication-Efficient Distributed Optimization
neurips,2019,2,2485,Longbo,Huang,tsinghua,"IIIS, Tsinghua Univeristy",longbohuang@tsinghua.edu.cn,Double Quantization for Communication-Efficient Distributed Optimization
neurips,2019,0,4676,Colin,Graber,illinois,University of Illinois at Urbana-Champaign,cgraber2@illinois.edu,Graph Structured Prediction Energy Networks
neurips,2019,1,4676,Alexander,Schwing,illinois,University of Illinois at Urbana-Champaign,aschwing@illinois.edu,Graph Structured Prediction Energy Networks
neurips,2019,0,3832,Nicolas,Keriven,ens,Ecole Normale Supérieure,nicolas.keriven@ens.fr,Universal Invariant and Equivariant Graph Neural Networks
neurips,2019,1,3832,Gabriel,Peyré,ens,CNRS and ENS,gabriel.peyre@ens.fr,Universal Invariant and Equivariant Graph Neural Networks
neurips,2019,0,205,Hisham,Husain,,The Australian National University,,A Primal-Dual link between GANs and Autoencoders
neurips,2019,1,205,Richard,Nock,,"Data61, the Australian National University and the University of Sydney",,A Primal-Dual link between GANs and Autoencoders
neurips,2019,2,205,Robert,Williamson,,Australian National University & Data61,,A Primal-Dual link between GANs and Autoencoders
neurips,2019,0,1852,Maithra,Raghu,,Cornell University and Google Brain,,Transfusion: Understanding Transfer Learning for Medical Imaging
neurips,2019,1,1852,Chiyuan,Zhang,,Google Brain,,Transfusion: Understanding Transfer Learning for Medical Imaging
neurips,2019,2,1852,Jon,Kleinberg,,Cornell University,,Transfusion: Understanding Transfer Learning for Medical Imaging
neurips,2019,3,1852,Samy,Bengio,,"Google Research, Brain Team",,Transfusion: Understanding Transfer Learning for Medical Imaging
neurips,2019,0,9259,Parikshit,Gopalan,vmware,VMware Research,pgopalan@vmware.com,PIDForest: Anomaly Detection via Partial Identification
neurips,2019,1,9259,Vatsal,Sharan,stanford,Stanford University,vsharan@stanford.edu,PIDForest: Anomaly Detection via Partial Identification
neurips,2019,2,9259,Udi,Wieder,vmware,VMware Research,uwieder@vmware.com,PIDForest: Anomaly Detection via Partial Identification
neurips,2019,0,1228,Ruoqi,Shen,washington,University of Washington,shenr3@cs.washington.edu,The Randomized Midpoint Method for Log-Concave Sampling
neurips,2019,1,1228,Yin Tat,Lee,uw,UW,yintat@uw.edu,The Randomized Midpoint Method for Log-Concave Sampling
neurips,2019,0,2846,Yandong,Wen,cmu,Carnegie Mellon University,yandongw@andrew.cmu.edu,Face Reconstruction from Voice using Generative Adversarial Networks
neurips,2019,1,2846,Bhiksha,Raj,cmu,Carnegie Mellon University,rsingh@cs.cmu.edu,Face Reconstruction from Voice using Generative Adversarial Networks
neurips,2019,2,2846,Rita,Singh,cmu,Carnegie Mellon University,bhiksha@cs.cmu.edu,Face Reconstruction from Voice using Generative Adversarial Networks
neurips,2019,0,7875,Harm,Van Seijen,microsoft,Microsoft Research,harm.vanseijen@microsoft.com,Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning
neurips,2019,1,7875,Mehdi,Fatemi,microsoft,Microsoft Research,mehdi.fatemi@microsoft.com,Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning
neurips,2019,2,7875,Arash,Tavakoli,imperial,Imperial College London,a.tavakoli@imperial.ac.uk,Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning
neurips,2019,0,4751,Yue,Wang,mit,MIT,jsolomon@mit.edu,PRNet: Self-Supervised Learning for Partial-to-Partial Registration
neurips,2019,1,4751,Justin,Solomon,mit,MIT,yuewangx@mit.edu,PRNet: Self-Supervised Learning for Partial-to-Partial Registration
neurips,2019,0,6421,Juncheng,Li,cmu,Carnegie Mellon University,junchenl@cs.cmu.edu,Adversarial Music: Real world Audio Adversary against Wake-word Detection System
neurips,2019,1,6421,Shuhui,Qu,stanford,Stanford University,shuhuiq@stanford.edu,Adversarial Music: Real world Audio Adversary against Wake-word Detection System
neurips,2019,2,6421,Xinjian,Li,cmu,Carnegie Mellon University,xinjianl@cs.cmu.edu,Adversarial Music: Real world Audio Adversary against Wake-word Detection System
neurips,2019,3,6421,Joseph,Szurley,bosch,Bosch Center for Artificial Intelligence,jszurley@bosch.com,Adversarial Music: Real world Audio Adversary against Wake-word Detection System
neurips,2019,4,6421,J. Zico,Kolter,cmu,Carnegie Mellon University / Bosch Center for AI,zkolter@cs.cmu.edu,Adversarial Music: Real world Audio Adversary against Wake-word Detection System
neurips,2019,5,6421,Florian,Metze,cmu,Carnegie Mellon University,fmetze@cs.cmu.edu,Adversarial Music: Real world Audio Adversary against Wake-word Detection System
neurips,2019,0,8583,Yue,Cao,tamu,Texas A&M University,cyppsp@tamu.edu,Learning to Optimize in Swarms
neurips,2019,1,8583,Tianlong,Chen,tamu,Texas A&M University,wiwjp619@tamu.edu,Learning to Optimize in Swarms
neurips,2019,2,8583,Zhangyang,Wang,tamu,TAMU,atlaswang@tamu.edu,Learning to Optimize in Swarms
neurips,2019,3,8583,Yang,Shen,tamu,Texas A&M University,yshen@tamu.edu,Learning to Optimize in Swarms
neurips,2019,0,4657,Gilad,Baruch,biu,Apple,moran.baruch@biu.ac.il,A Little Is Enough: Circumventing Defenses For Distributed Learning
neurips,2019,1,4657,Moran,Baruch,biu,Bar Ilan University,gilad.baruch@biu.ac.il,A Little Is Enough: Circumventing Defenses For Distributed Learning
neurips,2019,2,4657,Yoav,Goldberg,biu,Bar-Ilan University,yogo@cs.biu.ac.il,A Little Is Enough: Circumventing Defenses For Distributed Learning
neurips,2019,0,5862,Mikhail,Yurochkin,ibm,"IBM Research, MIT-IBM Watson AI Lab",mikhail.yurochkin@ibm.com,Statistical Model Aggregation via Parameter Matching
neurips,2019,1,5862,Mayank,Agarwal,ibm,"IBM Research AI, MIT-IBM Watson AI Lab",mayank.agarwal@ibm.com,Statistical Model Aggregation via Parameter Matching
neurips,2019,2,5862,Soumya,Ghosh,ibm,IBM Research,ghoshso@us.ibm.com,Statistical Model Aggregation via Parameter Matching
neurips,2019,3,5862,Kristjan,Greenewald,ibm,IBM Research,kristjan.h.greenewald@ibm.com,Statistical Model Aggregation via Parameter Matching
neurips,2019,4,5862,Nghia,Hoang,ibm,IBM Research,nghiaht@ibm.com,Statistical Model Aggregation via Parameter Matching
neurips,2019,0,98,Chao,Yang,,Tsinghua University,,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement
neurips,2019,1,98,Xiaojian,Ma,,"University of California, Los Angeles",,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement
neurips,2019,2,98,Wenbing,Huang,,Tsinghua University,,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement
neurips,2019,3,98,Fuchun,Sun,,Tsinghua,,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement
neurips,2019,4,98,Huaping,Liu,,Tsinghua University,,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement
neurips,2019,5,98,Junzhou,Huang,,University of Texas at Arlington / Tencent AI Lab,,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement
neurips,2019,6,98,Chuang,Gan,,MIT-IBM Watson AI Lab,,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement
neurips,2019,0,6422,Muhammad,Osama,uu,Uppsala University,muhammad.osama@it.uu.se,Prediction of Spatial Point Processes: Regularized Method with Out-of-Sample Guarantees
neurips,2019,1,6422,Dave,Zachariah,uu,Uppsala University,dave.zachariah@it.uu.se,Prediction of Spatial Point Processes: Regularized Method with Out-of-Sample Guarantees
neurips,2019,2,6422,Peter,Stoica,uu,Uppsala University,peter.stoica@it.uu.se,Prediction of Spatial Point Processes: Regularized Method with Out-of-Sample Guarantees
neurips,2019,0,5406,Corey,Snyder,illinois,University of Illinois at Urbana-Champaign,cesnyde2@illinois.edu,STREETS: A Novel Camera Network Dataset for Traffic Flow
neurips,2019,1,5406,Minh,Do,illinois,University of Illinois,minhdo@illinois.edu,STREETS: A Novel Camera Network Dataset for Traffic Flow
neurips,2019,0,4929,Rebecca,Roelofs,berkeley,UC Berkeley,roelofs@berkeley.edu,A Meta-Analysis of Overfitting in Machine Learning
neurips,2019,1,4929,Vaishaal,Shankar,berkeley,UC Berkeley,sfk@berkeley.edu,A Meta-Analysis of Overfitting in Machine Learning
neurips,2019,2,4929,Benjamin,Recht,berkeley,UC Berkeley,miller_john@berkeley.edu,A Meta-Analysis of Overfitting in Machine Learning
neurips,2019,3,4929,Sara,Fridovich-Keil,berkeley,UC Berkeley,vaishaal@berkeley.edu,A Meta-Analysis of Overfitting in Machine Learning
neurips,2019,4,4929,Moritz,Hardt,berkeley,"University of California, Berkeley",hardt@berkeley.edu,A Meta-Analysis of Overfitting in Machine Learning
neurips,2019,5,4929,John,Miller,berkeley,"University of California, Berkeley",brecht@berkeley.edu,A Meta-Analysis of Overfitting in Machine Learning
neurips,2019,6,4929,Ludwig,Schmidt,berkeley,UC Berkeley,ludwig@berkeley.edu,A Meta-Analysis of Overfitting in Machine Learning
neurips,2019,0,8670,Peng,Chen,utexas,The University of Texas at Austin,peng@oden.utexas.edu,Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions
neurips,2019,1,8670,Keyi,Wu,utexas,The University of Texas at Austin,keyi@oden.utexas.edu,Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions
neurips,2019,2,8670,Joshua,Chen,utexas,The University of Texas at Austin,joshua@oden.utexas.edu,Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions
neurips,2019,3,8670,Tom,O'Leary-Roseberry,utexas,The University of Texas at Austin,tom@oden.utexas.edu,Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions
neurips,2019,4,8670,Omar,Ghattas,utexas,The University of Texas at Austin,omar@oden.utexas.edu,Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions
neurips,2019,0,4613,Hidenori,Tanaka,,Stanford,,From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction
neurips,2019,1,4613,Aran,Nayebi,,Stanford University,,From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction
neurips,2019,2,4613,Niru,Maheswaranathan,,Google Brain,,From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction
neurips,2019,3,4613,Lane,McIntosh,,Telsa,,From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction
neurips,2019,4,4613,Stephen,Baccus,,Stanford University,,From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction
neurips,2019,5,4613,Surya,Ganguli,,Stanford,,From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction
neurips,2019,0,3118,Kecheng,Zheng,,University of Science and Technology of China,,Abstract Reasoning with Distracting Features
neurips,2019,1,3118,Zheng-Jun,Zha,,University of Science and Technology of China,,Abstract Reasoning with Distracting Features
neurips,2019,2,3118,Wei,Wei,,Google AI,,Abstract Reasoning with Distracting Features
neurips,2019,0,4016,Daniel,Worrall,uva,University of Amsterdam,d.e.worrall@uva.nl,Deep Scale-spaces: Equivariance Over Scale
neurips,2019,1,4016,Max,Welling,uva,University of Amsterdam / Qualcomm AI Research,m.welling@uva.nl,Deep Scale-spaces: Equivariance Over Scale
neurips,2019,0,4375,Ananda Theertha,Suresh,google,Google,theertha@google.com,Differentially Private Anonymized Histograms
neurips,2019,0,106,Soheil,Kolouri,hrl,HRL Laboratories LLC,skolouri@hrl.com,Generalized Sliced Wasserstein Distances
neurips,2019,1,106,Kimia,Nadjahi,virginia,Télécom ParisTech,gustavo@virginia.edu,Generalized Sliced Wasserstein Distances
neurips,2019,2,106,Umut,Simsekli,telecom-paris,Institut Polytechnique de Paris/ University of Oxford,kimia.nadjahi@telecom-paris.fr,Generalized Sliced Wasserstein Distances
neurips,2019,3,106,Roland,Badeau,telecom-paris,Télécom ParisTech,umut.simsekli@telecom-paris.fr,Generalized Sliced Wasserstein Distances
neurips,2019,4,106,Gustavo,Rohde,telecom-paris,University of Virginia,roland.badeau@telecom-paris.fr,Generalized Sliced Wasserstein Distances
neurips,2019,0,7240,Arnak,Dalalyan,,ENSAE ParisTech,,Outlier-robust estimation of a sparse linear model using $\ell_1$-penalized Huber's $M$-estimator
neurips,2019,1,7240,Philip,Thompson,,"University of Cambridge, Statistical Laboratory",,Outlier-robust estimation of a sparse linear model using $\ell_1$-penalized Huber's $M$-estimator
neurips,2019,0,2642,Cole,Hurwitz,ed,University of Edinburgh,kai.xu@ed.ac.uk,Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference
neurips,2019,1,2642,Kai,Xu,ed,University of Ediburgh,cole.hurwitz@ed.ac.uk,Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference
neurips,2019,2,2642,Akash,Srivastava,ibm,MITIBM Watson AI Lab,Akash.Srivastava@ibm.com,Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference
neurips,2019,3,2642,Alessio,Buccino,uio,"CINPLA, University of Oslo",alessiob@ifi.uio.no,Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference
neurips,2019,4,2642,Matthias,Hennig,ed,University of Edinburgh,m.hennig@ed.ac.uk,Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference
neurips,2019,0,2682,Yuan,Deng,duke,Duke University,ericdy@cs.duke.edu,Prior-Free Dynamic Auctions with Low Regret Buyers
neurips,2019,1,2682,Jon,Schneider,google,Google Research,jschnei@google.com,Prior-Free Dynamic Auctions with Low Regret Buyers
neurips,2019,2,2682,Balasubramanian,Sivan,google,Google Research,balusivan@google.com,Prior-Free Dynamic Auctions with Low Regret Buyers
neurips,2019,0,2624,Rafael,Müller,,Google Brain,,When does label smoothing help?
neurips,2019,1,2624,Simon,Kornblith,,Google Brain,,When does label smoothing help?
neurips,2019,2,2624,Geoffrey,Hinton,,Google & University of Toronto,,When does label smoothing help?
neurips,2019,0,6733,Moses,Charikar,stanford,Stanford University,moses@cs.stanford.edu,A General Framework for Symmetric Property Estimation
neurips,2019,1,6733,Kirankumar,Shiragur,stanford,Stanford University,shiragur@stanford.edu,A General Framework for Symmetric Property Estimation
neurips,2019,2,6733,Aaron,Sidford,stanford,Stanford,sidford@stanford.edu,A General Framework for Symmetric Property Estimation
neurips,2019,0,4970,Salvator,Lombardo,,Disney Research,,Deep Generative Video Compression
neurips,2019,1,4970,JUN,HAN,,Dartmouth College,,Deep Generative Video Compression
neurips,2019,2,4970,Christopher,Schroers,,Disney Research|Studios,,Deep Generative Video Compression
neurips,2019,3,4970,Stephan,Mandt,,Disney Research,,Deep Generative Video Compression
neurips,2019,0,767,Brandon,Yang,google,Google Brain,bcyang@google.com,CondConv: Conditionally Parameterized Convolutions for Efficient Inference
neurips,2019,1,767,Gabriel,Bender,google,Google Brain,qvl@google.com,CondConv: Conditionally Parameterized Convolutions for Efficient Inference
neurips,2019,2,767,Quoc,Le,google,Google,gbender@google.com,CondConv: Conditionally Parameterized Convolutions for Efficient Inference
neurips,2019,3,767,Jiquan,Ngiam,google,Google Brain,jngiam@google.com,CondConv: Conditionally Parameterized Convolutions for Efficient Inference
neurips,2019,0,3303,Zhao,Song,gmail,University of Washington,magic.linuxkde@gmail.com,Towards a Zero-One Law for Column Subset Selection
neurips,2019,1,3303,David,Woodruff,cmu,Carnegie Mellon University,dwoodruf@cs.cmu.edu,Towards a Zero-One Law for Column Subset Selection
neurips,2019,2,3303,Peilin,Zhong,columbia,Columbia University,pz2225@columbia.edu,Towards a Zero-One Law for Column Subset Selection
neurips,2019,0,6375,Rahul,Gupta,iisc,Indian Institute of Science,rahulg@iisc.ac.in,Neural Attribution for Semantic Bug-Localization in Student Programs
neurips,2019,1,6375,Aditya,Kanade,iisc,Indian Institute of Science,kanade@iisc.ac.in,Neural Attribution for Semantic Bug-Localization in Student Programs
neurips,2019,2,6375,Shirish,Shevade,iisc,iisc,shirish@iisc.ac.in,Neural Attribution for Semantic Bug-Localization in Student Programs
neurips,2019,0,6703,Igor,Colin,,Huawei,,Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning
neurips,2019,1,6703,Ludovic,DOS SANTOS,,Huawei,,Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning
neurips,2019,2,6703,Kevin,Scaman,,Huawei Noah's Ark Lab,,Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning
neurips,2019,0,1820,Zelda,Mariet,mit,MIT,zelda@csail.mit.edu,DppNet: Approximating Determinantal Point Processes with Deep Networks
neurips,2019,1,1820,Yaniv,Ovadia,google,Princeton University,yovadia@google.com,DppNet: Approximating Determinantal Point Processes with Deep Networks
neurips,2019,2,1820,Jasper,Snoek,google,Google Brain,jsnoek@google.com,DppNet: Approximating Determinantal Point Processes with Deep Networks
neurips,2019,0,3972,Sarath,Yasodharan,iisc,Indian Institute of Science,sarath@iisc.ac.in,Nonzero-sum Adversarial Hypothesis Testing Games
neurips,2019,1,3972,Patrick,Loiseau,inria,Inria,patrick.loiseau@inria.fr,Nonzero-sum Adversarial Hypothesis Testing Games
neurips,2019,0,3438,Xiaohan,Ding,tsinghua,Tsinghua University,dinggg@tsinghua.edu.cn,Global Sparse Momentum SGD for Pruning Very Deep Neural Networks
neurips,2019,1,3438,guiguang,ding,tsinghua,"Tsinghua University, China",dxh17@mails.tsinghua.edu.cn,Global Sparse Momentum SGD for Pruning Very Deep Neural Networks
neurips,2019,2,3438,Xiangxin,Zhou,tsinghua,Tsinghua University,xx-zhou16@mails.tsinghua.edu.cn,Global Sparse Momentum SGD for Pruning Very Deep Neural Networks
neurips,2019,3,3438,Yuchen,Guo,gmail,Tsinghua University,yuchen.w.guo@gmail.com,Global Sparse Momentum SGD for Pruning Very Deep Neural Networks
neurips,2019,4,3438,Jungong,Han,gmail,Lancaster University,jungonghan77@gmail.com,Global Sparse Momentum SGD for Pruning Very Deep Neural Networks
neurips,2019,5,3438,Ji,Liu,gmail,"University of Rochester, Tencent AI lab",ji.liu.uwisc@gmail.com,Global Sparse Momentum SGD for Pruning Very Deep Neural Networks
neurips,2019,0,4739,My,Phan,umass,University of Massachusetts Amherst,myphan@cs.umass.edu,Thompson Sampling and Approximate Inference
neurips,2019,1,4739,Yasin,Abbasi Yadkori,gmail,"VinAI Research/ VinTech JSC.,",yasin.abbasi@gmail.com,Thompson Sampling and Approximate Inference
neurips,2019,2,4739,Justin,Domke,umass,"University of Massachusetts, Amherst",domke@cs.umass.edu,Thompson Sampling and Approximate Inference
neurips,2019,0,3674,Shouvanik,Chakrabarti,umd,University of Maryland,shouv@cs.umd.edu,Quantum Wasserstein Generative Adversarial Networks
neurips,2019,1,3674,Huang,Yiming,umd,University of Electronic Science and Technology of China; University of Maryland,tongyang@cs.umd.edu,Quantum Wasserstein Generative Adversarial Networks
neurips,2019,2,3674,Tongyang,Li,umd,University of Maryland,sfeizi@cs.umd.edu,Quantum Wasserstein Generative Adversarial Networks
neurips,2019,3,3674,Soheil,Feizi,umd,University of Maryland,xwu@cs.umd.edu,Quantum Wasserstein Generative Adversarial Networks
neurips,2019,4,3674,Xiaodi,Wu,gmail,University of Maryland,yiminghwang@gmail.com,Quantum Wasserstein Generative Adversarial Networks
neurips,2019,0,553,Mohamed,Akrout,,University of Toronto,,Deep Learning without Weight Transport
neurips,2019,1,553,Collin,Wilson,,University of Toronto,,Deep Learning without Weight Transport
neurips,2019,2,553,Peter,Humphreys,,Deepmind,,Deep Learning without Weight Transport
neurips,2019,3,553,Timothy,Lillicrap,,DeepMind & UCL,,Deep Learning without Weight Transport
neurips,2019,4,553,Douglas,Tweed,,University of Toronto,,Deep Learning without Weight Transport
neurips,2019,0,1800,Gauthier,Gidel,,Mila,,Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks
neurips,2019,1,1800,Francis,Bach,,INRIA - Ecole Normale Superieure,,Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks
neurips,2019,2,1800,Simon,Lacoste-Julien,,"Mila, Université de Montréal",,Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks
neurips,2019,0,9260,John,Ingraham,mit,MIT,ingraham@csail.mit.edu,Generative Models for Graph-Based Protein Design
neurips,2019,1,9260,Vikas,Garg,mit,MIT,vgarg@csail.mit.edu,Generative Models for Graph-Based Protein Design
neurips,2019,2,9260,Regina,Barzilay,mit,Massachusetts Institute of Technology,regina@csail.mit.edu,Generative Models for Graph-Based Protein Design
neurips,2019,3,9260,Tommi,Jaakkola,mit,MIT,tommi@csail.mit.edu,Generative Models for Graph-Based Protein Design
neurips,2019,0,4223,Wenrui,Zhang,ucsb,"University of California, Santa Barbara",wenruizhang@ucsb.edu,Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks
neurips,2019,1,4223,Peng,Li,ucsb,"University of California, Santa Barbara",lip@ucsb.edu,Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks
neurips,2019,0,3340,Derek,Yang,gmail,UC San Diego,dyang1206@gmail.com,Fully Parameterized Quantile Function for Distributional Reinforcement Learning
neurips,2019,1,3340,Li,Zhao,microsoft,Microsoft Research,lizo@microsoft.com,Fully Parameterized Quantile Function for Distributional Reinforcement Learning
neurips,2019,2,3340,Zichuan,Lin,tsinghua,Tsinghua University,linzc16@mails.tsinghua.edu.cn,Fully Parameterized Quantile Function for Distributional Reinforcement Learning
neurips,2019,3,3340,Tao,Qin,microsoft,Microsoft Research,taoqin@microsoft.com,Fully Parameterized Quantile Function for Distributional Reinforcement Learning
neurips,2019,4,3340,Jiang,Bian,microsoft,Microsoft,jiang.bian@microsoft.com,Fully Parameterized Quantile Function for Distributional Reinforcement Learning
neurips,2019,5,3340,Tie-Yan,Liu,microsoft,Microsoft Research Asia,tyliu@microsoft.com,Fully Parameterized Quantile Function for Distributional Reinforcement Learning
neurips,2019,0,8988,Aria,Wang,,Carnegie Mellon University,,Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity
neurips,2019,1,8988,Michael,Tarr,,Carnegie Mellon University,,Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity
neurips,2019,2,8988,Leila,Wehbe,,Carnegie Mellon University,,Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity
neurips,2019,0,3181,Mikhail,Khodak,cmu,CMU,khodak@cmu.edu,Adaptive Gradient-Based Meta-Learning Methods
neurips,2019,1,3181,Maria-Florina,Balcan,cmu,Carnegie Mellon University,ninamf@cs.cmu.edu,Adaptive Gradient-Based Meta-Learning Methods
neurips,2019,2,3181,Ameet,Talwalkar,cmu,CMU,talwalkar@cmu.edu,Adaptive Gradient-Based Meta-Learning Methods
neurips,2019,0,5200,Brenden,Lake,nyu,New York University,brenden@nyu.edu,Compositional generalization through meta sequence-to-sequence learning
neurips,2019,0,1052,Khurram,Javed,ualberta,University of Alberta,kjaved@ualberta.ca,Meta-Learning Representations for Continual Learning
neurips,2019,1,1052,Martha,White,ualberta,University of Alberta,whitem@ualberta.ca,Meta-Learning Representations for Continual Learning
neurips,2019,0,2482,Jason,Altschuler,mit,MIT,jasonalt@mit.edu,Massively scalable Sinkhorn distances via the Nyström method
neurips,2019,1,2482,Francis,Bach,inria,INRIA - Ecole Normale Superieure,francis.bach@inria.fr,Massively scalable Sinkhorn distances via the Nyström method
neurips,2019,2,2482,Alessandro,Rudi,inria,"INRIA, Ecole Normale Superieure",alessandro.rudi@inria.fr,Massively scalable Sinkhorn distances via the Nyström method
neurips,2019,3,2482,Jonathan,Niles-Weed,nyu,NYU,jnw@cims.nyu.edu,Massively scalable Sinkhorn distances via the Nyström method
neurips,2019,0,6542,Ming,Hou,hdu,RIKEN AIP,jhzhang@hdu.edu.cn,Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling
neurips,2019,1,6542,Jiajia,Tang,hdu,Hangzhou Dianzi University / RIKEN AIP,kongwanzeng@hdu.edu.cn,Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling
neurips,2019,2,6542,Jianhai,Zhang,riken,Hangzhou Dianzi University,qibin.zhao@riken.jp,Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling
neurips,2019,3,6542,Wanzeng,Kong,riken,Hangzhou Dianzi University,ming.hou@riken.jp,Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling
neurips,2019,4,6542,Qibin,Zhao,163,RIKEN AIP,hdutangjiajia@163.com,Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling
neurips,2019,0,7143,Kishor,Jothimurugan,upenn,University of Pennsylvania,kishor@cis.upenn.edu,A Composable Specification Language for Reinforcement Learning Tasks
neurips,2019,1,7143,Rajeev,Alur,upenn,University of Pennsylvania,alur@cis.upenn.edu,A Composable Specification Language for Reinforcement Learning Tasks
neurips,2019,2,7143,Osbert,Bastani,upenn,University of Pennysylvania,obastani@cis.upenn.edu,A Composable Specification Language for Reinforcement Learning Tasks
neurips,2019,0,5098,Wenzheng,Chen,nvidia,University of Toronto,wenzchen@nvidia.com,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer
neurips,2019,1,5098,Huan,Ling,nvidia,"University of Toronto, NVIDIA",huling@nvidia.com,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer
neurips,2019,2,5098,Jun,Gao,nvidia,University of Toronto,jung@nvidia.com,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer
neurips,2019,3,5098,Edward,Smith,nvidia,McGill University,esmith@nvidia.com,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer
neurips,2019,4,5098,Jaakko,Lehtinen,nvidia,NVIDIA Research; Aalto University,jlehtinen@nvidia.com,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer
neurips,2019,5,5098,Alec,Jacobson,nvidia,University of Toronto,sfidler@nvidia.com,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer
neurips,2019,6,5098,Sanja,Fidler,toronto,University of Toronto,jacobson@cs.toronto.edu,Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer
neurips,2019,0,2815,Micah,Carroll,,UC Berkeley,,On the Utility of Learning about Humans for Human-AI Coordination
neurips,2019,1,2815,Rohin,Shah,,UC Berkeley,,On the Utility of Learning about Humans for Human-AI Coordination
neurips,2019,2,2815,Mark,Ho,,Princeton University,,On the Utility of Learning about Humans for Human-AI Coordination
neurips,2019,3,2815,Tom,Griffiths,,Princeton University,,On the Utility of Learning about Humans for Human-AI Coordination
neurips,2019,4,2815,Sanjit,Seshia,,UC Berkeley,,On the Utility of Learning about Humans for Human-AI Coordination
neurips,2019,5,2815,Pieter,Abbeel,,UC Berkeley & covariant.ai,,On the Utility of Learning about Humans for Human-AI Coordination
neurips,2019,6,2815,Anca,Dragan,,UC Berkeley,,On the Utility of Learning about Humans for Human-AI Coordination
neurips,2019,0,1780,Yi,Ren,zju,Zhejiang University,rayeren@zju.edu.cn,"FastSpeech: Fast, Robust and Controllable Text to Speech"
neurips,2019,1,1780,Yangjun,Ruan,zju,Zhejiang University,ruanyj3107@zju.edu.cn,"FastSpeech: Fast, Robust and Controllable Text to Speech"
neurips,2019,2,1780,Xu,Tan,microsoft,Microsoft Research,xuta@microsoft.com,"FastSpeech: Fast, Robust and Controllable Text to Speech"
neurips,2019,3,1780,Tao,Qin,microsoft,Microsoft Research,taoqin@microsoft.com,"FastSpeech: Fast, Robust and Controllable Text to Speech"
neurips,2019,4,1780,Sheng,Zhao,microsoft,Microsoft,Sheng.Zhao@microsoft.com,"FastSpeech: Fast, Robust and Controllable Text to Speech"
neurips,2019,5,1780,Zhou,Zhao,zju,Zhejiang University,zhaozhou@zju.edu.cn,"FastSpeech: Fast, Robust and Controllable Text to Speech"
neurips,2019,6,1780,Tie-Yan,Liu,microsoft,Microsoft Research,tyliu@microsoft.com,"FastSpeech: Fast, Robust and Controllable Text to Speech"
neurips,2019,0,4174,Falcon,Dai,ttic,TTI-Chicago,mwalter@ttic.edu,Maximum Expected Hitting Cost of a Markov Decision Process and Informativeness of Rewards
neurips,2019,1,4174,Matthew,Walter,ttic,TTI-Chicago,dai@ttic.edu,Maximum Expected Hitting Cost of a Markov Decision Process and Informativeness of Rewards
neurips,2019,0,1442,Hongzi,Mao,,MIT,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,1,1442,Parimarjan,Negi,,MIT CSAIL,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,2,1442,Akshay,Narayan,,MIT CSAIL,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,3,1442,Hanrui,Wang,,Massachusetts Institute of Technology,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,4,1442,Jiacheng,Yang,,MIT CSAIL,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,5,1442,Haonan,Wang,,MIT CSAIL,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,6,1442,Ryan,Marcus,,MIT CSAIL,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,7,1442,ravichandra,addanki,,Massachusetts Institute of Technology,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,8,1442,Mehrdad,Khani Shirkoohi,,MIT,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,9,1442,Songtao,He,,Massachusetts Institute of Technology,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,10,1442,Vikram,Nathan,,MIT,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,11,1442,Frank,Cangialosi,,MIT CSAIL,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,12,1442,Shaileshh,Venkatakrishnan,,MIT,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,13,1442,Wei-Hung,Weng,,MIT,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,14,1442,Song,Han,,MIT,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,15,1442,Tim,Kraska,,MIT,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,16,1442,Dr.Mohammad,Alizadeh,,Massachusetts institute of technology,,Park: An Open Platform for Learning-Augmented Computer Systems
neurips,2019,0,2990,Binghui,Peng,columbia,Columbia University,bp2601@columbia.edu,Adaptive Influence Maximization with Myopic Feedback
neurips,2019,1,2990,Wei,Chen,microsoft,Microsoft Research,weic@microsoft.com,Adaptive Influence Maximization with Myopic Feedback
neurips,2019,0,2132,Jonathan,Ho,berkeley,UC Berkeley,jonathanho@berkeley.edu,Compression with Flows via Local Bits-Back Coding
neurips,2019,1,2132,Evan,Lohn,berkeley,"University of California, Berkeley",evan.lohn@berkeley.edu,Compression with Flows via Local Bits-Back Coding
neurips,2019,2,2132,Pieter,Abbeel,berkeley,UC Berkeley & covariant.ai,pabbeel@cs.berkeley.edu,Compression with Flows via Local Bits-Back Coding
neurips,2019,0,2434,Christopher,Beckham,,Mila,,On Adversarial Mixup Resynthesis
neurips,2019,1,2434,Sina,Honari,,"Mila, EPFL",,On Adversarial Mixup Resynthesis
neurips,2019,2,2434,Vikas,Verma,,Aalto University,,On Adversarial Mixup Resynthesis
neurips,2019,3,2434,Alex,Lamb,,UMontreal (MILA),,On Adversarial Mixup Resynthesis
neurips,2019,4,2434,Farnoosh,Ghadiri,,Mila,,On Adversarial Mixup Resynthesis
neurips,2019,5,2434,R Devon,Hjelm,,Microsoft Research,,On Adversarial Mixup Resynthesis
neurips,2019,6,2434,Yoshua,Bengio,,Mila,,On Adversarial Mixup Resynthesis
neurips,2019,7,2434,Chris,Pal,,"MILA, Polytechnique Montréal, Element AI",,On Adversarial Mixup Resynthesis
neurips,2019,0,44,Ruben,Villegas,,Adobe Research / U. Michigan,,High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks
neurips,2019,1,44,Arkanath,Pathak,,Google,,High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks
neurips,2019,2,44,Harini,Kannan,,Google Brain,,High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks
neurips,2019,3,44,Dumitru,Erhan,,Google Brain,,High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks
neurips,2019,4,44,Quoc,Le,,Google,,High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks
neurips,2019,5,44,Honglak,Lee,,Google / U. Michigan,,High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks
neurips,2019,0,7343,Yixin,Wang,,Columbia University,,Variational Bayes under Model Misspecification
neurips,2019,1,7343,David,Blei,,Columbia University,,Variational Bayes under Model Misspecification
neurips,2019,0,8790,Mislav,Balunovic,ethz,ETH Zurich,mislav.balunovic@inf.ethz.ch,Certifying Geometric Robustness of Neural Networks
neurips,2019,1,8790,Maximilian,Baader,ethz,ETH Zürich,mbaader@inf.ethz.ch,Certifying Geometric Robustness of Neural Networks
neurips,2019,2,8790,Gagandeep,Singh,ethz,ETH Zurich,gsingh@inf.ethz.ch,Certifying Geometric Robustness of Neural Networks
neurips,2019,3,8790,Timon,Gehr,ethz,ETH Zurich,timon.gehr@inf.ethz.ch,Certifying Geometric Robustness of Neural Networks
neurips,2019,4,8790,Martin,Vechev,ethz,"ETH Zurich, Switzerland",martin.vechev@inf.ethz.ch,Certifying Geometric Robustness of Neural Networks
neurips,2019,0,3263,Florian,Scheidegger,,IBM Research -- Zurich,,Constrained deep neural network architecture search for IoT devices accounting for hardware calibration
neurips,2019,1,3263,Luca,Benini,,"ETHZ, University of Bologna",,Constrained deep neural network architecture search for IoT devices accounting for hardware calibration
neurips,2019,2,3263,Costas,Bekas,,IBM Research GmbH,,Constrained deep neural network architecture search for IoT devices accounting for hardware calibration
neurips,2019,3,3263,A. Cristiano I.,Malossi,,IBM Research - Zurich,,Constrained deep neural network architecture search for IoT devices accounting for hardware calibration
neurips,2019,0,4160,Anuj,Mahajan,,University of Oxford,,MAVEN: Multi-Agent Variational Exploration
neurips,2019,1,4160,Tabish,Rashid,,University of Oxford,,MAVEN: Multi-Agent Variational Exploration
neurips,2019,2,4160,Mikayel,Samvelyan,,Russian-Armenian University,,MAVEN: Multi-Agent Variational Exploration
neurips,2019,3,4160,Shimon,Whiteson,,University of Oxford,,MAVEN: Multi-Agent Variational Exploration
neurips,2019,0,7283,Gabriel,Loaiza-Ganem,columbia,Columbia University,gl2480@columbia.edu,The continuous Bernoulli: fixing a pervasive error in variational autoencoders
neurips,2019,1,7283,John,Cunningham,columbia,University of Columbia,jpc2181@columbia.edu,The continuous Bernoulli: fixing a pervasive error in variational autoencoders
neurips,2019,0,2425,Alberto Maria,Metelli,polimi,Politecnico di Milano,albertomaria.metelli@polimi.it,Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters
neurips,2019,1,2425,Amarildo,Likmeta,polimi,Politecnico di Milano,amarildo.likmeta@polimi.it,Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters
neurips,2019,2,2425,Marcello,Restelli,polimi,Politecnico di Milano,marcello.restelli@polimi.it,Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters
neurips,2019,0,3235,W. O. K. Asiri Suranga,Wijesinghe,anu,The Australian National University,asiri.wijesinghe@anu.edu.au,DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters
neurips,2019,1,3235,Qing,Wang,anu,Australian National University,qing.wang@anu.edu.au,DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters
neurips,2019,0,4552,Sauptik,Dhar,lge,LG Electronics,sauptik.dhar@lge.com,Multiclass Learning from Contradictions
neurips,2019,1,4552,Vladimir,Cherkassky,umn,University of Minnesota,cherk001@umn.edu,Multiclass Learning from Contradictions
neurips,2019,2,4552,Mohak,Shah,lge,LG Electronics,mohak.shah@lge.com,Multiclass Learning from Contradictions
neurips,2019,0,2511,Ivana,Balazevic,ed,University of Edinburgh,ivana.balazevic@ed.ac.uk,Multi-relational Poincaré Graph Embeddings
neurips,2019,1,2511,Carl,Allen,ed,University of Edinburgh,carl.allen@ed.ac.uk,Multi-relational Poincaré Graph Embeddings
neurips,2019,2,2511,Timothy,Hospedales,ed,University of Edinburgh,t.hospedales@ed.ac.uk,Multi-relational Poincaré Graph Embeddings
neurips,2019,0,2060,Ananya,Kumar,,Stanford University,,Verified Uncertainty Calibration
neurips,2019,1,2060,Percy,Liang,,Stanford University,,Verified Uncertainty Calibration
neurips,2019,2,2060,Tengyu,Ma,,Stanford University,,Verified Uncertainty Calibration
neurips,2019,0,7197,Cyprien,de Masson d'Autume,google,Google DeepMind,cyprien@google.com,Episodic Memory in Lifelong Language Learning
neurips,2019,1,7197,Sebastian,Ruder,google,DeepMind,ruder@google.com,Episodic Memory in Lifelong Language Learning
neurips,2019,2,7197,Lingpeng,Kong,google,DeepMind,lingpenk@google.com,Episodic Memory in Lifelong Language Learning
neurips,2019,3,7197,Dani,Yogatama,google,DeepMind,dyogatama@google.com,Episodic Memory in Lifelong Language Learning
neurips,2019,0,2171,Shangyu,Chen,ntu,"Nanyang Technological University, Singapore",schen025@e.ntu.edu.sg,MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization
neurips,2019,1,2171,Wenya,Wang,ntu,Nanyang Technological University,wangwy@ntu.edu.sg,MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization
neurips,2019,2,2171,Sinno Jialin,Pan,ntu,"Nanyang Technological University, Singapore",sinnopan@ntu.edu.sg,MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization
neurips,2019,0,4004,Lu,Hou,ust,"Huawei Technologies Co., Ltd",lhouab@cse.ust.hk,Normalization Helps Training of Quantized LSTM
neurips,2019,1,4004,Jinhua,Zhu,ust,University of Science and Technology of China,jamesk@cse.ust.hk,Normalization Helps Training of Quantized LSTM
neurips,2019,2,4004,James,Kwok,ustc,Hong Kong University of Science and Technology,teslazhu@mail.ustc.edu.cn,Normalization Helps Training of Quantized LSTM
neurips,2019,3,4004,Fei,Gao,microsoft,University of Chinese Academy of Sciences,feiga@microsoft.com,Normalization Helps Training of Quantized LSTM
neurips,2019,4,4004,Tao,Qin,microsoft,Microsoft Research,taoqin@microsoft.com,Normalization Helps Training of Quantized LSTM
neurips,2019,5,4004,Tie-Yan,Liu,microsoft,Microsoft Research,tyliu@microsoft.com,Normalization Helps Training of Quantized LSTM
neurips,2019,0,290,Garrett,Bernstein,umass,University of Massachusetts Amherst,gbernstein@cs.umass.edu,Differentially Private Bayesian Linear Regression
neurips,2019,1,290,Daniel,Sheldon,umass,University of Massachusetts Amherst,sheldon@cs.umass.edu,Differentially Private Bayesian Linear Regression
neurips,2019,0,9051,Sherjil,Ozair,,"Mila, Université de Montréal",,Wasserstein Dependency Measure for Representation Learning
neurips,2019,1,9051,Corey,Lynch,,Google Brain,,Wasserstein Dependency Measure for Representation Learning
neurips,2019,2,9051,Yoshua,Bengio,,Mila,,Wasserstein Dependency Measure for Representation Learning
neurips,2019,3,9051,Aaron,van den Oord,,Google Deepmind,,Wasserstein Dependency Measure for Representation Learning
neurips,2019,4,9051,Sergey,Levine,,UC Berkeley,,Wasserstein Dependency Measure for Representation Learning
neurips,2019,5,9051,Pierre,Sermanet,,Google Brain,,Wasserstein Dependency Measure for Representation Learning
neurips,2019,0,5259,Christian,Schroeder de Witt,,University of Oxford,,Multi-Agent Common Knowledge Reinforcement Learning
neurips,2019,1,5259,Jakob,Foerster,,Facebook AI Research,,Multi-Agent Common Knowledge Reinforcement Learning
neurips,2019,2,5259,Gregory,Farquhar,,University of Oxford,,Multi-Agent Common Knowledge Reinforcement Learning
neurips,2019,3,5259,Philip,Torr,,University of Oxford,,Multi-Agent Common Knowledge Reinforcement Learning
neurips,2019,4,5259,Wendelin,Boehmer,,University of Oxford,,Multi-Agent Common Knowledge Reinforcement Learning
neurips,2019,5,5259,Shimon,Whiteson,,University of Oxford,,Multi-Agent Common Knowledge Reinforcement Learning
neurips,2019,0,3749,Boris,Muzellec,ensae,"ENSAE, Institut Polytechnique de Paris",boris.muzellec@ensae.fr,Subspace Detours: Building Transport Plans that are Optimal on Subspace Projections
neurips,2019,1,3749,Marco,Cuturi,google,Google Brain  &  CREST - ENSAE,cuturi@google.com,Subspace Detours: Building Transport Plans that are Optimal on Subspace Projections
neurips,2019,0,5897,Yi,Hao,ucsd,"University of California, San Diego",yih179@ucsd.edu,The Broad Optimality of Profile Maximum Likelihood
neurips,2019,1,5897,Alon,Orlitsky,ucsd,"University of California, San Diego",alon@ucsd.edu,The Broad Optimality of Profile Maximum Likelihood
neurips,2019,0,2720,Guang-He,Lee,mit,MIT,guanghe@csail.mit.edu,Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers
neurips,2019,1,2720,Yang,Yuan,mit,MIT,yangyuan@csail.mit.edu,Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers
neurips,2019,2,2720,Shiyu,Chang,mit,IBM T.J. Watson Research Center,tommi@csail.mit.edu,Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers
neurips,2019,3,2720,Tommi,Jaakkola,ibm,MIT,shiyu.chang@ibm.com,Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers
neurips,2019,0,6160,Michal,Derezinski,berkeley,UC Berkeley,mderezin@berkeley.edu,Exact sampling of determinantal point processes with sublinear time preprocessing
neurips,2019,1,6160,Daniele,Calandriello,iit,LCSL IIT/MIT,daniele.calandriello@iit.it,Exact sampling of determinantal point processes with sublinear time preprocessing
neurips,2019,2,6160,Michal,Valko,deepmind,DeepMind Paris and Inria Lille - Nord Europe,valkom@deepmind.com,Exact sampling of determinantal point processes with sublinear time preprocessing
neurips,2019,0,827,Jian,Sun,xjtu,Xi'an Jiaotong University,jiansun@xjtu.edu.cn,Neural Diffusion Distance for Image Segmentation
neurips,2019,1,827,Zongben,Xu,xjtu,XJTU,zbxu@xjtu.edu.cn,Neural Diffusion Distance for Image Segmentation
neurips,2019,0,164,David,Rolnick,upenn,UPenn,drolnick@seas.upenn.edu,Experience Replay for Continual Learning
neurips,2019,1,164,Arun,Ahuja,google,DeepMind,arahuja@google.com,Experience Replay for Continual Learning
neurips,2019,2,164,Jonathan,Schwarz,google,"DeepMind & Gatsby Unit, UCL",schwarzjn@google.com,Experience Replay for Continual Learning
neurips,2019,3,164,Timothy,Lillicrap,google,DeepMind & UCL,countzero@google.com,Experience Replay for Continual Learning
neurips,2019,4,164,Gregory,Wayne,google,Google DeepMind,gregwayne@google.com,Experience Replay for Continual Learning
neurips,2019,0,5030,Rémi,Jézéquel,inria,"INRIA, École Normale Supérieure",remi.jezequel@inria.fr,Efficient online learning with kernels for adversarial large scale problems
neurips,2019,1,5030,Pierre,Gaillard,inria,,pierre.gaillard@inria.fr,Efficient online learning with kernels for adversarial large scale problems
neurips,2019,2,5030,Alessandro,Rudi,inria,"INRIA, Ecole Normale Superieure",alessandro.rudi@inria.fr,Efficient online learning with kernels for adversarial large scale problems
neurips,2019,0,5398,Matthew,Reimherr,psu,Pennsylvania State University,mreimherr@psu.edu,KNG: The K-Norm Gradient Mechanism
neurips,2019,1,5398,Jordan,Awan,psu,Penn State University,awan@psu.edu,KNG: The K-Norm Gradient Mechanism
neurips,2019,0,6320,Avner,May,stanford,Stanford University,avnermay@cs.stanford.edu,On the Downstream Performance of Compressed Word Embeddings
neurips,2019,1,6320,Jian,Zhang,stanford,Stanford University,zjian@cs.stanford.edu,On the Downstream Performance of Compressed Word Embeddings
neurips,2019,2,6320,Tri,Dao,stanford,Stanford University,trid@cs.stanford.edu,On the Downstream Performance of Compressed Word Embeddings
neurips,2019,3,6320,Christopher,Ré,stanford,Stanford,chrismre@cs.stanford.edu,On the Downstream Performance of Compressed Word Embeddings
neurips,2019,0,7759,Qi,Lei,,University of Texas at Austin,,Primal-Dual Block Generalized Frank-Wolfe
neurips,2019,1,7759,JIACHENG,ZHUO,,University of Texas at Austin,,Primal-Dual Block Generalized Frank-Wolfe
neurips,2019,2,7759,Constantine,Caramanis,,UT Austin,,Primal-Dual Block Generalized Frank-Wolfe
neurips,2019,3,7759,Inderjit,Dhillon,,UT Austin & Amazon,,Primal-Dual Block Generalized Frank-Wolfe
neurips,2019,4,7759,Alexandros,Dimakis,,"University of Texas, Austin",,Primal-Dual Block Generalized Frank-Wolfe
neurips,2019,0,4874,Ananya,Uppal,cmu,Carnegie Mellon University,auppal@andrew.cmu.edu,Nonparametric Density Estimation & Convergence Rates for GANs under Besov IPM Losses
neurips,2019,1,4874,Shashank,Singh,cmu,CMU,sss1@cs.cmu.edu,Nonparametric Density Estimation & Convergence Rates for GANs under Besov IPM Losses
neurips,2019,2,4874,Barnabas,Poczos,cmu,Carnegie Mellon University,bapoczos@cs.cmu.edu,Nonparametric Density Estimation & Convergence Rates for GANs under Besov IPM Losses
neurips,2019,0,1204,Cyrille,Combettes,,Georgia Institute of Technology,,Blended Matching Pursuit
neurips,2019,1,1204,Sebastian,Pokutta,,Zuse Institute Berlin,,Blended Matching Pursuit
neurips,2019,0,5460,Aditya,Gangrade,bu,Boston University,gangrade@bu.edu,Efficient Near-Optimal Testing of Community Changes in Balanced Stochastic Block Models
neurips,2019,1,5460,Praveen,Venkatesh,bu,Carnegie Mellon University,bobak@bu.edu,Efficient Near-Optimal Testing of Community Changes in Balanced Stochastic Block Models
neurips,2019,2,5460,Bobak,Nazer,cmu,Boston University,vpraveen@cmu.edu,Efficient Near-Optimal Testing of Community Changes in Balanced Stochastic Block Models
neurips,2019,3,5460,Venkatesh,Saligrama,bu,Boston University,srv@bu.edu,Efficient Near-Optimal Testing of Community Changes in Balanced Stochastic Block Models
neurips,2019,0,4667,Stefano,Sarao Mannelli,,Institut de Physique Théorique,,Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models
neurips,2019,1,4667,Giulio,Biroli,,ENS,,Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models
neurips,2019,2,4667,Chiara,Cammarota,,King's College London,,Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models
neurips,2019,3,4667,Florent,Krzakala,,École Normale Supérieure,,Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models
neurips,2019,4,4667,Lenka,Zdeborová,,CEA Saclay,,Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models
neurips,2019,0,7255,Jianhao,Peng,illinois,University of Illinois at Urbana Champaign,abhiag@illinois.edu,Online Convex Matrix Factorization with Representative Regions
neurips,2019,1,7255,Olgica,Milenkovic,illinois,University of Illinois at Urbana-Champaign,jianhao2@illinois.edu,Online Convex Matrix Factorization with Representative Regions
neurips,2019,2,7255,Abhishek,Agarwal,illinois,University of Illinois at Urbana Champaign,milenkov@illinois.edu,Online Convex Matrix Factorization with Representative Regions
neurips,2019,0,8969,Eugene,Bagdasaryan,cornell,"Cornell Tech, Cornell University",eugene@cs.cornell.edu,Differential Privacy Has Disparate Impact on Model Accuracy
neurips,2019,1,8969,Omid,Poursaeed,cornell,Cornell University,op63@cornell.edu,Differential Privacy Has Disparate Impact on Model Accuracy
neurips,2019,2,8969,Vitaly,Shmatikov,cornell,Cornell University,shmat@cs.cornell.edu,Differential Privacy Has Disparate Impact on Model Accuracy
neurips,2019,0,2750,Suman,Bera,ucsc,University of California Santa Cruz,sbera@ucsc.edu,Fair Algorithms for Clustering
neurips,2019,1,2750,Deeparnab,Chakrabarty,dartmouth,Dartmouth,deeparnab@dartmouth.edu,Fair Algorithms for Clustering
neurips,2019,2,2750,Nicolas,Flores,dartmouth,Dartmouth College,nicolasflores.19@dartmouth.edu,Fair Algorithms for Clustering
neurips,2019,3,2750,Maryam,Negahbani,dartmouth,Dartmouth College,maryam@cs.dartmouth.edu,Fair Algorithms for Clustering
neurips,2019,0,1069,Alex,Lu,toronto,University of Toronto,alexlu@cs.toronto.edu,The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
neurips,2019,1,1069,Amy,Lu,toronto,University of Toronto/Vector Institute,amyxlu@cs.toronto.edu,The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
neurips,2019,2,1069,Wiebke,Schormann,utoronto,Sunnybrook Research Institute,wiebke.schormann@sri.utoronto.ca,The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
neurips,2019,3,1069,Marzyeh,Ghassemi,toronto,"University of Toronto, Vector Institute",marzyeh@cs.toronto.edu,The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
neurips,2019,4,1069,David,Andrews,utoronto,Sunnybrook Research Institute,david.andrews@sri.utoronto.ca,The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
neurips,2019,5,1069,Alan,Moses,utoronto,University of Toronto,alan.moses@utoronto.ca,The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
neurips,2019,0,6511,Radu,Marinescu,ibm,IBM Research,radu.marinescu@ie.ibm.com,Counting the Optimal Solutions in Graphical Models
neurips,2019,1,6511,Rina,Dechter,uci,UCI,dechter@ics.uci.edu,Counting the Optimal Solutions in Graphical Models
neurips,2019,0,7592,Asma,Ghandeharioun,mit,MIT,asma_gh@mit.edu,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems
neurips,2019,1,7592,Judy Hanwen,Shen,mit,Massachusetts Institute of Technology / Microsoft,judyshen@mit.edu,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems
neurips,2019,2,7592,Natasha,Jaques,mit,MIT,jaquesn@mit.edu,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems
neurips,2019,3,7592,Craig,Ferguson,mit,MIT,fergusoc@mit.edu,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems
neurips,2019,4,7592,Noah,Jones,mit,MIT,ncjones@mit.edu,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems
neurips,2019,5,7592,Agata,Lapedriza,mit,Massachusetts Institute of Technology,agata@mit.edu,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems
neurips,2019,6,7592,Rosalind,Picard,mit,MIT Media Lab,picard@media.mit.edu,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems
neurips,2019,0,1749,Alexander,Peysakhovich,,Facebook,,Robust Multi-agent Counterfactual Prediction
neurips,2019,1,1749,Christian,Kroer,,Columbia University,,Robust Multi-agent Counterfactual Prediction
neurips,2019,2,1749,Adam,Lerer,,Facebook AI Research,,Robust Multi-agent Counterfactual Prediction
neurips,2019,0,5985,Pasha,Khosravi,ucla,UCLA,pashak@cs.ucla.edu,On Tractable Computation of Expected Predictions
neurips,2019,1,5985,YooJung,Choi,ucla,UCLA,yjchoi@cs.ucla.edu,On Tractable Computation of Expected Predictions
neurips,2019,2,5985,Yitao,Liang,ucla,UCLA,yliang@cs.ucla.edu,On Tractable Computation of Expected Predictions
neurips,2019,3,5985,Antonio,Vergari,ucla,"University of California, Los Angeles",aver@cs.ucla.edu,On Tractable Computation of Expected Predictions
neurips,2019,4,5985,Guy,Van den Broeck,ucla,UCLA,guyvdb@cs.ucla.edu,On Tractable Computation of Expected Predictions
neurips,2019,0,1494,Zhuoning,Yuan,uiowa,University of Iowa,zhuoning-yuan@uiowa.edu,Stagewise Training Accelerates Convergence of Testing Error Over SGD
neurips,2019,1,1494,Yan,Yan,uiowa,the University of Iowa,yan-yan-2@uiowa.edu,Stagewise Training Accelerates Convergence of Testing Error Over SGD
neurips,2019,2,1494,Rong,Jin,uiowa,Alibaba,tianbao-yang@uiowa.edu,Stagewise Training Accelerates Convergence of Testing Error Over SGD
neurips,2019,3,1494,Tianbao,Yang,alibaba-inc,The University of Iowa,jinrong.jr@alibaba-inc.com,Stagewise Training Accelerates Convergence of Testing Error Over SGD
neurips,2019,0,7490,Biwei,Huang,,Carnegie Mellon University,,Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering
neurips,2019,1,7490,Kun,Zhang,,CMU,,Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering
neurips,2019,2,7490,Pengtao,Xie,,Petuum / CMU,,Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering
neurips,2019,3,7490,Mingming,Gong,,University of Melbourne,,Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering
neurips,2019,4,7490,Eric,Xing,,Petuum Inc.,,Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering
neurips,2019,5,7490,Clark,Glymour,,Carnegie Mellon University,,Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering
neurips,2019,0,8572,Kunal,Talwar,google,Google,kunal@google.com,Computational Separations between Sampling and Optimization
neurips,2019,0,6641,Suman,Ravuri,,DeepMind,,Classification Accuracy Score for Conditional Generative Models
neurips,2019,1,6641,Oriol,Vinyals,,Google DeepMind,,Classification Accuracy Score for Conditional Generative Models
neurips,2019,0,5350,Siavash,Khodadadeh,ucf,University of Central Florida,siavash.khodadadeh@knights.ucf.edu,Unsupervised Meta-Learning for Few-Shot Image Classification
neurips,2019,1,5350,Ladislau,Boloni,ucf,University of Central Florida,lboloni@cs.ucf.edu,Unsupervised Meta-Learning for Few-Shot Image Classification
neurips,2019,2,5350,Mubarak,Shah,ucf,University of Central Florida,shah@crcv.ucf.edu,Unsupervised Meta-Learning for Few-Shot Image Classification
neurips,2019,0,1135,Ximei,Wang,tsinghua,Tsinghua University,wxm17@mails.tsinghua.edu.cn,Transferable Normalization: Towards Improving Transferability of Deep Neural Networks
neurips,2019,1,1135,Ying,Jin,tsinghua,Tsinghua University,jiny18@mails.tsinghua.edu.cn,Transferable Normalization: Towards Improving Transferability of Deep Neural Networks
neurips,2019,2,1135,Mingsheng,Long,tsinghua,Tsinghua University,mingsheng@tsinghua.edu.cn,Transferable Normalization: Towards Improving Transferability of Deep Neural Networks
neurips,2019,3,1135,Jianmin,Wang,tsinghua,Tsinghua University,jimwang@tsinghua.edu.cn,Transferable Normalization: Towards Improving Transferability of Deep Neural Networks
neurips,2019,4,1135,Michael,Jordan,berkeley,UC Berkeley,jordan@cs.berkeley.edu,Transferable Normalization: Towards Improving Transferability of Deep Neural Networks
neurips,2019,0,5716,Arman,Hasanzadeh,tamu,Texas A&M University,armanihm@tamu.edu,Semi-Implicit Graph Variational Auto-Encoders
neurips,2019,1,5716,Ehsan,Hajiramezanali,tamu,Texas A&M University,ehsanr@tamu.edu,Semi-Implicit Graph Variational Auto-Encoders
neurips,2019,2,5716,Krishna,Narayanan,tamu,Texas A&M University,duffieldng@tamu.edu,Semi-Implicit Graph Variational Auto-Encoders
neurips,2019,3,5716,Nick,Duffield,tamu,Texas A&M University,krn@tamu.edu,Semi-Implicit Graph Variational Auto-Encoders
neurips,2019,4,5716,Mingyuan,Zhou,tamu,University of Texas at Austin,xqian@tamu.edu,Semi-Implicit Graph Variational Auto-Encoders
neurips,2019,5,5716,Xiaoning,Qian,utexas,Texas A&M,mingyuan.zhou@mccombs.utexas.edu,Semi-Implicit Graph Variational Auto-Encoders
neurips,2019,0,4453,Minshuo,Chen,gatech,Georgia Tech,mchen393@gatech.edu,Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds
neurips,2019,1,4453,Haoming,Jiang,gatech,Georgia Institute of Technology,jianghm@gatech.edu,Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds
neurips,2019,2,4453,Wenjing,Liao,gatech,Georgia Tech,wliao60@gatech.edu,Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds
neurips,2019,3,4453,Tuo,Zhao,gatech,Georgia Tech,tourzhao@gatech.edu,Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds
neurips,2019,0,7760,Hermina,Petric Maretic,epfl,EPFL,hermina.petricmaretic@epfl.ch,GOT: An Optimal Transport framework for Graph comparison
neurips,2019,1,7760,Mireille,El Gheche,epfl,EPFL,mireille.elgheche@epfl.ch,GOT: An Optimal Transport framework for Graph comparison
neurips,2019,2,7760,Giovanni,Chierchia,esiee,ESIEE Paris,giovanni.chierchia@esiee.fr,GOT: An Optimal Transport framework for Graph comparison
neurips,2019,3,7760,Pascal,Frossard,epfl,EPFL,pascal.frossard@epfl.ch,GOT: An Optimal Transport framework for Graph comparison
neurips,2019,0,6322,Jose,Blanchet,stanford,Stanford University,jose.blanchet@stanford.edu,Multivariate Distributionally Robust Convex Regression under Absolute Error Loss
neurips,2019,1,6322,Peter,Glynn,stanford,Stanford University,junyan65@stanford.edu,Multivariate Distributionally Robust Convex Regression under Absolute Error Loss
neurips,2019,2,6322,Jun,Yan,stanford,Stanford,glynn@stanford.edu,Multivariate Distributionally Robust Convex Regression under Absolute Error Loss
neurips,2019,3,6322,Zhengqing,Zhou,stanford,Stanford University,zqzhou@stanford.edu,Multivariate Distributionally Robust Convex Regression under Absolute Error Loss
neurips,2019,0,5145,Sara,Hooker,google,Google Brain,shooker@google.com,A Benchmark for Interpretability Methods in Deep Neural Networks
neurips,2019,1,5145,Dumitru,Erhan,google,Google Brain,dumitru@google.com,A Benchmark for Interpretability Methods in Deep Neural Networks
neurips,2019,2,5145,Pieter-Jan,Kindermans,google,Google Brain,pikinder@google.com,A Benchmark for Interpretability Methods in Deep Neural Networks
neurips,2019,3,5145,Been,Kim,google,Google,beenkim@google.com,A Benchmark for Interpretability Methods in Deep Neural Networks
neurips,2019,0,7196,Tom,Eccles,google,DeepMind,yorambac@google.com,Biases for Emergent Communication in Multi-agent Reinforcement Learning
neurips,2019,1,7196,Yoram,Bachrach,google,,eccles@google.com,Biases for Emergent Communication in Multi-agent Reinforcement Learning
neurips,2019,2,7196,Guy,Lever,google,Google DeepMind,guylever@google.com,Biases for Emergent Communication in Multi-agent Reinforcement Learning
neurips,2019,3,7196,Angeliki,Lazaridou,google,DeepMind,thore@google.com,Biases for Emergent Communication in Multi-agent Reinforcement Learning
neurips,2019,4,7196,Thore,Graepel,google,DeepMind,angeliki@google.com,Biases for Emergent Communication in Multi-agent Reinforcement Learning
neurips,2019,0,5082,Paul,Micaelli,ed,The University of Edinburgh,paul.micaelli@ed.ac.uk,Zero-shot Knowledge Transfer via Adversarial Belief Matching
neurips,2019,1,5082,Amos,Storkey,ed,University of Edinburgh,a.storkey@ed.ac.uk,Zero-shot Knowledge Transfer via Adversarial Belief Matching
neurips,2019,0,330,Armin,Lederer,tum,Technical University of Munich,armin.lederer@tum.de,Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control
neurips,2019,1,330,Jonas,Umlauft,tum,Technical University of Munich,jonas.umlauft@tum.de,Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control
neurips,2019,2,330,Sandra,Hirche,tum,Technische Universitaet Muenchen,hirche@tum.de,Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control
neurips,2019,0,5205,Yunfei,Teng,nyu,New York University,yt1208@nyu.edu,Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models
neurips,2019,1,5205,Wenbo,Gao,columbia,Columbia University,wg2279@columbia.edu,Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models
neurips,2019,2,5205,François,Chalus,gmail,Credit Suisse,chalusf3@gmail.com,Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models
neurips,2019,3,5205,Anna,Choromanska,nyu,NYU,ac5455@nyu.edu,Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models
neurips,2019,4,5205,Donald,Goldfarb,columbia,Columbia University,goldfarb@columbia.edu,Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models
neurips,2019,5,5205,Adrian,Weller,cam,"Cambridge, Alan Turing Institute",aw665@cam.ac.uk,Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models
neurips,2019,0,1145,Giacomo,De Palma,mit,MIT,gdepalma@mit.edu,Random deep neural networks are biased towards simple functions
neurips,2019,1,1145,Bobak,Kiani,mit,Massachusetts Institute of Technology,bkiani@mit.edu,Random deep neural networks are biased towards simple functions
neurips,2019,2,1145,Seth,Lloyd,mit,MIT,slloyd@mit.edu,Random deep neural networks are biased towards simple functions
neurips,2019,0,5452,Ari,Seff,princeton,Princeton University,aseff@princeton.edu,Discrete Object Generation with Reversible Inductive Construction
neurips,2019,1,5452,Wenda,Zhou,columbia,Columbia University,wz2335@columbia.edu,Discrete Object Generation with Reversible Inductive Construction
neurips,2019,2,5452,Farhan,Damani,princeton,Princeton University,fdamani@princeton.edu,Discrete Object Generation with Reversible Inductive Construction
neurips,2019,3,5452,Abigail,Doyle,princeton,Princeton University,agdoyle@princeton.edu,Discrete Object Generation with Reversible Inductive Construction
neurips,2019,4,5452,Ryan,Adams,princeton,Princeton University,rpa@princeton.edu,Discrete Object Generation with Reversible Inductive Construction
neurips,2019,0,4799,Lun,Huang,,Peking University,,Adaptively Aligned Image Captioning via Adaptive Attention Time
neurips,2019,1,4799,Wenmin,Wang,,Peking University,,Adaptively Aligned Image Captioning via Adaptive Attention Time
neurips,2019,2,4799,Yaxian,Xia,,Peking University,,Adaptively Aligned Image Captioning via Adaptive Attention Time
neurips,2019,3,4799,Jie,Chen,,Peng Cheng Laboratory,,Adaptively Aligned Image Captioning via Adaptive Attention Time
neurips,2019,0,1827,Vincent,Cohen-Addad,ens,CNRS & Sorbonne Université,vcohen@di.ens.fr,Fully Dynamic Consistent Facility Location
neurips,2019,1,1827,Niklas Oskar,Hjuler,ku,University of Copenhagen,hjuler@di.ku.dk,Fully Dynamic Consistent Facility Location
neurips,2019,2,1827,Nikos,Parotsidis,ku,University of Copenhagen,nipa@di.ku.dk,Fully Dynamic Consistent Facility Location
neurips,2019,3,1827,David,Saulpic,uniroma1,Ecole normale supérieure,schwiegelshohn@diag.uniroma1.it,Fully Dynamic Consistent Facility Location
neurips,2019,4,1827,Chris,Schwiegelshohn,lip6,"Sapienza, University of Rome",david.saulpic@lip6.fr,Fully Dynamic Consistent Facility Location
neurips,2019,0,8693,Ravi,Kumar,gmail,Google,ravi.k53@gmail.com,Efficient Rematerialization for Deep Networks
neurips,2019,1,8693,Manish,Purohit,google,Google,mpurohit@google.com,Efficient Rematerialization for Deep Networks
neurips,2019,2,8693,Zoya,Svitkina,google,Google,zoya@google.com,Efficient Rematerialization for Deep Networks
neurips,2019,3,8693,Erik,Vee,google,Google,erikvee@google.com,Efficient Rematerialization for Deep Networks
neurips,2019,4,8693,Joshua,Wang,google,Google,joshuawang@google.com,Efficient Rematerialization for Deep Networks
neurips,2019,0,2302,Ruho,Kondo,tytlabs,"Toyota Central R&D Labs., Inc.",r-kondo@mosk.tytlabs.co.jp,Flow-based Image-to-Image Translation with Feature Disentanglement
neurips,2019,1,2302,Keisuke,Kawano,tytlabs,"Toyota Central R&D Labs., Inc",kawano@mosk.tytlabs.co.jp,Flow-based Image-to-Image Translation with Feature Disentanglement
neurips,2019,2,2302,Satoshi,Koide,tytlabs,Toyota Central R&D Labs.,koide@mosk.tytlabs.co.jp,Flow-based Image-to-Image Translation with Feature Disentanglement
neurips,2019,3,2302,Takuro,Kutsuna,tytlabs,Toyota Central R&D Labs. Inc.,kutsuna@mosk.tytlabs.co.jp,Flow-based Image-to-Image Translation with Feature Disentanglement
