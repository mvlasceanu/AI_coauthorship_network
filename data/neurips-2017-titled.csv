conference,year,index,paper_id,given_name,family_name,org,institution,email,title
neurips,2017,0,3492,Piotr,Dabkowski,cam,Cambridge University,pd437@cam.ac.uk,Real Time Image Saliency for Black Box Classifiers
neurips,2017,1,3492,Yarin,Gal,cam,University of Oxford,yarin.gal@eng.cam.ac.uk,Real Time Image Saliency for Black Box Classifiers
neurips,2017,0,2081,Nicolas,Courty,univ-ubs,IRISA / University South Brittany,courty@univ-ubs.fr,Joint distribution optimal transportation for domain adaptation
neurips,2017,1,2081,Rémi,Flamary,unice,Université Côte d'Azur,remi.flamary@unice.fr,Joint distribution optimal transportation for domain adaptation
neurips,2017,2,2081,Amaury,Habrard,univ-st-etienne,"University of Saint-Etienne, Univ. Lyon, Lab. H Curien, France",amaury.habrard@univ-st-etienne.fr,Joint distribution optimal transportation for domain adaptation
neurips,2017,3,2081,Alain,Rakotomamonjy,insa-rouen,Université de Rouen Normandie,alain.rakoto@insa-rouen.fr,Joint distribution optimal transportation for domain adaptation
neurips,2017,0,2184,Feiping,Nie,gmail,University of Texas Arlington,feipingnie@gmail.com,Learning A Structured Optimal Bipartite Graph for Co-Clustering
neurips,2017,1,2184,Xiaoqian,Wang,gmail,University of Pittsburgh,xqwang1991@gmail.com,Learning A Structured Optimal Bipartite Graph for Co-Clustering
neurips,2017,2,2184,Cheng,Deng,xidian,"School of Electronic Engineering, Xidian University, China",chdeng@mail.xidian.edu.cn,Learning A Structured Optimal Bipartite Graph for Co-Clustering
neurips,2017,3,2184,Heng,Huang,pitt,University of Pittsburgh,heng.huang@pitt.edu,Learning A Structured Optimal Bipartite Graph for Co-Clustering
neurips,2017,0,833,Mohammad Haris,Baig,,Dartmouth College,,Learning to Inpaint for Image Compression
neurips,2017,1,833,Vladlen,Koltun,,Intel Labs,,Learning to Inpaint for Image Compression
neurips,2017,2,833,Lorenzo,Torresani,,Dartmouth/Facebook,,Learning to Inpaint for Image Compression
neurips,2017,0,2213,Robert,Mattila,kth,KTH Royal Institute of Technology,rmattila@kth.se,Inverse Filtering for Hidden Markov Models
neurips,2017,1,2213,Cristian,Rojas,cornell,KTH Royal Institute of Technology,vikramk@cornell.edu,Inverse Filtering for Hidden Markov Models
neurips,2017,2,2213,Vikram,Krishnamurthy,kth,Cornell University,crro@kth.se,Inverse Filtering for Hidden Markov Models
neurips,2017,3,2213,Bo,Wahlberg,kth,KTH Royal Inst. of Technology,bo@kth.se,Inverse Filtering for Hidden Markov Models
neurips,2017,0,3550,Soumendu Sundar,Mukherjee,berkeley,"University of California, Berkeley",soumendu@berkeley.edu,On clustering network-valued data
neurips,2017,1,3550,Purnamrita,Sarkar,utexas,UT Austin,purna.sarkar@austin.utexas.edu,On clustering network-valued data
neurips,2017,2,3550,Lizhen,Lin,nd,The University of Texas at Austin,lizhen.lin@nd.edu,On clustering network-valued data
neurips,2017,0,433,Nanyang,Ye,,University of Cambridge,,Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks
neurips,2017,1,433,Zhanxing,Zhu,,Peking University / Beijing Institute of Big Data Research,,Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks
neurips,2017,2,433,Rafal,Mantiuk,,University of Cambridge,,Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks
neurips,2017,0,2491,Omar,El Housni,columbia,Columbia University,oe2148@columbia.edu,Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization
neurips,2017,1,2491,Vineet,Goyal,columbia,Columbia University,vg2277@columbia.edu,Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization
neurips,2017,0,1329,Eleni,Triantafillou,,University of Toronto,,Few-Shot Learning Through an Information Retrieval  Lens
neurips,2017,1,1329,Richard,Zemel,,University of Toronto,,Few-Shot Learning Through an Information Retrieval  Lens
neurips,2017,2,1329,Raquel,Urtasun,,University of Toronto,,Few-Shot Learning Through an Information Retrieval  Lens
neurips,2017,0,890,Patrick,Rebeschini,ox,University of Oxford,patrick.rebeschini@stats.ox.ac.uk,Accelerated consensus via Min-Sum Splitting
neurips,2017,1,890,Sekhar,Tatikonda,yale,Yale University,sekhar.tatikonda@yale.edu,Accelerated consensus via Min-Sum Splitting
neurips,2017,0,2670,Sean,Welleck,nyu,NYU,wellecks@nyu.edu,Saliency-based Sequential Image Attention with Multiset Prediction
neurips,2017,1,2670,Jialin,Mao,nyu,New York University,jialin.mao@nyu.edu,Saliency-based Sequential Image Attention with Multiset Prediction
neurips,2017,2,2670,Kyunghyun,Cho,nyu,NYU,kyunghyun.cho@nyu.edu,Saliency-based Sequential Image Attention with Multiset Prediction
neurips,2017,3,2670,Zheng,Zhang,nyu,Shanghai New York Univeristy,zz@nyu.edu,Saliency-based Sequential Image Attention with Multiset Prediction
neurips,2017,0,841,Anirban,Roychowdhury,osu,Ohio State University,roychowdhury.7@osu.edu,Adaptive Bayesian Sampling with Monte Carlo EM
neurips,2017,1,841,Srinivasan,Parthasarathy,ohio-state,The Ohio State University,srini@cse.ohio-state.edu,Adaptive Bayesian Sampling with Monte Carlo EM
neurips,2017,0,2121,Phillip,Jang,,Cornell University,,Scalable Levy Process Priors for Spectral Kernel Learning
neurips,2017,1,2121,Andrew,Loeb,,Cornell University,,Scalable Levy Process Priors for Spectral Kernel Learning
neurips,2017,2,2121,Matthew,Davidow,,Cornell University,,Scalable Levy Process Priors for Spectral Kernel Learning
neurips,2017,3,2121,Andrew,Wilson,,Cornell University,,Scalable Levy Process Priors for Spectral Kernel Learning
neurips,2017,0,1707,Rajat,Sen,,University of Texas at Austin,,Model-Powered Conditional Independence Test
neurips,2017,1,1707,Ananda Theertha,Suresh,,Google,,Model-Powered Conditional Independence Test
neurips,2017,2,1707,Karthikeyan,Shanmugam,,"IBM Research, NY",,Model-Powered Conditional Independence Test
neurips,2017,3,1707,Alexandros,Dimakis,,"University of Texas, Austin",,Model-Powered Conditional Independence Test
neurips,2017,4,1707,Sanjay,Shakkottai,,The University of Texas at Austin,,Model-Powered Conditional Independence Test
neurips,2017,0,1019,Mingsheng,Long,uic,Tsinghua University,psyu@uic.edu,Learning Multiple Tasks with Multilinear Relationship Networks
neurips,2017,1,1019,ZHANGJIE,CAO,tsinghua,"School of Software, Tsinghua University",mingsheng@tsinghua.edu.cn,Learning Multiple Tasks with Multilinear Relationship Networks
neurips,2017,2,1019,Jianmin,Wang,tsinghua,Tsinghua University,jimwang@tsinghua.edu.cn,Learning Multiple Tasks with Multilinear Relationship Networks
neurips,2017,3,1019,Philip,Yu,gmail,UIC,caozhangjie14@gmail.com,Learning Multiple Tasks with Multilinear Relationship Networks
neurips,2017,0,2454,Arya,Mazumdar,umass,University of Massachusetts Amherst,arya@cs.umass.edu,Query Complexity of Clustering with Side Information
neurips,2017,1,2454,Barna,Saha,umass,University of Massachusetts Amherst,barna@cs.umass.edu,Query Complexity of Clustering with Side Information
neurips,2017,0,2223,Andreas,Lehrmann,disneyresearch,Disney Research,andreas.lehrmann@disneyresearch.com,Non-parametric Structured Output Networks
neurips,2017,1,2223,Leonid,Sigal,disneyresearch,Disney Research / University of British Columbia,lsigal@disneyresearch.com,Non-parametric Structured Output Networks
neurips,2017,0,2754,Ziyu,Wang,google,Deepmind,ziyu@google.com,Robust Imitation of Diverse Behaviors
neurips,2017,1,2754,Josh,Merel,google,DeepMind,jsmerel@google.com,Robust Imitation of Diverse Behaviors
neurips,2017,2,2754,Scott,Reed,google,Google DeepMind,reedscot@google.com,Robust Imitation of Diverse Behaviors
neurips,2017,3,2754,Nando,de Freitas,google,DeepMind,gregwayne@google.com,Robust Imitation of Diverse Behaviors
neurips,2017,4,2754,Gregory,Wayne,google,Google DeepMind,nandodefreitas@google.com,Robust Imitation of Diverse Behaviors
neurips,2017,5,2754,Nicolas,Heess,google,Google DeepMind,heess@google.com,Robust Imitation of Diverse Behaviors
neurips,2017,0,2048,Idan,Schwartz,technion,Technion,idansc@cs.technion.ac.il,High-Order Attention Models for Visual Question Answering
neurips,2017,1,2048,Alexander,Schwing,illinois,University of Illinois at Urbana-Champaign,aschwing@illinois.edu,High-Order Attention Models for Visual Question Answering
neurips,2017,2,2048,Tamir,Hazan,gmail,Technion,tamir.hazan@gmail.com,High-Order Attention Models for Visual Question Answering
neurips,2017,0,2109,Alessandro,Rudi,,INRIA,,FALKON: An Optimal Large Scale Kernel Method
neurips,2017,1,2109,Luigi,Carratino,,University of Genoa,,FALKON: An Optimal Large Scale Kernel Method
neurips,2017,2,2109,Lorenzo,Rosasco,,University of Genova- MIT - IIT,,FALKON: An Optimal Large Scale Kernel Method
neurips,2017,0,896,Jason,Xu,ucla,NSF Postdoctoral Fellow UCLA,jqxu@ucla.edu,Generalized Linear Model Regression under Distance-to-set Penalties
neurips,2017,1,896,Eric,Chi,ncsu,North Carolina State University,eric_chi@ncsu.edu,Generalized Linear Model Regression under Distance-to-set Penalties
neurips,2017,2,896,Kenneth,Lange,ucla,UCLA,klange@ucla.edu,Generalized Linear Model Regression under Distance-to-set Penalties
neurips,2017,0,1460,Youssef,Mroueh,ibm,IBM T.J Watson Research Center,mroueh@us.ibm.com,Fisher GAN
neurips,2017,1,1460,Tom,Sercu,ibm,IBM Research,tom.sercu1@ibm.com,Fisher GAN
neurips,2017,0,2520,Addison,Hu,yale,Yale University,addison.hu@yale.edu,Minimax Estimation of Bandable Precision Matrices
neurips,2017,1,2520,Sahand,Negahban,yale,Yale University,sahand.negahban@yale.edu,Minimax Estimation of Bandable Precision Matrices
neurips,2017,0,3425,Matthäus,Kleindessner,rutgers,University of Tübingen,mk1572@cs.rutgers.edu,Kernel functions based on triplet comparisons
neurips,2017,1,3425,Ulrike,von Luxburg,uni-tuebingen,University of Tübingen,luxburg@informatik.uni-tuebingen.de,Kernel functions based on triplet comparisons
neurips,2017,0,60,Fabian,Pedregosa,,UC Berkeley / ETH Zurich,,Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization
neurips,2017,1,60,Rémi,Leblond,,INRIA,,Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization
neurips,2017,2,60,Simon,Lacoste-Julien,,Université de Montréal,,Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization
neurips,2017,0,521,Guangcan,Liu,nuist,Nanjing University of Information Science & Technology,gcliu@nuist.edu.cn,A New Theory for Matrix Completion
neurips,2017,1,521,Qingshan,Liu,nuist,,qsliu@nuist.edu.cn,A New Theory for Matrix Completion
neurips,2017,2,521,Xiaotong,Yuan,nuist,,xtyuan@nuist.edu.cn,A New Theory for Matrix Completion
neurips,2017,0,1583,Toan,Tran,,The University of Adelaide,,A Bayesian Data Augmentation Approach for Learning Deep Models
neurips,2017,1,1583,Trung,Pham,,The University of Adelaide,,A Bayesian Data Augmentation Approach for Learning Deep Models
neurips,2017,2,1583,Gustavo,Carneiro,,The University of Adelaide,,A Bayesian Data Augmentation Approach for Learning Deep Models
neurips,2017,3,1583,Lyle,Palmer,,The University of Adelaide,,A Bayesian Data Augmentation Approach for Learning Deep Models
neurips,2017,4,1583,Ian,Reid,,University of Adelaide,,A Bayesian Data Augmentation Approach for Learning Deep Models
neurips,2017,0,1021,Muhammad,Yousefnezhad,nuaa,Nanjing University of Aeronautics and Astronautics,myousefnezhad@nuaa.edu.cn,Deep Hyperalignment
neurips,2017,1,1021,Daoqiang,Zhang,nuaa,Nanjing University of Aeronautics and Astronautics,dqzhang@nuaa.edu.cn,Deep Hyperalignment
neurips,2017,0,249,Jiasen,Lu,,Georgia Tech,,Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model
neurips,2017,1,249,Anitha,Kannan,,,,Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model
neurips,2017,2,249,Jianwei,Yang,,Georgia Tech,,Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model
neurips,2017,3,249,Devi,Parikh,,Georgia Tech / Facebook AI Research (FAIR),,Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model
neurips,2017,4,249,Dhruv,Batra,,,,Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model
neurips,2017,0,2024,Jonathan,Huggins,mit,Massachusetts Institute of Technology,jhuggins@mit.edu,PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference
neurips,2017,1,2024,Ryan,Adams,princeton,,rpa@princeton.edu,PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference
neurips,2017,2,2024,Tamara,Broderick,mit,MIT,tbroderick@csail.mit.edu,PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference
neurips,2017,0,590,Young Hun,Jung,umich,Universith of Michigan,yhjung@umich.edu,Online multiclass boosting
neurips,2017,1,590,Jack,Goetz,umich,University of Michigan,jrgoetz@umich.edu,Online multiclass boosting
neurips,2017,2,590,Ambuj,Tewari,umich,University of Michigan,tewaria@umich.edu,Online multiclass boosting
neurips,2017,0,1677,Yannick,Schroecker,gatech,Georgia Institute of Technology,yannickschroecker@gatech.edu,State Aware Imitation Learning
neurips,2017,1,1677,Charles,Isbell,gatech,Georgia Tech,isbell@cc.gatech.edu,State Aware Imitation Learning
neurips,2017,0,1863,Yi,Xu,uiowa,The University of Iowa,yi-xu@uiowa.edu,Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter
neurips,2017,1,1863,Qihang,Lin,uiowa,University of Iowa,qihang-lin@uiowa.edu,Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter
neurips,2017,2,1863,Tianbao,Yang,uiowa,The University of Iowa,tianbao-yang@uiowa.edu,Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter
neurips,2017,0,1173,Wei-Ning,Hsu,mit,Massachusetts Institute of Technology,wnhsu@csail.mit.edu,Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data
neurips,2017,1,1173,Yu,Zhang,mit,Google Brain,yzhang87@csail.mit.edu,Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data
neurips,2017,2,1173,James,Glass,mit,MIT CSAIL,glass@csail.mit.edu,Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data
neurips,2017,0,3060,Isabeau,Prémont-Schwarz,cai,The Curious Ai Company,isabeau@cai.fi,Recurrent Ladder Networks
neurips,2017,1,3060,Alexander,Ilin,cai,The Curious AI company,alexilin@cai.fi,Recurrent Ladder Networks
neurips,2017,2,3060,Tele,Hao,cai,The Curious AI Company,hotloo@cai.fi,Recurrent Ladder Networks
neurips,2017,3,3060,Antti,Rasmus,cai,,antti@cai.fi,Recurrent Ladder Networks
neurips,2017,4,3060,Rinu,Boney,cai,,rinu@cai.fi,Recurrent Ladder Networks
neurips,2017,5,3060,Harri,Valpola,cai,The Curious AI Company,harri@cai.fi,Recurrent Ladder Networks
neurips,2017,0,2351,Yee,Teh,,DeepMind,,Distral: Robust multitask reinforcement learning
neurips,2017,1,2351,Victor,Bapst,,DeepMind,,Distral: Robust multitask reinforcement learning
neurips,2017,2,2351,Wojciech,Czarnecki,,DeepMind,,Distral: Robust multitask reinforcement learning
neurips,2017,3,2351,John,Quan,,Google DeepMind,,Distral: Robust multitask reinforcement learning
neurips,2017,4,2351,James,Kirkpatrick,,Google DeepMind,,Distral: Robust multitask reinforcement learning
neurips,2017,5,2351,Raia,Hadsell,,DeepMind,,Distral: Robust multitask reinforcement learning
neurips,2017,6,2351,Nicolas,Heess,,Google DeepMind,,Distral: Robust multitask reinforcement learning
neurips,2017,7,2351,Razvan,Pascanu,,Google DeepMind,,Distral: Robust multitask reinforcement learning
neurips,2017,0,2669,arthur,flajolet,mit,MIT,flajolet@mit.edu,Real-Time Bidding with Side Information
neurips,2017,1,2669,Patrick,Jaillet,mit,MIT,jaillet@mit.edu,Real-Time Bidding with Side Information
neurips,2017,0,369,Yu-Chuan,Su,,UT Austin,,Learning Spherical Convolution for Fast Features from 360° Imagery
neurips,2017,1,369,Kristen,Grauman,,University of Texas at Austin,,Learning Spherical Convolution for Fast Features from 360° Imagery
neurips,2017,0,2799,Luiz,Chamon,upenn,University of Pennsylvania,luizf@seas.upenn.edu,Approximate Supermodularity Bounds for Experimental Design
neurips,2017,1,2799,Alejandro,Ribeiro,upenn,University of Pennsylvania,aribeiro@seas.upenn.edu,Approximate Supermodularity Bounds for Experimental Design
neurips,2017,0,1347,Fan,Yang,cmu,Carnegie Mellon University,fanyang1@cs.cmu.edu,Differentiable Learning of Logical Rules for Knowledge Base Reasoning
neurips,2017,1,1347,Zhilin,Yang,cmu,Carnegie Mellon University,zhiliny@cs.cmu.edu,Differentiable Learning of Logical Rules for Knowledge Base Reasoning
neurips,2017,2,1347,William,Cohen,cmu,Carnegie Mellon University,wcohen@cs.cmu.edu,Differentiable Learning of Logical Rules for Knowledge Base Reasoning
neurips,2017,0,3507,Mert,Gurbuzbalaban,,Rutgers University,,When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent
neurips,2017,1,3507,Asuman,Ozdaglar,,Massachusetts Institute of Technology,,When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent
neurips,2017,2,3507,Pablo,Parrilo,,Massachusetts Institute of Technology,,When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent
neurips,2017,3,3507,Nuri,Vanli,,Massachusetts Institute of Technology,,When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent
neurips,2017,0,1594,Michael,Hauser,psu,Pennsylvania State University,mzh190@psu.edu,Principles of Riemannian Geometry  in Neural Networks
neurips,2017,1,1594,Asok,Ray,psu,Pennsylvania State University,axr2@psu.edu,Principles of Riemannian Geometry  in Neural Networks
neurips,2017,0,1715,Hanul,Shin,mit,Massachusetts Institute of Technology,skyshin@mit.edu,Continual Learning with Deep Generative Replay
neurips,2017,1,1715,Jung Kwon,Lee,sktbrain,SK T-Brain,jklee@sktbrain.com,Continual Learning with Deep Generative Replay
neurips,2017,2,1715,Jaehong,Kim,sktbrain,T-Brain,xhark@sktbrain.com,Continual Learning with Deep Generative Replay
neurips,2017,3,1715,Jiwon,Kim,sktbrain,SK T-Brain,jk@sktbrain.com,Continual Learning with Deep Generative Replay
neurips,2017,0,1512,Jeffrey,Pennington,google,Google Brain,jpennin@google.com,Nonlinear random matrix theory for deep learning
neurips,2017,1,1512,Pratik,Worah,google,Google,pworah@google.com,Nonlinear random matrix theory for deep learning
neurips,2017,0,2748,Stefanos,Eleftheriadis,prowler,PROWLER.io,stefanos@prowler.io,Identification of Gaussian Process State Space Models
neurips,2017,1,2748,Tom,Nicholson,prowler,PROWLER.IO,tom@prowler.io,Identification of Gaussian Process State Space Models
neurips,2017,2,2748,Marc,Deisenroth,prowler,Imperial College London,marc@prowler.io,Identification of Gaussian Process State Space Models
neurips,2017,3,2748,James,Hensman,prowler,PROWLER.io,james@prowler.io,Identification of Gaussian Process State Space Models
neurips,2017,0,1641,Xiaohan,Wei,usc,University of Southern California,minsker@usc.edu,Estimation of the covariance structure of heavy-tailed distributions
neurips,2017,1,1641,Stanislav,Minsker,usc,USC,xiaohanw@usc.edu,Estimation of the covariance structure of heavy-tailed distributions
neurips,2017,0,2462,Robert,Chen,,Harvard University,,Robust Optimization for Non-Convex Objectives
neurips,2017,1,2462,Brendan,Lucier,,Microsoft Research,,Robust Optimization for Non-Convex Objectives
neurips,2017,2,2462,Yaron,Singer,,Harvard University,,Robust Optimization for Non-Convex Objectives
neurips,2017,3,2462,Vasilis,Syrgkanis,,Microsoft Research,,Robust Optimization for Non-Convex Objectives
neurips,2017,0,3037,Behnam,Neyshabur,ttic,Institute for Advanced Study,bneyshabur@ttic.edu,Exploring Generalization in Deep Learning
neurips,2017,1,3037,Srinadh,Bhojanapalli,ttic,Toyota Technological Institute at Chicago,srinadh@ttic.edu,Exploring Generalization in Deep Learning
neurips,2017,2,3037,David,Mcallester,ttic,Toyota Tech Institute Chicago,mcallester@ttic.edu,Exploring Generalization in Deep Learning
neurips,2017,3,3037,Nati,Srebro,ttic,TTI-Chicago,nati@ttic.edu,Exploring Generalization in Deep Learning
neurips,2017,0,1952,Wouter,Boomsma,ku,University of Copenhagen,wb@di.ku.dk,Spherical convolutions and their application in molecular modelling
neurips,2017,1,1952,Jes,Frellsen,itu,IT University of Copenhagen,jefr@itu.dk,Spherical convolutions and their application in molecular modelling
neurips,2017,0,2290,Sebastian,Stich,epfl,EPFL,sebastian.stich@epfl.ch,Safe Adaptive Importance Sampling
neurips,2017,1,2290,Anant,Raj,mpg,Max Planck Institute for Intelligent Systems,anant.raj@tuebingen.mpg.de,Safe Adaptive Importance Sampling
neurips,2017,2,2290,Martin,Jaggi,epfl,EPFL,martin.jaggi@epfl.ch,Safe Adaptive Importance Sampling
neurips,2017,0,556,Long,Jin,ucsd,University of California San Diego,longjin@ucsd.edu,Introspective Classification with Convolutional Nets
neurips,2017,1,556,Justin,Lazarow,ucsd,UC San Diego,jlazarow@ucsd.edu,Introspective Classification with Convolutional Nets
neurips,2017,2,556,Zhuowen,Tu,ucsd,"University of California, San Diego",ztu@ucsd.edu,Introspective Classification with Convolutional Nets
neurips,2017,0,2792,Harm,Van Seijen,microsoft,Microsoft Research,harm.vanseijen@microsoft.com,Hybrid Reward Architecture for Reinforcement Learning
neurips,2017,1,2792,Mehdi,Fatemi,microsoft,Microsoft,mehdi.fatemi@microsoft.com,Hybrid Reward Architecture for Reinforcement Learning
neurips,2017,2,2792,Joshua,Romoff,mcgill,McGill University,joshua.romoff@mail.mcgill.ca,Hybrid Reward Architecture for Reinforcement Learning
neurips,2017,3,2792,Romain,Laroche,microsoft,Microsoft Research Maluuba,romain.laroche@microsoft.com,Hybrid Reward Architecture for Reinforcement Learning
neurips,2017,4,2792,Tavian,Barnes,microsoft,Microsoft,tavian.barnes@microsoft.com,Hybrid Reward Architecture for Reinforcement Learning
neurips,2017,5,2792,Jeffrey,Tsang,microsoft,,tsang.jeffrey@microsoft.com,Hybrid Reward Architecture for Reinforcement Learning
neurips,2017,0,3209,Chris,Russell,turing,The Alan Turing Institute/ The University of Surrey,crussell@turing.ac.uk,When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness
neurips,2017,1,3209,Matt,Kusner,turing,Alan Turing Institute,mkusner@turing.ac.uk,When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness
neurips,2017,2,3209,Joshua,Loftus,nyu,The Alan Turing Institute,loftus@nyu.edu,When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness
neurips,2017,3,3209,Ricardo,Silva,ucl,ucl.ac.uk,ricardo@stats.ucl.ac.uk,When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness
neurips,2017,0,2876,Yujia,Li,toronto,University of Toronto,yujiali@cs.toronto.edu,Dualing GANs
neurips,2017,1,2876,Alexander,Schwing,toronto,University of Illinois at Urbana-Champaign,wangkua1@cs.toronto.edu,Dualing GANs
neurips,2017,2,2876,Kuan-Chieh,Wang,toronto,University of Toronto,zemel@cs.toronto.edu,Dualing GANs
neurips,2017,3,2876,Richard,Zemel,illinois,University of Toronto,aschwing@illinois.edu,Dualing GANs
neurips,2017,0,1937,Ashkan,Panahi,ncsu,North Carolina State University,apanahi@ncsu.edu,A Universal Analysis of Large-Scale Regularized Least Squares Solutions
neurips,2017,1,1937,Babak,Hassibi,caltech,Caltech,hassibi@caltech.edu,A Universal Analysis of Large-Scale Regularized Least Squares Solutions
neurips,2017,0,443,Chris Junchi,Li,princeton,Princeton University,junchil@princeton.edu,Diffusion Approximations for Online Principal Component Estimation and Global Convergence
neurips,2017,1,443,Mengdi,Wang,princeton,Princeton University,mengdiw@princeton.edu,Diffusion Approximations for Online Principal Component Estimation and Global Convergence
neurips,2017,2,443,Han,Liu,princeton,Tencent AI Lab,hanliu@princeton.edu,Diffusion Approximations for Online Principal Component Estimation and Global Convergence
neurips,2017,3,443,Tong,Zhang,tongzhang-ml,Tencent AI Lab,tongzhang@tongzhang-ml.org,Diffusion Approximations for Online Principal Component Estimation and Global Convergence
neurips,2017,0,224,Cong Han,Lim,wisc,University of Wisconsin-Madison,clim9@wisc.edu,k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms
neurips,2017,1,224,Stephen,Wright,wisc,UW-Madison,swright@cs.wisc.edu,k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms
neurips,2017,0,3544,Yu-Xiong,Wang,cmu,Carnegie Mellon University,yuxiongw@cs.cmu.edu,Learning to Model the Tail
neurips,2017,1,3544,Deva,Ramanan,cmu,Carnegie Mellon University,dramanan@cs.cmu.edu,Learning to Model the Tail
neurips,2017,2,3544,Martial,Hebert,cmu,cmu,hebert@cs.cmu.edu,Learning to Model the Tail
neurips,2017,0,3376,Volodymyr,Kuleshov,stanford,Stanford University,kuleshov@cs.stanford.edu,Neural Variational Inference and Learning in Undirected Graphical Models
neurips,2017,1,3376,Stefano,Ermon,stanford,Stanford,ermon@cs.stanford.edu,Neural Variational Inference and Learning in Undirected Graphical Models
neurips,2017,0,2192,Bikash,Joshi,imag,University of Grenoble Alpes,bikash.joshi@imag.fr,Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification
neurips,2017,1,2192,Massih R.,Amini,imag,University Grenoble Alps,massih-reza.amini@imag.fr,Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification
neurips,2017,2,2192,Ioannis,Partalas,expedia,Expedia LPS Geneva,ipartalas@expedia.com,Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification
neurips,2017,3,2192,Franck,Iutzeler,imag,Univ. Grenoble Alpes,franck.iutzeler@imag.fr,Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification
neurips,2017,4,2192,Yury,Maximov,lanl,Los Alamos National Laboratory and  Skolkovo Institute of Science and Technology,yury@lanl.gov,Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification
neurips,2017,0,3368,Elad,Hazan,princeton,Princeton University,ehazan@cs.princeton.edu,Learning Linear Dynamical Systems via Spectral Filtering
neurips,2017,1,3368,Karan,Singh,princeton,Princeton University,karans@cs.princeton.edu,Learning Linear Dynamical Systems via Spectral Filtering
neurips,2017,2,3368,Cyril,Zhang,princeton,Princeton University,cyril.zhang@cs.princeton.edu,Learning Linear Dynamical Systems via Spectral Filtering
neurips,2017,0,2656,Zhenwen,Dai,amazon,Amazon,zhenwend@amazon.com,Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes
neurips,2017,1,2656,Mauricio,Álvarez,sheffield,University of Sheffield,mauricio.alvarez@sheffield.ac.uk,Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes
neurips,2017,2,2656,Neil,Lawrence,amazon,Amazon.com,lawrennd@amazon.com,Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes
neurips,2017,0,286,Wei-Sheng,Lai,,"University of California, Merced",,Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks
neurips,2017,1,286,Jia-Bin,Huang,,Virginia Tech,,Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks
neurips,2017,2,286,Ming-Hsuan,Yang,,UC Merced,,Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks
neurips,2017,0,292,Jonathan,Scarlett,ep,EPFL,jonathan.scarlett@ep.ch,Phase Transitions in the Pooled Data Problem
neurips,2017,1,292,Volkan,Cevher,ep,EPFL,volkan.cevher@ep.ch,Phase Transitions in the Pooled Data Problem
neurips,2017,0,2921,Christoph,Dann,cdann,Carnegie Mellon University,cdann@cdann.net,Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning
neurips,2017,1,2921,Tor,Lattimore,gmail,DeepMind,tor.lattimore@gmail.com,Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning
neurips,2017,2,2921,Emma,Brunskill,stanford,Stanford University,ebrun@cs.stanford.edu,Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning
neurips,2017,0,1762,Qiang,Liu,dartmouth,Dartmouth College,qiang.liu@dartmouth.edu,Stein Variational Gradient Descent as Gradient Flow
neurips,2017,0,1327,Futoshi,Futami,u-tokyo,University of Tokyo/RIKEN,futami@ms.k.u-tokyo.ac.jp,Expectation Propagation for t-Exponential Family Using q-Algebra
neurips,2017,1,1327,Issei,Sato,u-tokyo,The University of Tokyo/RIKEN,sato@k.u-tokyo.ac.jp,Expectation Propagation for t-Exponential Family Using q-Algebra
neurips,2017,2,1327,Masashi,Sugiyama,u-tokyo,RIKEN / University of Tokyo,sugi@k.u-tokyo.ac.jp,Expectation Propagation for t-Exponential Family Using q-Algebra
neurips,2017,0,1404,Avrim,Blum,ttic,Toyota Technological Institute at Chicago,avrim@ttic.edu,Collaborative PAC Learning
neurips,2017,1,1404,Nika,Haghtalab,cmu,Carnegie Mellon University,nhaghtal@cs.cmu.edu,Collaborative PAC Learning
neurips,2017,2,1404,Ariel,Procaccia,cmu,Carnegie Mellon University,arielpro@cs.cmu.edu,Collaborative PAC Learning
neurips,2017,3,1404,Mingda,Qiao,tsinghua,Tsinghua University,qmd14@mails.tsinghua.edu.cn,Collaborative PAC Learning
neurips,2017,0,2592,Chengtao,Li,mit,MIT,ctli@mit.edu,Polynomial time algorithms for dual volume sampling
neurips,2017,1,2592,Stefanie,Jegelka,mit,MIT,stefje@csail.mit.edu,Polynomial time algorithms for dual volume sampling
neurips,2017,2,2592,Suvrit,Sra,mit,MIT,suvrit@mit.edu,Polynomial time algorithms for dual volume sampling
neurips,2017,0,1567,Mingzhe,Wang,,University of Michigan,,Premise Selection for Theorem Proving by Deep Graph Embedding
neurips,2017,1,1567,Yihe,Tang,,Carnegie Mellon University,,Premise Selection for Theorem Proving by Deep Graph Embedding
neurips,2017,2,1567,Jian,Wang,,University of Michigan,,Premise Selection for Theorem Proving by Deep Graph Embedding
neurips,2017,3,1567,Jia,Deng,,University of Michigan,,Premise Selection for Theorem Proving by Deep Graph Embedding
neurips,2017,0,668,Josip,Djolonga,ethz,ETH Zurich,josipd@inf.ethz.ch,Differentiable Learning of Submodular Models
neurips,2017,1,668,Andreas,Krause,ethz,ETHZ,krausea@ethz.ch,Differentiable Learning of Submodular Models
neurips,2017,0,2138,Jin Hyung,Lee,,Columbia University,,YASS: Yet Another Spike Sorter
neurips,2017,1,2138,David,Carlson,,Duke University,,YASS: Yet Another Spike Sorter
neurips,2017,2,2138,Hooshmand,Shokri Razaghi,,Columbia University,,YASS: Yet Another Spike Sorter
neurips,2017,3,2138,Weichi,Yao,,Columbia University,,YASS: Yet Another Spike Sorter
neurips,2017,4,2138,Georges,Goetz,,Stanford University,,YASS: Yet Another Spike Sorter
neurips,2017,5,2138,Espen,Hagen,,,,YASS: Yet Another Spike Sorter
neurips,2017,6,2138,Eleanor,Batty,,Columbia University,,YASS: Yet Another Spike Sorter
neurips,2017,7,2138,E.J.,Chichilnisky,,Stanford University,,YASS: Yet Another Spike Sorter
neurips,2017,8,2138,Gaute,Einevoll,,Norwegian University of Life Sciences,,YASS: Yet Another Spike Sorter
neurips,2017,9,2138,Liam,Paninski,,Columbia University,,YASS: Yet Another Spike Sorter
neurips,2017,0,2096,Dario,Zanca,unifi,"University of Florence, University of Siena",dario.zanca@unifi.it,Variational Laws of Visual Attention for Dynamic Scenes
neurips,2017,1,2096,Marco,Gori,unisi,University of Siena,marco@diism.unisi.it,Variational Laws of Visual Attention for Dynamic Scenes
neurips,2017,0,1459,Amirhossein,Taghvaei,illinois,University of Illinois at Urbana-Champaign,taghvae2@illinois.edu,How regularization affects the critical points in linear networks
neurips,2017,1,1459,Jin,Kim,illinois,University of Illinois,jkim684@illinois.edu,How regularization affects the critical points in linear networks
neurips,2017,2,1459,Prashant,Mehta,illinois,University of Illinois,mehtapg@illinois.edu,How regularization affects the critical points in linear networks
neurips,2017,0,2119,Masaaki,Imaizumi,ism,Institute of Statistical Mathematics / RIKEN,imaizumi@ism.ac.jp,On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm
neurips,2017,1,2119,Takanori,Maehara,riken,RIKEN AIP,takanori.maehara@riken.jp,On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm
neurips,2017,2,2119,Kohei,Hayashi,gmail,AIST / RIKEN,hayashi.kohei@gmail.com,On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm
neurips,2017,0,1489,Justin,Fu,berkeley,UC Berkeley,justinfu@eecs.berkeley.edu,EX2: Exploration with Exemplar Models for Deep Reinforcement Learning
neurips,2017,1,1489,John,Co-Reyes,berkeley,UC Berkeley,jcoreyes@eecs.berkeley.edu,EX2: Exploration with Exemplar Models for Deep Reinforcement Learning
neurips,2017,2,1489,Sergey,Levine,berkeley,UC Berkeley,svlevine@eecs.berkeley.edu,EX2: Exploration with Exemplar Models for Deep Reinforcement Learning
neurips,2017,0,2965,Hao,Li,umd,"University of Maryland, College Park",haoli@cs.umd.edu,Training Quantized Nets: A Deeper Understanding
neurips,2017,1,2965,Soham,De,umd,"University of Maryland, College Park",sohamde@cs.umd.edu,Training Quantized Nets: A Deeper Understanding
neurips,2017,2,2965,Zheng,Xu,umd,"University of Maryland, College Park",xuzh@cs.umd.edu,Training Quantized Nets: A Deeper Understanding
neurips,2017,3,2965,Christoph,Studer,umd,Cornell University,hjs@cs.umd.edu,Training Quantized Nets: A Deeper Understanding
neurips,2017,4,2965,Hanan,Samet,umd,University of Maryland at College Park,tomg@cs.umd.edu,Training Quantized Nets: A Deeper Understanding
neurips,2017,5,2965,Tom,Goldstein,cornell,University of Maryland,studer@cornell.edu,Training Quantized Nets: A Deeper Understanding
neurips,2017,0,1636,Mark,van der Wilk,,University of Cambridge,,Convolutional Gaussian Processes
neurips,2017,1,1636,Carl Edward,Rasmussen,,University of Cambridge,,Convolutional Gaussian Processes
neurips,2017,2,1636,James,Hensman,,PROWLER.io,,Convolutional Gaussian Processes
neurips,2017,0,963,Omer,Ben-Porat,technion,Technion – Israel Institute of Technology,omerbp@campus.technion.ac.il,Best Response Regression
neurips,2017,1,963,Moshe,Tennenholtz,technion,Technion--Israel Institute of Technology,moshet@ie.technion.ac.il,Best Response Regression
neurips,2017,0,1282,Zelda,Mariet,mit,MIT,zelda@csail.mit.edu,Elementary Symmetric Polynomials for Optimal Experimental Design
neurips,2017,1,1282,Suvrit,Sra,mit,MIT,suvrit@mit.edu,Elementary Symmetric Polynomials for Optimal Experimental Design
neurips,2017,0,2887,Takashi,Ishida,ms,"Sumitomo Mitsui Asset Management, The University of Tokyo, RIKEN",ishida@ms.,Learning from Complementary Labels
neurips,2017,1,2887,Gang,Niu,ms,The University of Tokyo / RIKEN,gang@ms.,Learning from Complementary Labels
neurips,2017,2,2887,Weihua,Hu,ms,The University of Tokyo,hu@ms.,Learning from Complementary Labels
neurips,2017,3,2887,Masashi,Sugiyama,u-tokyo,RIKEN / University of Tokyo,sugi@k.u-tokyo.ac.jp,Learning from Complementary Labels
neurips,2017,0,1807,Qi,Lou,uci,UCI,qlou@ics.uci.edu,Dynamic Importance Sampling for Anytime Bounds of the Partition Function
neurips,2017,1,1807,Rina,Dechter,uci,UCI,dechter@ics.uci.edu,Dynamic Importance Sampling for Anytime Bounds of the Partition Function
neurips,2017,2,1807,Alexander,Ihler,uci,UC Irvine,ihler@ics.uci.edu,Dynamic Importance Sampling for Anytime Bounds of the Partition Function
neurips,2017,0,1946,Pratibha,Vellanki,deakin,Deakin University,pratibha.vellanki@deakin.edu.au,Process-constrained batch Bayesian optimisation
neurips,2017,1,1946,Santu,Rana,deakin,Deakin University,santu.rana@deakin.edu.au,Process-constrained batch Bayesian optimisation
neurips,2017,2,1946,Sunil,Gupta,deakin,Deakin University,sunil.gupta@deakin.edu.au,Process-constrained batch Bayesian optimisation
neurips,2017,3,1946,David,Rubin,deakin,,svetha.venkatesh@deakin.edu.au,Process-constrained batch Bayesian optimisation
neurips,2017,4,1946,Alessandra,Sutti,deakin,Deakin University,d.rubindecelisleal@deakin.edu.au,Process-constrained batch Bayesian optimisation
neurips,2017,5,1946,Thomas,Dorin,deakin,Deakin University,alessandra.sutti@deakin.edu.au,Process-constrained batch Bayesian optimisation
neurips,2017,6,1946,Murray,Height,deakin,Deakin University,thomas.dorin@deakin.edu.au,Process-constrained batch Bayesian optimisation
neurips,2017,7,1946,Paul,Sanders,deakin,,murray.height@deakin.edu.au,Process-constrained batch Bayesian optimisation
neurips,2017,8,1946,Svetha,Venkatesh,mtu,Deakin University,sanders@mtu.edu,Process-constrained batch Bayesian optimisation
neurips,2017,0,172,Mark,Rowland,cam,University of Cambridge,mr504@cam.ac.uk,Uprooting and Rerooting Higher-Order Graphical Models
neurips,2017,1,172,Adrian,Weller,cam,University of Cambridge,aw665@cam.ac.uk,Uprooting and Rerooting Higher-Order Graphical Models
neurips,2017,0,3167,Bryan,McCann,salesforce,Salesforce Research,bmccann@salesforce.com,Learned in Translation: Contextualized Word Vectors
neurips,2017,1,3167,James,Bradbury,salesforce,Salesforce Research,james.bradbury@salesforce.com,Learned in Translation: Contextualized Word Vectors
neurips,2017,2,3167,Caiming,Xiong,salesforce,Salesforce Research,cxiong@salesforce.com,Learned in Translation: Contextualized Word Vectors
neurips,2017,3,3167,Richard,Socher,salesforce,MetaMind,rsocher@salesforce.com,Learned in Translation: Contextualized Word Vectors
neurips,2017,0,3256,Arya,Mazumdar,umass,University of Massachusetts Amherst,arya@cs.umass.edu,"Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding"
neurips,2017,1,3256,Soumyabrata,Pal,umass,University of Massachusetts Amherst,soumyabratap@umass.edu,"Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding"
neurips,2017,0,2653,Hyeonwoo,Noh,postech,POSTECH,shgusdngogo@postech.ac.kr,Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization
neurips,2017,1,2653,Tackgeun,You,postech,POSTECH,tackgeun.you@postech.ac.kr,Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization
neurips,2017,2,2653,Jonghwan,Mun,postech,POSTECH,choco1916@postech.ac.kr,Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization
neurips,2017,3,2653,Bohyung,Han,postech,POSTECH,bhhan@postech.ac.kr,Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization
neurips,2017,0,3343,Saeid,Motiian,wvu,West Virginia University,samotiian@mix.wvu.edu,Few-Shot Adversarial Domain Adaptation
neurips,2017,1,3343,Quinn,Jones,wvu,West Virginia University,qjones1@mix.wvu.edu,Few-Shot Adversarial Domain Adaptation
neurips,2017,2,3343,Seyed,Iranmanesh,wvu,West Virginia University,seiranmanesh@mix.wvu.edu,Few-Shot Adversarial Domain Adaptation
neurips,2017,3,3343,Gianfranco,Doretto,wvu,West Virginia University,gidoretto@mix.wvu.edu,Few-Shot Adversarial Domain Adaptation
neurips,2017,0,3156,Taylor,Killian,,Harvard University,,Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes
neurips,2017,1,3156,Samuel,Daulton,,Harvard University,,Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes
neurips,2017,2,3156,George,Konidaris,,Brown University,,Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes
neurips,2017,3,3156,Finale,Doshi-Velez,,Harvard,,Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes
neurips,2017,0,2811,Christos,Dimitrakakis,,Chalmers / Harvard / Lille / Oslo,,Multi-View Decision Processes: The Helper-AI Problem
neurips,2017,1,2811,David,Parkes,,Harvard University,,Multi-View Decision Processes: The Helper-AI Problem
neurips,2017,2,2811,Goran,Radanovic,,Harvard,,Multi-View Decision Processes: The Helper-AI Problem
neurips,2017,3,2811,Paul,Tylkin,,Harvard University,,Multi-View Decision Processes: The Helper-AI Problem
neurips,2017,0,2558,Alexandre,Drouin,ulaval,Université Laval + Element AI,alexandre.drouin.8@ulaval.ca,Maximum Margin Interval Trees
neurips,2017,1,2558,Toby,Hocking,r-project,"McGill Genome Center, McGill University",toby.hocking@r-project.org,Maximum Margin Interval Trees
neurips,2017,2,2558,Francois,Laviolette,ulaval,Université Laval,francois.laviolette@ift.ulaval.ca,Maximum Margin Interval Trees
neurips,2017,0,2743,Ofer,Dekel,microsoft,Microsoft Research,oferd@microsoft.com,Online Learning with a Hint
neurips,2017,1,2743,arthur,flajolet,cmu,MIT,nika@cmu.edu,Online Learning with a Hint
neurips,2017,2,2743,Nika,Haghtalab,mit,Carnegie Mellon University,flajolet@mit.edu,Online Learning with a Hint
neurips,2017,3,2743,Patrick,Jaillet,mit,MIT,jaillet@mit.edu,Online Learning with a Hint
neurips,2017,0,870,Kartik,Ahuja,ucla,"University of California, Los Angeles",ahujak@ucla.edu,DPSCREEN: Dynamic Personalized Screening
neurips,2017,1,870,William,Zame,ucla,UCLA,zame@econ.ucla.edu,DPSCREEN: Dynamic Personalized Screening
neurips,2017,2,870,Mihaela,van der Schaar,ox,UCLA and Oxford University,mihaela.vanderschaar@oxford-man.ox.ac.uk,DPSCREEN: Dynamic Personalized Screening
neurips,2017,0,2353,M. Sevi,Baltaoglu,cornell,Cornell University,msb372@cornell.edu,Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions
neurips,2017,1,2353,Lang,Tong,cornell,Cornell University,lt35@cornell.edu,Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions
neurips,2017,2,2353,Qing,Zhao,cornell,Cornell University,qz16@cornell.edu,Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions
neurips,2017,0,2661,Jiaming,Song,stanford,Stanford University,tsong@cs.stanford.edu,A-NICE-MC: Adversarial Training for MCMC
neurips,2017,1,2661,Shengjia,Zhao,stanford,Stanford University,zhaosj12@cs.stanford.edu,A-NICE-MC: Adversarial Training for MCMC
neurips,2017,2,2661,Stefano,Ermon,stanford,Stanford,ermon@cs.stanford.edu,A-NICE-MC: Adversarial Training for MCMC
neurips,2017,0,683,Anselm,Rothe,nyu,New York University,anselm@nyu.edu,Question Asking as Program Generation
neurips,2017,1,683,Brenden,Lake,nyu,New York University,brenden@nyu.edu,Question Asking as Program Generation
neurips,2017,2,683,Todd,Gureckis,nyu,New York University,todd.gureckis@nyu.edu,Question Asking as Program Generation
neurips,2017,0,2992,Hamed,Hassani,upenn,UPenn,hassani@seas.upenn.edu,Gradient Methods for Submodular Maximization
neurips,2017,1,2992,Mahdi,Soltanolkotabi,usc,University of Southern california,soltanol@usc.edu,Gradient Methods for Submodular Maximization
neurips,2017,2,2992,Amin,Karbasi,yale,Yale,amin.karbasi@yale.edu,Gradient Methods for Submodular Maximization
neurips,2017,0,456,Novi,Quadrianto,sussex,University of Sussex and HSE,n.quadrianto@sussex.ac.uk,Recycling Privileged Learning and Distribution Matching for Fairness
neurips,2017,1,456,Viktoriia,Sharmanska,gmail,Imperial College London,sharmanska.v@gmail.com,Recycling Privileged Learning and Distribution Matching for Fairness
neurips,2017,0,2012,Bolin,Ding,microsoft,Microsoft,bolind@microsoft.com,Collecting Telemetry Data Privately
neurips,2017,1,2012,Janardhan,Kulkarni,microsoft,Microsoft Research,jakul@microsoft.com,Collecting Telemetry Data Privately
neurips,2017,2,2012,Sergey,Yekhanin,microsoft,Microsoft,yekhanin@microsoft.com,Collecting Telemetry Data Privately
neurips,2017,0,1521,Matthew,Staib,mit,MIT,mstaib@mit.edu,Parallel Streaming Wasserstein Barycenters
neurips,2017,1,1521,Sebastian,Claici,mit,MIT,sclaici@mit.edu,Parallel Streaming Wasserstein Barycenters
neurips,2017,2,1521,Justin,Solomon,mit,MIT,jsolomon@mit.edu,Parallel Streaming Wasserstein Barycenters
neurips,2017,3,1521,Stefanie,Jegelka,mit,MIT,stefje@mit.edu,Parallel Streaming Wasserstein Barycenters
neurips,2017,0,1758,Mingrui,Liu,uiowa,The University of Iowa,mingrui-liu@uiowa.edu,"Adaptive Accelerated Gradient Converging Method under H\""{o}lderian Error Bound Condition"
neurips,2017,1,1758,Tianbao,Yang,uiowa,The University of Iowa,tianbao-yang@uiowa.edu,"Adaptive Accelerated Gradient Converging Method under H\""{o}lderian Error Bound Condition"
neurips,2017,0,2871,Alex,Kendall,cam,University of Cambridge,agk34@cam.ac.uk,What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?
neurips,2017,1,2871,Yarin,Gal,cam,University of Oxford,yg279@cam.ac.uk,What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?
neurips,2017,0,2377,Erinc,Merdivan,ait,Austrian Institute of Technology GmbH,erinc.merdivan@ait.ac.at,Reconstruct & Crush Network
neurips,2017,1,2377,Mohammad Reza,Loghmani,tuwien,TU Wien,loghmani@acin.tuwien.ac.at,Reconstruct & Crush Network
neurips,2017,2,2377,Matthieu,Geist,univ-lorraine,Université de Lorraine,matthieu.geist@univ-lorraine.fr,Reconstruct & Crush Network
neurips,2017,0,2985,Yuhao,Wang,mit,MIT,yuhaow@mit.edu,Permutation-based Causal Inference Algorithms with Interventions
neurips,2017,1,2985,Liam,Solus,kth,KTH Royal Institute of Technology,solus@kth.se,Permutation-based Causal Inference Algorithms with Interventions
neurips,2017,2,2985,Karren,Yang,mit,MIT,karren@mit.edu,Permutation-based Causal Inference Algorithms with Interventions
neurips,2017,3,2985,Caroline,Uhler,mit,Massachusetts Institute of Technology,cuhler@mit.edu,Permutation-based Causal Inference Algorithms with Interventions
neurips,2017,0,1039,Chengyue,Gong,pku,Peking University,cygong@pku.edu.cn,Deep Dynamic Poisson Factorization Model
neurips,2017,1,1039,win-bin,huang,pku,peking university,huangwb@pku.edu.cn,Deep Dynamic Poisson Factorization Model
neurips,2017,0,77,Kwang-Sung,Jun,wisc,UW-Madison,kjun@discovery.wisc.edu,Scalable Generalized Linear Bandits: Online Computation and Hashing
neurips,2017,1,77,Aniruddha,Bhargava,wisc,University of Wisconsin-Madison,aniruddha@wisc.edu,Scalable Generalized Linear Bandits: Online Computation and Hashing
neurips,2017,2,77,Robert,Nowak,wisc,University of Wisconsion-Madison,rdnowak@wisc.edu,Scalable Generalized Linear Bandits: Online Computation and Hashing
neurips,2017,3,77,Rebecca,Willett,wisc,University of Wisconsin,willett@discovery.wisc.edu,Scalable Generalized Linear Bandits: Online Computation and Hashing
neurips,2017,0,3536,Murat,Kocaoglu,utexas,University of Texas at Austin,mkocaoglu@utexas.edu,Experimental Design for Learning Causal Graphs with Latent Variables
neurips,2017,1,3536,Karthikeyan,Shanmugam,ibm,"IBM Research, NY",karthikeyan.shanmugam2@ibm.com,Experimental Design for Learning Causal Graphs with Latent Variables
neurips,2017,2,3536,Elias,Bareinboim,purdue,Purdue,eb@purdue.edu,Experimental Design for Learning Causal Graphs with Latent Variables
neurips,2017,0,532,Jonathan,Peck,,Ghent University,,Lower bounds on the robustness to adversarial perturbations
neurips,2017,1,532,Joris,Roels,,Ghent University,,Lower bounds on the robustness to adversarial perturbations
neurips,2017,2,532,Bart,Goossens,,Ghent University,,Lower bounds on the robustness to adversarial perturbations
neurips,2017,3,532,Yvan,Saeys,,Ghent University,,Lower bounds on the robustness to adversarial perturbations
neurips,2017,0,1060,Peter,Schulam,jhu,Johns Hopkins University,pschulam@cs.jhu.edu,Reliable Decision Support using Counterfactual Models
neurips,2017,1,1060,Suchi,Saria,jhu,Johns Hopkins University,ssaria@cs.jhu.edu,Reliable Decision Support using Counterfactual Models
neurips,2017,0,2539,Chao,Pan,gmail,Purdue University,panchao25@gmail.com,Group Additive Structure Identification for Kernel Nonparametric Regression
neurips,2017,1,2539,Michael,Zhu,purdue,Purdue University,yuzhu@purdue.edu,Group Additive Structure Identification for Kernel Nonparametric Regression
neurips,2017,0,2035,Julien,Pérolat,google,CNRS,perolat@google.com,A multi-agent reinforcement learning model of common-pool resource appropriation
neurips,2017,1,2035,Joel,Leibo,google,DeepMind,jzl@google.com,A multi-agent reinforcement learning model of common-pool resource appropriation
neurips,2017,2,2035,Vinicius,Zambaldi,google,Deepmind,vzambaldi@google.com,A multi-agent reinforcement learning model of common-pool resource appropriation
neurips,2017,3,2035,Charles,Beattie,google,DeepMind,cbeattie@google.com,A multi-agent reinforcement learning model of common-pool resource appropriation
neurips,2017,4,2035,Karl,Tuyls,google,University of Liverpool,karltuyls@google.com,A multi-agent reinforcement learning model of common-pool resource appropriation
neurips,2017,5,2035,Thore,Graepel,google,DeepMind,thore@google.com,A multi-agent reinforcement learning model of common-pool resource appropriation
neurips,2017,0,145,Di,He,pku,Peking University,di_he@pku.edu.cn,Decoding with Value Networks for Neural Machine Translation
neurips,2017,1,145,Hanqing,Lu,cmu,Zhejiang University,hanqinglu@cmu.edu,Decoding with Value Networks for Neural Machine Translation
neurips,2017,2,145,Yingce,Xia,ustc,University of Science and Technology of China,xiayingc@mail.ustc.edu.cn,Decoding with Value Networks for Neural Machine Translation
neurips,2017,3,145,Tao,Qin,microsoft,Microsoft Research,taoqin@microsoft.com,Decoding with Value Networks for Neural Machine Translation
neurips,2017,4,145,Liwei,Wang,pku,Peking University,wanglw@cis.pku.edu.cn,Decoding with Value Networks for Neural Machine Translation
neurips,2017,5,145,Tie-Yan,Liu,microsoft,Microsoft Research,tie-yan.liu@microsoft.com,Decoding with Value Networks for Neural Machine Translation
neurips,2017,0,3157,Jianfei,Chen,tsinghua,Tsinghua University,chenjian14@mails.tsinghua.edu.cn,Population Matching Discrepancy and Applications in Deep Learning
neurips,2017,1,3157,Chongxuan,LI,tsinghua,Tsinghua University,licx14@mails.tsinghua.edu.cn,Population Matching Discrepancy and Applications in Deep Learning
neurips,2017,2,3157,Yizhong,Ru,tsinghua,Tsinghua University,ruyz13@mails.tsinghua.edu.cn,Population Matching Discrepancy and Applications in Deep Learning
neurips,2017,3,3157,Jun,Zhu,tsinghua,Tsinghua University,dcszj@tsinghua.edu.cn,Population Matching Discrepancy and Applications in Deep Learning
neurips,2017,0,3085,Carlton,Downey,cmu,Carnegie Mellon University,cmdowney@cs.cmu.edu,Predictive State Recurrent Neural Networks
neurips,2017,1,3085,Ahmed,Hefny,cmu,Carnegie Mellon University,ahefny@cs.cmu.edu,Predictive State Recurrent Neural Networks
neurips,2017,2,3085,Byron,Boots,cmu,Georgia Tech / Google Brain,boyue@cs.cmu.edu,Predictive State Recurrent Neural Networks
neurips,2017,3,3085,Geoffrey,Gordon,gatech,CMU,bboots@cc.gatech.edu,Predictive State Recurrent Neural Networks
neurips,2017,4,3085,Boyue,Li,cmu,Carnegie Mellon University,ggordon@cs.cmu.edu,Predictive State Recurrent Neural Networks
neurips,2017,0,523,Jeremiah,Liu,,Harvard University,zhl112@mail,Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes
neurips,2017,1,523,Brent,Coull,harvard,Harvard University,bcoull@hsph.harvard.edu,Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes
neurips,2017,0,760,Vincent,Roulet,,INRIA / ENS Ulm,,"Sharpness, Restart and Acceleration"
neurips,2017,1,760,Alexandre,d'Aspremont,,CNRS - Ecole Normale Supérieure,,"Sharpness, Restart and Acceleration"
neurips,2017,0,2100,Sara,Sabour,google,Google,sasabour@google.com,Dynamic Routing Between Capsules
neurips,2017,1,2100,Nicholas,Frosst,google,Google,frosst@google.com,Dynamic Routing Between Capsules
neurips,2017,2,2100,Geoffrey,Hinton,google,Google & University of Toronto,geoffhinton@google.com,Dynamic Routing Between Capsules
neurips,2017,0,2095,Yunzhu,Li,mit,MIT,liyunzhu@mit.edu,InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations
neurips,2017,1,2095,Jiaming,Song,stanford,Stanford University,tsong@cs.stanford.edu,InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations
neurips,2017,2,2095,Stefano,Ermon,stanford,Stanford,ermon@cs.stanford.edu,InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations
neurips,2017,0,1888,Vlad,Niculae,cornell,Cornell University,vlad@cs.cornell.edu,A Regularized Framework for Sparse and Structured Neural Attention
neurips,2017,1,1888,Mathieu,Blondel,mblondel,NTT,mathieu@mblondel.org,A Regularized Framework for Sparse and Structured Neural Attention
neurips,2017,0,3430,Tianxiao,Shen,mit,MIT,1tianxiao@csail.mit.edu,Style Transfer from Non-Parallel Text by Cross-Alignment
neurips,2017,1,3430,Tao,Lei,mit,MIT,regina@csail.mit.edu,Style Transfer from Non-Parallel Text by Cross-Alignment
neurips,2017,2,3430,Regina,Barzilay,mit,Massachusetts Institute of Technology,tommi@csail.mit.edu,Style Transfer from Non-Parallel Text by Cross-Alignment
neurips,2017,3,3430,Tommi,Jaakkola,asapp,MIT,2tao@asapp.com,Style Transfer from Non-Parallel Text by Cross-Alignment
neurips,2017,0,2305,Emily,Denton,nyu,New York University,denton@cs.nyu.edu,Unsupervised Learning of Disentangled Representations from Video
neurips,2017,1,2305,vighnesh,Birodkar,nyu,New York University,vighneshbirodkar@nyu.edu,Unsupervised Learning of Disentangled Representations from Video
neurips,2017,0,3123,Zhengyuan,Zhou,stanford,Stanford University,zyzhou@stanford.edu,Countering Feedback Delays in Multi-Agent Learning
neurips,2017,1,3123,Panayotis,Mertikopoulos,imag,CNRS (French National Center for Scientific Research),panayotis.mertikopoulos@imag.fr,Countering Feedback Delays in Multi-Agent Learning
neurips,2017,2,3123,Nicholas,Bambos,stanford,,bambos@stanford.edu,Countering Feedback Delays in Multi-Agent Learning
neurips,2017,3,3123,Peter,Glynn,stanford,Stanford University,glynn@stanford.edu,Countering Feedback Delays in Multi-Agent Learning
neurips,2017,4,3123,Claire,Tomlin,berkeley,UC Berkeley,tomlin@eecs.berkeley.edu,Countering Feedback Delays in Multi-Agent Learning
neurips,2017,0,3447,Mohammadhossein,Bateni,google,Google research,bateni@google.com,Affinity Clustering: Hierarchical Clustering at Scale
neurips,2017,1,3447,Soheil,Behnezhad,umd,University of Maryland,soheil@cs.umd.edu,Affinity Clustering: Hierarchical Clustering at Scale
neurips,2017,2,3447,Mahsa,Derakhshan,umd,University of Maryland,mahsaa@cs.umd.edu,Affinity Clustering: Hierarchical Clustering at Scale
neurips,2017,3,3447,MohammadTaghi,Hajiaghayi,umd,University of Maryland,hajiagha@cs.umd.edu,Affinity Clustering: Hierarchical Clustering at Scale
neurips,2017,4,3447,Raimondas,Kiveris,google,Google research,rkiveris@google.com,Affinity Clustering: Hierarchical Clustering at Scale
neurips,2017,5,3447,Silvio,Lattanzi,google,Google Research,silviol@google.com,Affinity Clustering: Hierarchical Clustering at Scale
neurips,2017,6,3447,Vahab,Mirrokni,google,Google Research NYC,mirrokni@google.com,Affinity Clustering: Hierarchical Clustering at Scale
neurips,2017,0,2059,Federico,Monti,usi,Università della Svizzera italiana,federico.monti@usi.ch,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks
neurips,2017,1,2059,Michael,Bronstein,usi,USI Lugano / Tel Aviv University / Intel,michael.bronstein@usi.ch,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks
neurips,2017,2,2059,Xavier,Bresson,ntu,NTU,xbresson@ntu.edu.sg,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks
neurips,2017,0,2802,Jinseok,Nam,,TU Darmstadt,,Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification
neurips,2017,1,2802,Eneldo,Loza Mencía,,Technische Universität Darmstadt,,Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification
neurips,2017,2,2802,Hyunwoo,Kim,,University of Wisconsin-Madison,,Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification
neurips,2017,3,2802,Johannes,Fürnkranz,,TU Darmstadt,,Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification
neurips,2017,0,334,Richard,Nock,,"Data61, The Australian National University & The University of Sydney",,f-GANs in an Information Geometric Nutshell
neurips,2017,1,334,Zac,Cranko,,The Australian National University & Data61,,f-GANs in an Information Geometric Nutshell
neurips,2017,2,334,Aditya,Menon,,Data61/CSIRO,,f-GANs in an Information Geometric Nutshell
neurips,2017,3,334,Lizhen,Qu,,Data61,,f-GANs in an Information Geometric Nutshell
neurips,2017,4,334,Robert,Williamson,,Australian National University & Data61,,f-GANs in an Information Geometric Nutshell
neurips,2017,0,658,Haw-Shiuan,Chang,umass,"UMass, Amherst",hschang@cs.umass.edu,Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples
neurips,2017,1,658,Erik,Learned-Miller,umass,UMass Amherst,elm@cs.umass.edu,Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples
neurips,2017,2,658,Andrew,McCallum,umass,UMass Amherst,mccallum@cs.umass.edu,Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples
neurips,2017,0,647,Kristof,Schütt,,TU Berlin,,SchNet: A continuous-filter convolutional neural network for modeling quantum interactions
neurips,2017,1,647,Pieter-Jan,Kindermans,,Google AI Resident,,SchNet: A continuous-filter convolutional neural network for modeling quantum interactions
neurips,2017,2,647,Huziel Enoc,Sauceda Felix,,Fritz-Haber-Institut der Max-Planck-Gesellschaft,,SchNet: A continuous-filter convolutional neural network for modeling quantum interactions
neurips,2017,3,647,Stefan,Chmiela,,Technische Universität Berlin,,SchNet: A continuous-filter convolutional neural network for modeling quantum interactions
neurips,2017,4,647,Alexandre,Tkatchenko,,University of Luxembourg,,SchNet: A continuous-filter convolutional neural network for modeling quantum interactions
neurips,2017,5,647,Klaus-Robert,Müller,,TU Berlin,,SchNet: A continuous-filter convolutional neural network for modeling quantum interactions
neurips,2017,0,2649,Alex,Lamb,,UMontreal (MILA),,GibbsNet: Iterative Adversarial Inference for Deep Graphical Models
neurips,2017,1,2649,Devon,Hjelm,,MILA,,GibbsNet: Iterative Adversarial Inference for Deep Graphical Models
neurips,2017,2,2649,Yaroslav,Ganin,,Université de Montréal,,GibbsNet: Iterative Adversarial Inference for Deep Graphical Models
neurips,2017,3,2649,Joseph Paul,Cohen,,Montreal Institute for Learning Algorithms,,GibbsNet: Iterative Adversarial Inference for Deep Graphical Models
neurips,2017,4,2649,Aaron,Courville,,U. Montreal,,GibbsNet: Iterative Adversarial Inference for Deep Graphical Models
neurips,2017,5,2649,Yoshua,Bengio,,U. Montreal,,GibbsNet: Iterative Adversarial Inference for Deep Graphical Models
neurips,2017,0,2025,Yunus,Saatci,,Uber AI Labs,,Bayesian GAN
neurips,2017,1,2025,Andrew,Wilson,,Cornell University,,Bayesian GAN
neurips,2017,0,1226,Niladri,Chatterji,berkeley,UC Berkeley,niladri.chatterji@berkeley.edu,Alternating minimization for dictionary learning with random initialization
neurips,2017,1,1226,Peter,Bartlett,berkeley,UC Berkeley,peter@berkeley.edu,Alternating minimization for dictionary learning with random initialization
neurips,2017,0,1880,Weiwei,Liu,gmail,UTS,liuweiwei863@gmail.com,Sparse Embedded $k$-Means Clustering
neurips,2017,1,1880,Xiaobo,Shen,gmail,NJUST,njust.shenxiaobo@gmail.com,Sparse Embedded $k$-Means Clustering
neurips,2017,2,1880,Ivor,Tsang,uts,"University of Technology, Sydney",ivor.tsang@uts.edu.au,Sparse Embedded $k$-Means Clustering
neurips,2017,0,2069,Andrew,Miller,harvard,Harvard,acm@seas.harvard.edu,Reducing Reparameterization Gradient Variance
neurips,2017,1,2069,Nick,Foti,uw,University of Washington,nfoti@uw.edu,Reducing Reparameterization Gradient Variance
neurips,2017,2,2069,Alexander,D'Amour,berkeley,UC Berkeley,alexdamour@berkeley.edu,Reducing Reparameterization Gradient Variance
neurips,2017,3,2069,Ryan,Adams,princeton,,rpa@princeton.edu,Reducing Reparameterization Gradient Variance
neurips,2017,0,2870,Christopher,Srinivasa,,University of Toronto/Borealis AI,,Min-Max Propagation
neurips,2017,1,2870,Inmar,Givoni,,University of Toronto,,Min-Max Propagation
neurips,2017,2,2870,Siamak,Ravanbakhsh,,CMU/UBC,,Min-Max Propagation
neurips,2017,3,2870,Brendan,Frey,,"Deep Genomics, Vector Institute, Univ. Toronto",,Min-Max Propagation
neurips,2017,0,3149,Eric,Balkanski,harvard,Harvard University,ericbalkanski@g.harvard.edu,Statistical Cost Sharing
neurips,2017,1,3149,Umar,Syed,google,Google Research,usyed@google.com,Statistical Cost Sharing
neurips,2017,2,3149,Sergei,Vassilvitskii,google,Google,sergeiv@google.com,Statistical Cost Sharing
neurips,2017,0,72,Shiyu,Chang,ibm,IBM T.J. Watson Research Center,yum@us.ibm.com,Dilated Recurrent Neural Networks
neurips,2017,1,72,Yang,Zhang,ibm,IBM T. J. Watson Research,wtan@us.ibm.com,Dilated Recurrent Neural Networks
neurips,2017,2,72,Wei,Han,ibm,University of Illinois at Urbana-Champaign,cuix@us.ibm.com,Dilated Recurrent Neural Networks
neurips,2017,3,72,Mo,Yu,ibm,Johns Hopkins University,witbrock@us.ibm.com,Dilated Recurrent Neural Networks
neurips,2017,4,72,Xiaoxiao,Guo,illinois,IBM Research,weihan3@illinois.edu,Dilated Recurrent Neural Networks
neurips,2017,5,72,Wei,Tan,illinois,IBM T. J. Watson Research Center,jhasegaw@illinois.edu,Dilated Recurrent Neural Networks
neurips,2017,6,72,Xiaodong,Cui,illinois,IBM T. J. Watson Research Center,t-huang1@illinois.edu,Dilated Recurrent Neural Networks
neurips,2017,7,72,Michael,Witbrock,ibm,"IBM Research, USA",shiyu.chang@ibm.com,Dilated Recurrent Neural Networks
neurips,2017,8,72,Mark,Hasegawa-Johnson,ibm,University of Illinois,yang.zhang2@ibm.com,Dilated Recurrent Neural Networks
neurips,2017,9,72,Thomas,Huang,ibm,UIUC,xiaoxiao.guo@ibm.com,Dilated Recurrent Neural Networks
neurips,2017,0,3152,Zhou,Lu,pku,Peking University,1400010739@pku.edu.cn,The Expressive Power of Neural Networks: A View from the Width
neurips,2017,1,3152,Hongming,Pu,pku,Peking university,1400010621@pku.edu.cn,The Expressive Power of Neural Networks: A View from the Width
neurips,2017,2,3152,Feicheng,Wang,pku,Peking University,1400010604@pku.edu.cn,The Expressive Power of Neural Networks: A View from the Width
neurips,2017,3,3152,Zhiqiang,Hu,pku,Peking University,huzq@pku.edu.cn,The Expressive Power of Neural Networks: A View from the Width
neurips,2017,4,3152,Liwei,Wang,pku,Peking University,wanglw@cis.pku.edu.cn,The Expressive Power of Neural Networks: A View from the Width
neurips,2017,0,3406,Dylan,Hadfield-Menell,berkeley,UC Berkeley,dhm@cs.berkeley.edu,Inverse Reward Design
neurips,2017,1,3406,Smitha,Milli,berkeley,UC Berkeley,smilli@cs.berkeley.edu,Inverse Reward Design
neurips,2017,2,3406,Pieter,Abbeel,berkeley,OpenAI / UC Berkeley / Gradescope,pabbeel@cs.berkeley.edu,Inverse Reward Design
neurips,2017,3,3406,Stuart,Russell,berkeley,UC Berkeley,russell@cs.berkeley.edu,Inverse Reward Design
neurips,2017,4,3406,Anca,Dragan,berkeley,UC Berkeley,anca@cs.berkeley.edu,Inverse Reward Design
neurips,2017,0,3334,Moein,Falahatgar,ucsd,UCSD,moein@ucsd.edu,The power of absolute discounting: all-dimensional distribution estimation
neurips,2017,1,3334,Mesrob,Ohannessian,gmail,Toyota Technological Institute at Chicago,mesrob@gmail.com,The power of absolute discounting: all-dimensional distribution estimation
neurips,2017,2,3334,Alon,Orlitsky,ucsd,"University of California, San Diego",alon@ucsd.edu,The power of absolute discounting: all-dimensional distribution estimation
neurips,2017,3,3334,Venkatadheeraj,Pichapati,ucsd,UCSD,dheerajpv7@ucsd.edu,The power of absolute discounting: all-dimensional distribution estimation
neurips,2017,0,2211,Marc,Lanctot,,DeepMind,,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
neurips,2017,1,2211,Vinicius,Zambaldi,,Deepmind,,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
neurips,2017,2,2211,Audrunas,Gruslys,,Google DeepMind,,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
neurips,2017,3,2211,Angeliki,Lazaridou,,DeepMind,,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
neurips,2017,4,2211,Karl,Tuyls,,DeepMind,,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
neurips,2017,5,2211,Julien,Perolat,,,,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
neurips,2017,6,2211,David,Silver,,DeepMind,,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
neurips,2017,7,2211,Thore,Graepel,,DeepMind,,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
neurips,2017,0,3348,Gabriel,Parra,uchile,Universidad de Chile,gparra@dim.uchile.cl,Spectral Mixture Kernels for Multi-Output Gaussian Processes
neurips,2017,1,3348,Felipe,Tobar,uchile,Universidad de Chile,ftobar@dim.uchile.cl,Spectral Mixture Kernels for Multi-Output Gaussian Processes
neurips,2017,0,2490,Tomer,Koren,google,Google,tkoren@google.com,Affine-Invariant Online Optimization and the Low-rank Experts Problem
neurips,2017,1,2490,Roi,Livni,princeton,Princeton,rlivni@cs.princeton.edu,Affine-Invariant Online Optimization and the Low-rank Experts Problem
neurips,2017,0,301,Liqian,Ma,,KU Leuven,,Pose Guided Person Image Generation
neurips,2017,1,301,Xu,Jia,,KU Leuven,,Pose Guided Person Image Generation
neurips,2017,2,301,Qianru,Sun,,MPI Informatics,,Pose Guided Person Image Generation
neurips,2017,3,301,Bernt,Schiele,,Max Planck Institute for Informatics,,Pose Guided Person Image Generation
neurips,2017,4,301,Tinne,Tuytelaars,,KU Leuven,,Pose Guided Person Image Generation
neurips,2017,5,301,Luc,Van Gool,,KU Leuven,,Pose Guided Person Image Generation
neurips,2017,0,2151,Andre,Barreto,google,DeepMind,andrebarreto@google.com,Successor Features for Transfer in Reinforcement Learning
neurips,2017,1,2151,Will,Dabney,google,DeepMind,wdabney@google.com,Successor Features for Transfer in Reinforcement Learning
neurips,2017,2,2151,Remi,Munos,google,DeepMind,munos@google.com,Successor Features for Transfer in Reinforcement Learning
neurips,2017,3,2151,Jonathan,Hunt,google,DeepMind,jjhunt@google.com,Successor Features for Transfer in Reinforcement Learning
neurips,2017,4,2151,Tom,Schaul,google,DeepMind,schaul@google.com,Successor Features for Transfer in Reinforcement Learning
neurips,2017,5,2151,Hado,van Hasselt,google,DeepMind,hado@google.com,Successor Features for Transfer in Reinforcement Learning
neurips,2017,6,2151,David,Silver,google,DeepMind,davidsilver@google.com,Successor Features for Transfer in Reinforcement Learning
neurips,2017,0,1552,Xingguo,Li,,University of Minnesota,,On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning
neurips,2017,1,1552,Lin,Yang,,Johns Hopkins University,,On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning
neurips,2017,2,1552,Jason,Ge,,Princeton University,,On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning
neurips,2017,3,1552,Jarvis,Haupt,,University of Minnesota,,On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning
neurips,2017,4,1552,Tong,Zhang,,Tencent AI Lab,,On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning
neurips,2017,5,1552,Tuo,Zhao,,Georgia Tech,,On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning
neurips,2017,0,399,Simon,Du,cmu,Carnegie Mellon University,ssdu@cs.cmu.edu,Hypothesis Transfer Learning via Transformation Functions
neurips,2017,1,399,Jayanth,Koushik,cmu,Carnegie Mellon University,jayanthkoushik@cmu.edu,Hypothesis Transfer Learning via Transformation Functions
neurips,2017,2,399,Aarti,Singh,cmu,CMU,aartisingh@cmu.edu,Hypothesis Transfer Learning via Transformation Functions
neurips,2017,3,399,Barnabas,Poczos,cmu,Carnegie Mellon University,bapoczos@cs.cmu.edu,Hypothesis Transfer Learning via Transformation Functions
neurips,2017,0,2837,Yue,Wang,bjtu,Beijing Jiaotong University,11271012@bjtu.edu.cn,Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting
neurips,2017,1,2837,Wei,Chen,microsoft,Microsoft Research,wche@microsoft.com,Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting
neurips,2017,2,2837,Yuting,Liu,bjtu,Beijing Jiaotong University,ytliu@bjtu.edu.cn,Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting
neurips,2017,3,2837,Zhi-Ming,Ma,amt,,mazm@amt.ac.cn,Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting
neurips,2017,4,2837,Tie-Yan,Liu,microsoft,Microsoft Research,Tie-Yan.Liu@microsoft.com,Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting
neurips,2017,0,1548,Adji Bousso,Dieng,,Columbia University,,Variational Inference via $\chi$ Upper Bound Minimization
neurips,2017,1,1548,Dustin,Tran,,Columbia University & OpenAI,,Variational Inference via $\chi$ Upper Bound Minimization
neurips,2017,2,1548,Rajesh,Ranganath,,Princeton University,,Variational Inference via $\chi$ Upper Bound Minimization
neurips,2017,3,1548,John,Paisley,,,,Variational Inference via $\chi$ Upper Bound Minimization
neurips,2017,4,1548,David,Blei,,Columbia University,,Variational Inference via $\chi$ Upper Bound Minimization
neurips,2017,0,2339,Qinliang,Su,duke,Duke University,qs15@duke.edu,A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks
neurips,2017,1,2339,xuejun,Liao,duke,,xjliao@duke.edu,A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks
neurips,2017,2,2339,Lawrence,Carin,duke,Duke University,lcarin@duke.edu,A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks
neurips,2017,0,2732,Yuhuai,Wu,toronto,University of Toronto,ywu@cs.toronto.edu,Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation
neurips,2017,1,2732,Elman,Mansimov,nyu,New York University,mansimov@cs.nyu.edu,Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation
neurips,2017,2,2732,Roger,Grosse,toronto,University of Toronto,sliao3@cs.toronto.edu,Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation
neurips,2017,3,2732,Shun,Liao,toronto,University of Toronto,rgrosse@cs.toronto.edu,Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation
neurips,2017,4,2732,Jimmy,Ba,utoronto,University of Toronto / Vector Institute,jimmy@psi.utoronto.ca,Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation
neurips,2017,0,787,Shipra,Agrawal,,Columbia University,,Optimistic posterior sampling for reinforcement learning: worst-case regret bounds
neurips,2017,1,787,Randy,Jia,,Columbia University,,Optimistic posterior sampling for reinforcement learning: worst-case regret bounds
neurips,2017,0,3115,Daniele,Calandriello,inria,INRIA Lille - Nord Europe,daniele.calandriello@inria.fr,Efficient Second-Order Online Kernel Learning with Adaptive Embedding
neurips,2017,1,3115,Alessandro,Lazaric,inria,INRIA Lille-Nord Europe,alessandro.lazaric@inria.fr,Efficient Second-Order Online Kernel Learning with Adaptive Embedding
neurips,2017,2,3115,Michal,Valko,inria,Inria Lille - Nord Europe,michal.valko@inria.fr,Efficient Second-Order Online Kernel Learning with Adaptive Embedding
neurips,2017,0,1172,Gang,Wang,umn,University of Minnesota,gangwang@umn.edu,Solving Most Systems of Random Quadratic Equations
neurips,2017,1,1172,Georgios,Giannakis,umn,University of Minnesota,georgios@umn.edu,Solving Most Systems of Random Quadratic Equations
neurips,2017,2,1172,Yousef,Saad,umn,University of Minnesota,saad@umn.edu,Solving Most Systems of Random Quadratic Equations
neurips,2017,3,1172,Jie,Chen,bit,Beijing Institute of Technology,chenjie@bit.edu.cn.,Solving Most Systems of Random Quadratic Equations
neurips,2017,0,2585,Chen-Yu,Wei,sinica,Academia Sinica,bahh723@iis.sinica.edu.tw,Online Reinforcement Learning in Stochastic Games
neurips,2017,1,2585,Yi-Te,Hong,sinica,National Taiwan University,ted0504@iis.sinica.edu.tw,Online Reinforcement Learning in Stochastic Games
neurips,2017,2,2585,Chi-Jen,Lu,sinica,Academia Sinica,cjlu@iis.sinica.edu.tw,Online Reinforcement Learning in Stochastic Games
neurips,2017,0,2139,Daniil,Ryabko,ryabko,INRIA,daniil@ryabko.net,Independence clustering (without a matrix)
neurips,2017,0,3243,Michael,Kamp,uni-bonn,University of Bonn / Fraunhofer IAIS,kamp@cs.uni-bonn.de,Effective Parallelisation for Machine Learning
neurips,2017,1,3243,Mario,Boley,google,Max Planck Institute for Informatics and Saarland University,olanam@google.com,Effective Parallelisation for Machine Learning
neurips,2017,2,3243,Olana,Missura,mpg,Google Inc,mboley@mpi-inf.mpg.de,Effective Parallelisation for Machine Learning
neurips,2017,3,3243,Thomas,Gärtner,nottingham,University of Nottingham,thomas.gaertner@nottingham.ac.uk,Effective Parallelisation for Machine Learning
neurips,2017,0,517,Siavash,Arjomand Bigdeli,unibe,Universität Bern,bigdeli@inf.unibe.ch,Deep Mean-Shift Priors for Image Restoration
neurips,2017,1,517,Matthias,Zwicker,unibe,"University of Maryland, College Park",jin@inf.unibe.ch,Deep Mean-Shift Priors for Image Restoration
neurips,2017,2,517,Paolo,Favaro,unibe,University of Bern,favaro@inf.unibe.ch,Deep Mean-Shift Priors for Image Restoration
neurips,2017,3,517,Meiguang,Jin,umd,University of Bern,zwicker@cs.umd.edu,Deep Mean-Shift Priors for Image Restoration
neurips,2017,0,246,Anton,Osokin,,CS HSE,,On Structured Prediction Theory with Calibrated Convex Surrogate Losses
neurips,2017,1,246,Francis,Bach,,Inria,,On Structured Prediction Theory with Calibrated Convex Surrogate Losses
neurips,2017,2,246,Simon,Lacoste-Julien,,Université de Montréal,,On Structured Prediction Theory with Calibrated Convex Surrogate Losses
neurips,2017,0,3147,Alberto,Bietti,inria,Inria,alberto.bietti@inria.fr,Invariance and Stability of Deep Convolutional Representations
neurips,2017,1,3147,Julien,Mairal,inria,Inria,julien.mairal@inria.fr,Invariance and Stability of Deep Convolutional Representations
neurips,2017,0,2117,Jörg,Bornschein,google,DeepMind,bornschein@google.com,Variational Memory Addressing in Generative Models
neurips,2017,1,2117,Andriy,Mnih,google,DeepMind,amnih@google.com,Variational Memory Addressing in Generative Models
neurips,2017,2,2117,Daniel,Zoran,google,DeepMind,danielzoran@google.com,Variational Memory Addressing in Generative Models
neurips,2017,3,2117,Danilo,Jimenez Rezende,google,Google DeepMind,danilor@google.com,Variational Memory Addressing in Generative Models
neurips,2017,0,1785,Nir,Levine,gmail,Technion - Israel Institute of Technology,levin.nir1@gmail.com,Shallow Updates for Deep Reinforcement Learning
neurips,2017,1,1785,Tom,Zahavy,technion,The Technion,tomzahavy@campus.technion.ac.il,Shallow Updates for Deep Reinforcement Learning
neurips,2017,2,1785,Daniel,Mankowitz,technion,Technion,danielm@tx.technion.ac.il,Shallow Updates for Deep Reinforcement Learning
neurips,2017,3,1785,Aviv,Tamar,berkeley,UC Berkeley,avivt@berkeley.edu,Shallow Updates for Deep Reinforcement Learning
neurips,2017,4,1785,Shie,Mannor,technion,Technion,shie@ee.technion.ac.il,Shallow Updates for Deep Reinforcement Learning
neurips,2017,0,3185,Amélie,Heliou,lri,Univ. Grenoble Alpes,johanne.cohen@lri.fr,Learning with Bandit Feedback in Potential Games
neurips,2017,1,3185,Johanne,Cohen,polytechnique,LRI-CNRS,amelie.heliou@polytechnique.edu,Learning with Bandit Feedback in Potential Games
neurips,2017,2,3185,Panayotis,Mertikopoulos,imag,CNRS (French National Center for Scientific Research),panayotis.mertikopoulos@imag.fr,Learning with Bandit Feedback in Potential Games
neurips,2017,0,2813,Hsiang-Fu,Yu,utexas,U Texas,rofuyu@cs.utexas.edu,A Greedy Approach for Budgeted Maximum Inner Product Search
neurips,2017,1,2813,Cho-Jui,Hsieh,ucdavis,UC Davis,chohsieh@ucdavis.edu,A Greedy Approach for Budgeted Maximum Inner Product Search
neurips,2017,2,2813,Qi,Lei,utexas,"Institute for Computational Engineering and Sciences, University of Texas at Austin",leiqi@ices.utexas.edu,A Greedy Approach for Budgeted Maximum Inner Product Search
neurips,2017,3,2813,Inderjit,Dhillon,utexas,University of Texas at Austin,inderjit@cs.utexas.edu,A Greedy Approach for Budgeted Maximum Inner Product Search
neurips,2017,0,2686,Minhyung,Cho,gmail,Gracenote,mhyung.cho@gmail.com,Riemannian approach to batch normalization
neurips,2017,1,2686,Jaehyung,Lee,kaist,Gracenote,jaehyung.lee@kaist.ac.kr,Riemannian approach to batch normalization
neurips,2017,0,1121,Martin,Royer,u-psud,Université Paris-Saclay,martin.royer@math.u-psud.fr,Adaptive Clustering through Semidefinite Programming
neurips,2017,0,1560,Haoran,Tang,,UC Berkeley,,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,1,1560,Rein,Houthooft,,OpenAI,,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,2,1560,Davis,Foote,,Google Brain,,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,3,1560,Adam,Stooke,,UC Berkeley,,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,4,1560,OpenAI,Xi Chen,,"OpenAI, UC Berkeley",,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,5,1560,Yan,Duan,,,,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,6,1560,John,Schulman,,OpenAI,,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,7,1560,Filip,DeTurck,,,,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,8,1560,Pieter,Abbeel,,OpenAI / UC Berkeley / Gradescope,,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning
neurips,2017,0,768,Naoya,Takeishi,u-tokyo,The University of Tokyo,takeishi@ailab.t.u-tokyo.ac.jp,Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition
neurips,2017,1,768,Yoshinobu,Kawahara,u-tokyo,Osaka University / RIKEN,yairi@ailab.t.u-tokyo.ac.jp,Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition
neurips,2017,2,768,Takehisa,Yairi,osaka-u,The University of Tokyo,ykawahara@sanken.osaka-u.ac.jp,Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition
neurips,2017,0,865,Tim,Roughgarden,stanford,Stanford University,tim@cs.stanford.edu,Online Prediction with Selfish Experts
neurips,2017,1,865,Okke,Schrijvers,stanford,Facebook Inc.,okkes@cs.stanford.edu,Online Prediction with Selfish Experts
neurips,2017,0,2383,Slobodan,Mitrovic,,EPFL,,Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach
neurips,2017,1,2383,Ilija,Bogunovic,,EPFL Lausanne,,Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach
neurips,2017,2,2383,Ashkan,Norouzi-Fard,,EPFL,,Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach
neurips,2017,3,2383,Jakub,Tarnawski,,EPFL,,Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach
neurips,2017,4,2383,Volkan,Cevher,,EPFL,,Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach
neurips,2017,0,1255,Jacob,Devlin,google,Microsoft Research,jacobdevlin@google.com,Neural Program Meta-Induction
neurips,2017,1,1255,Rudy,Bunel,ox,Oxford University,rudy@robots.ox.ac.uk,Neural Program Meta-Induction
neurips,2017,2,1255,Rishabh,Singh,microsoft,Microsoft Research,risin@microsoft.com,Neural Program Meta-Induction
neurips,2017,3,1255,Matthew,Hausknecht,microsoft,Microsoft Research,mahauskn@microsoft.com,Neural Program Meta-Induction
neurips,2017,4,1255,Pushmeet,Kohli,google,DeepMind,pushmeet@google.com,Neural Program Meta-Induction
neurips,2017,0,3321,Chuang,Wang,harvard,Harvard University,chuangwang@seas.harvard.edu,The Scaling Limit of High-Dimensional Online Independent Component Analysis
neurips,2017,1,3321,Yue,Lu,harvard,Harvard University,yuelu@seas.harvard.edu,The Scaling Limit of High-Dimensional Online Independent Component Analysis
neurips,2017,0,1336,Raef,Bassily,,The Ohio State University,,Practical Locally Private Heavy Hitters
neurips,2017,1,1336,Kobbi,Nissim,,Georgetown University,,Practical Locally Private Heavy Hitters
neurips,2017,2,1336,Uri,Stemmer,,Harvard University,,Practical Locally Private Heavy Hitters
neurips,2017,3,1336,Abhradeep,Guha Thakurta,,University of California Santa Cruz,,Practical Locally Private Heavy Hitters
neurips,2017,0,341,Dongsheng,Li,ibm,IBM Research - China,ldsli@cn.ibm.com,Mixture-Rank Matrix Approximation for Collaborative Filtering
neurips,2017,1,341,Chao,Chen,ibm,Tongji University,cshchen@cn.ibm.com,Mixture-Rank Matrix Approximation for Collaborative Filtering
neurips,2017,2,341,Wei,Liu,ibm,Tencent AI Lab,schu@cn.ibm.com,Mixture-Rank Matrix Approximation for Collaborative Filtering
neurips,2017,3,341,Tun,Lu,columbia,Fudan University,wliu@ee.columbia.edu,Mixture-Rank Matrix Approximation for Collaborative Filtering
neurips,2017,4,341,Ning,Gu,fudan,Fudan University,lutun@fudan.edu.cn,Mixture-Rank Matrix Approximation for Collaborative Filtering
neurips,2017,5,341,Stephen,Chu,fudan,IBM Research - China,ninggu@fudan.edu.cn,Mixture-Rank Matrix Approximation for Collaborative Filtering
neurips,2017,0,2961,Veeranjaneyulu,Sadhanala,cmu,CMU,vsadhana@cs.cmu.edu,Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods
neurips,2017,1,2961,Yu-Xiang,Wang,amazon,CMU / Amazon AI,yuxiangw@amazon.com,Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods
neurips,2017,2,2961,James,Sharpnack,ucdavis,UC Davis,jsharpna@ucdavis.edu,Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods
neurips,2017,3,2961,Ryan,Tibshirani,cmu,Carnegie Mellon University,ryantibs@stat.cmu.edu,Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods
neurips,2017,0,3184,Yoav,Wald,tau,Hebrew University,gamir@mail.tau.ac.il,Robust Conditional Probabilities
neurips,2017,1,3184,Amir,Globerson,huji,HUJI,yoav.wald@mail.huji.ac.il,Robust Conditional Probabilities
neurips,2017,0,3058,Ashish,Vaswani,google,Google Brain,avaswani@google.com,Attention is All you Need
neurips,2017,1,3058,Noam,Shazeer,google,Google,noam@google.com,Attention is All you Need
neurips,2017,2,3058,Niki,Parmar,google,Google,nikip@google.com,Attention is All you Need
neurips,2017,3,3058,Jakob,Uszkoreit,google,"Google, Inc.",usz@google.com,Attention is All you Need
neurips,2017,4,3058,Llion,Jones,google,Google,llion@google.com,Attention is All you Need
neurips,2017,5,3058,Aidan,Gomez,toronto,University of Toronto,aidan@cs.toronto.edu,Attention is All you Need
neurips,2017,6,3058,ukasz,Kaiser,google,Google Brain,lukaszkaiser@google.com,Attention is All you Need
neurips,2017,7,3058,Illia,Polosukhin,gmail,,illia.polosukhin@gmail.com,Attention is All you Need
neurips,2017,0,3576,Ehsan,Emamjomeh-Zadeh,,U. of Southern California,,A General Framework for Robust Interactive Learning
neurips,2017,1,3576,David,Kempe,,U. of Southern California,,A General Framework for Robust Interactive Learning
neurips,2017,0,2501,Maria-Florina,Balcan,cmu,Carnegie Mellon University,ninamf@cs.cmu.edu,Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions
neurips,2017,1,2501,Hongyang,Zhang,cmu,Carnegie Mellon University,hongyanz@cs.cmu.edu,Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions
neurips,2017,0,1803,Alireza,Aghasi,gsu,Institute for Insight,aaghasi@gsu.edu,Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee
neurips,2017,1,1803,Afshin,Abdi,gatech,Georgia Institute of Technology,abdi@gatech.edu,Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee
neurips,2017,2,1803,Nam,Nguyen,ibm,IBM Thomas J. Watson Research Center,nnguyen@us.ibm.com,Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee
neurips,2017,3,1803,Justin,Romberg,gatech,Georgia Institute of Technology,jrom@ece.gatech.edu,Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee
neurips,2017,0,1522,Yuandong,Tian,fb,Facebook AI Research,1yuandong@fb.com,"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games"
neurips,2017,1,1522,Qucheng,Gong,fb,Facebook AI Research,qucheng@fb.com,"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games"
neurips,2017,2,1522,Wenling,Shang,fb,University of Amsterdam,yuxinwu@fb.com,"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games"
neurips,2017,3,1522,Yuxin,Wu,fb,Facebook AI Research,zitnick@fb.com,"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games"
neurips,2017,4,1522,C. Lawrence,Zitnick,oculus,Facebook AI Research,2wendy.shang@oculus.com,"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games"
neurips,2017,0,2825,Priya,Donti,cmu,Carnegie Mellon University,pdonti@cs.cmu.edu,Task-based End-to-end Model Learning in Stochastic Optimization
neurips,2017,1,2825,Brandon,Amos,cmu,Carnegie Mellon University,bamos@cs.cmu.edu,Task-based End-to-end Model Learning in Stochastic Optimization
neurips,2017,2,2825,J. Zico,Kolter,cmu,Carnegie Mellon University,zkolter@cs.cmu.edu,Task-based End-to-end Model Learning in Stochastic Optimization
neurips,2017,0,3042,Guillaume,Lample,fb,Facebook AI Research,gl@fb.com,Fader Networks:Manipulating Images by Sliding Attributes
neurips,2017,1,3042,Neil,Zeghidour,fb,Facebook A.I. Research / Ecole Normale Supérieure,neilz@fb.com,Fader Networks:Manipulating Images by Sliding Attributes
neurips,2017,2,3042,Nicolas,Usunier,fb,Facebook AI Research,usunier@fb.com,Fader Networks:Manipulating Images by Sliding Attributes
neurips,2017,3,3042,Antoine,Bordes,fb,Facebook AI Research,abordes@fb.com,Fader Networks:Manipulating Images by Sliding Attributes
neurips,2017,4,3042,Ludovic,DENOYER,fb,Universite Pierre et Marie Curie - Paris,ranzato@fb.com,Fader Networks:Manipulating Images by Sliding Attributes
neurips,2017,5,3042,Marc'Aurelio,Ranzato,lip6,Facebook,ludovic.denoyer@lip6.fr,Fader Networks:Manipulating Images by Sliding Attributes
neurips,2017,0,2225,Yuchen,Pu,duke,Duke University,yp42@duke.edu,VAE Learning via Stein Variational Gradient Descent
neurips,2017,1,2225,Zhe,Gan,duke,Duke University,zg27@duke.edu,VAE Learning via Stein Variational Gradient Descent
neurips,2017,2,2225,Ricardo,Henao,duke,Duke University,r.henao@duke.edu,VAE Learning via Stein Variational Gradient Descent
neurips,2017,3,2225,Chunyuan,Li,duke,Duke University,cl319@duke.edu,VAE Learning via Stein Variational Gradient Descent
neurips,2017,4,2225,Shaobo,Han,duke,Duke University,shaobo.han@duke.edu,VAE Learning via Stein Variational Gradient Descent
neurips,2017,5,2225,Lawrence,Carin,duke,Duke University,lcarin@duke.edu,VAE Learning via Stein Variational Gradient Descent
neurips,2017,0,2857,Shuang,Liu,ucsd,"University of California, San Diego",shuangliu@ucsd.edu,Approximation and Convergence Properties of Generative Adversarial Learning
neurips,2017,1,2857,Olivier,Bousquet,google,Google,obousquet@google.com,Approximation and Convergence Properties of Generative Adversarial Learning
neurips,2017,2,2857,Kamalika,Chaudhuri,ucsd,UCSD,kamalika@cs.ucsd.edu,Approximation and Convergence Properties of Generative Adversarial Learning
neurips,2017,0,1879,Akash,Srivastava,ed,University of Edinburgh,akash.srivastava@ed.ac.uk,VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning
neurips,2017,1,1879,Lazar,Valkov,ed,University of Edinburgh,L.Valkov@sms.ed.ac.uk,VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning
neurips,2017,2,1879,Chris,Russell,turing,The Alan Turing Institute/ The University of Surrey,crussell@turing.ac.uk,VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning
neurips,2017,3,1879,Michael,Gutmann,ed,University of Edinburgh,Michael.Gutmann@ed.ac.uk,VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning
neurips,2017,4,1879,Charles,Sutton,ed,University of Edinburgh,csutton@inf.ed.ac.uk,VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning
neurips,2017,0,2769,Vikas,Garg,,MIT,,Local Aggregative Games
neurips,2017,1,2769,Tommi,Jaakkola,,MIT,,Local Aggregative Games
neurips,2017,0,3428,Jonathan,Zung,princeton,Princeton University,jzung@princeton.edu,An Error Detection and Correction Framework for Connectomics
neurips,2017,1,3428,Ignacio,Tartavull,princeton,Princeton Universitiy,tartavull@princeton.edu,An Error Detection and Correction Framework for Connectomics
neurips,2017,2,3428,Kisuk,Lee,mit,MIT,kisuklee@mit.edu,An Error Detection and Correction Framework for Connectomics
neurips,2017,3,3428,H. Sebastian,Seung,princeton,Princeton University,sseung@princeton.edu,An Error Detection and Correction Framework for Connectomics
neurips,2017,0,2595,Marcin,Andrychowicz,,OpenAI,,Hindsight Experience Replay
neurips,2017,1,2595,Filip,Wolski,,Whisper.ai,,Hindsight Experience Replay
neurips,2017,2,2595,Alex,Ray,,OpenAI,,Hindsight Experience Replay
neurips,2017,3,2595,Jonas,Schneider,,OpenAI,,Hindsight Experience Replay
neurips,2017,4,2595,Rachel,Fong,,OpenAI,,Hindsight Experience Replay
neurips,2017,5,2595,Peter,Welinder,,OpenAI,,Hindsight Experience Replay
neurips,2017,6,2595,Bob,McGrew,,OpenAI,,Hindsight Experience Replay
neurips,2017,7,2595,Josh,Tobin,,OpenAI,,Hindsight Experience Replay
neurips,2017,8,2595,OpenAI,Pieter Abbeel,,"OpenAI, UC Berkeley",,Hindsight Experience Replay
neurips,2017,9,2595,Wojciech,Zaremba,,OpenAI,,Hindsight Experience Replay
neurips,2017,0,821,Joel,Tropp,caltech,Caltech,jtropp@caltech.edu,Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data
neurips,2017,1,821,Alp,Yurtsever,epfl,"École Polytechnique Fédérale de Lausanne, Switzerland",alp.yurtsever@epfl.ch,Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data
neurips,2017,2,821,Madeleine,Udell,cornell,Cornell,mru8@cornell.edu,Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data
neurips,2017,3,821,Volkan,Cevher,epfl,EPFL,volkan.cevher@epfl.ch,Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data
neurips,2017,0,1144,Lars,Mescheder,mpg,Max-Planck Institute Tuebingen,lars.mescheder@tuebingen.mpg.de,The Numerics of GANs
neurips,2017,1,1144,Sebastian,Nowozin,microsoft,Microsoft Research Cambridge,sebastian.nowozin@microsoft.com,The Numerics of GANs
neurips,2017,2,1144,Andreas,Geiger,mpg,MPI Tübingen,andreas.geiger@tuebingen.mpg.de,The Numerics of GANs
neurips,2017,0,216,Rui,Costa,ox,University of Oxford,rui.costa@cncb.ox.ac.uk,Cortical microcircuits as gated-recurrent neural networks
neurips,2017,1,216,Ioannis Alexandros,Assael,ox,DeepMind,yannis.assael@cs.ox.ac.uk,Cortical microcircuits as gated-recurrent neural networks
neurips,2017,2,216,Brendan,Shillingford,ox,University of Oxford,brendan.shillingford@cs.ox.ac.uk,Cortical microcircuits as gated-recurrent neural networks
neurips,2017,3,216,Nando,de Freitas,google,University of Oxford,nandodefreitas@google.com,Cortical microcircuits as gated-recurrent neural networks
neurips,2017,4,216,TIm,Vogels,ox,University of Oxford,tim.vogels@cncb.ox.ac.uk,Cortical microcircuits as gated-recurrent neural networks
neurips,2017,0,1713,Seungil,You,google,Google,siyou@google.com,Deep Lattice Networks and Partial Monotonic Functions
neurips,2017,1,1713,David,Ding,google,Google,dwding@google.com,Deep Lattice Networks and Partial Monotonic Functions
neurips,2017,2,1713,Kevin,Canini,google,Google,canini@google.com,Deep Lattice Networks and Partial Monotonic Functions
neurips,2017,3,1713,Jan,Pfeifer,google,Google,janpf@google.com,Deep Lattice Networks and Partial Monotonic Functions
neurips,2017,4,1713,Maya,Gupta,google,Google,mayagupta@google.com,Deep Lattice Networks and Partial Monotonic Functions
neurips,2017,0,1323,Adithya M,Devraj,ufl,University of Florida,adithyamdevraj@ufl.edu,Zap Q-Learning
neurips,2017,1,1323,Sean,Meyn,ufl,University of Florida,meyn@ece.ufl.edu,Zap Q-Learning
neurips,2017,0,582,Bo,Dai,cuhk,The Chinese University of Hong Kong,db014@ie.cuhk.edu.hk,Contrastive Learning for Image Captioning
neurips,2017,1,582,Dahua,Lin,cuhk,The Chinese University of Hong Kong,dhlin@ie.cuhk.edu.hk,Contrastive Learning for Image Captioning
neurips,2017,0,2300,Anirudh Goyal,ALIAS PARTH GOYAL,gmail,Université de Montréal,anirudhgoyal9119@gmail.com,Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net
neurips,2017,1,2300,Nan Rosemary,Ke,gmail,"MILA, École Polytechnique de Montréal",rosemary.nan.ke@gmail.com,Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net
neurips,2017,2,2300,Surya,Ganguli,stanford,Stanford,sganguli@stanford.edu,Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net
neurips,2017,3,2300,Yoshua,Bengio,gmail,U. Montreal,yoshua.umontreal@gmail.com,Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net
neurips,2017,0,3455,Han,Zhao,cmu,Carnegie Mellon University,han.zhao@cs.cmu.edu,Linear Time Computation of Moments in Sum-Product Networks
neurips,2017,1,3455,Geoffrey,Gordon,cmu,CMU,ggordon@cs.cmu.edu,Linear Time Computation of Moments in Sum-Product Networks
neurips,2017,0,1424,Amit,Daniely,huji,Google Research,amit.daniely@mail.huji.ac.il,SGD Learns the Conjugate Kernel Class of the Network
neurips,2017,0,624,Gilles,Louppe,nyu,New York University,g.louppe@nyu.edu,Learning to Pivot with Adversarial Networks
neurips,2017,1,624,Michael,Kagan,stanford,SLAC / Stanford,makagan@slac.stanford.edu,Learning to Pivot with Adversarial Networks
neurips,2017,2,624,Kyle,Cranmer,nyu,New York University,kyle.cranmer@nyu.edu,Learning to Pivot with Adversarial Networks
neurips,2017,0,1203,Jason,Altschuler,mit,MIT,jasonalt@mit.edu,Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration
neurips,2017,1,1203,Jonathan,Niles-Weed,mit,MIT,jweed@mit.edu,Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration
neurips,2017,2,1203,Philippe,Rigollet,mit,MIT,rigollet@mit.edu,Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration
neurips,2017,0,295,Yijun,Li,,"University of California, Merced",,Universal Style Transfer via Feature Transforms
neurips,2017,1,295,Chen,Fang,,Adobe Research,,Universal Style Transfer via Feature Transforms
neurips,2017,2,295,Jimei,Yang,,Adobe Research,,Universal Style Transfer via Feature Transforms
neurips,2017,3,295,Zhaowen,Wang,,Adobe Research,,Universal Style Transfer via Feature Transforms
neurips,2017,4,295,Xin,Lu,,Adobe,,Universal Style Transfer via Feature Transforms
neurips,2017,5,295,Ming-Hsuan,Yang,,UC Merced,,Universal Style Transfer via Feature Transforms
neurips,2017,0,1854,Xiuyuan,Lu,stanford,Stanford University,lxy@stanford.edu,Ensemble Sampling
neurips,2017,1,1854,Benjamin,Van Roy,stanford,Stanford University,bvr@stanford.edu,Ensemble Sampling
neurips,2017,0,1504,Piotr,Indyk,,MIT,,Practical Data-Dependent Metric Compression with Provable Guarantees
neurips,2017,1,1504,Ilya,Razenshteyn,,Columbia University,,Practical Data-Dependent Metric Compression with Provable Guarantees
neurips,2017,2,1504,Tal,Wagner,,MIT,,Practical Data-Dependent Metric Compression with Provable Guarantees
neurips,2017,0,1782,Jie,Shen,rutgers,Rutgers University,js2007@rutgers.edu,Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery
neurips,2017,1,1782,Ping,Li,rutgers,Rugters University,pingli@stat.rutgers.edu,Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery
neurips,2017,0,2519,Yonatan,Geifman,technion,Technion,yonatan.g@cs.technion.ac.il,Selective Classification for Deep Neural Networks
neurips,2017,1,2519,Ran,El-Yaniv,technion,Technion,rani@cs.technion.ac.il,Selective Classification for Deep Neural Networks
neurips,2017,0,2941,Liwei,Wang,illinois,University of Illinois at Urbana–Champaign,lwang97@illinois.edu,Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space
neurips,2017,1,2941,Alexander,Schwing,illinois,University of Illinois at Urbana-Champaign,aschwing@illinois.edu,Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space
neurips,2017,2,2941,Svetlana,Lazebnik,illinois,,slazebni@illinois.edu,Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space
neurips,2017,0,2204,Yizhe,Zhang,,Duke University,,Deconvolutional Paragraph Representation Learning
neurips,2017,1,2204,Dinghan,Shen,,Duke University,,Deconvolutional Paragraph Representation Learning
neurips,2017,2,2204,Guoyin,Wang,,Duke University,,Deconvolutional Paragraph Representation Learning
neurips,2017,3,2204,Zhe,Gan,,Duke University,,Deconvolutional Paragraph Representation Learning
neurips,2017,4,2204,Ricardo,Henao,,Duke University,,Deconvolutional Paragraph Representation Learning
neurips,2017,5,2204,Lawrence,Carin,,Duke University,,Deconvolutional Paragraph Representation Learning
neurips,2017,0,121,Jiajun,Wu,,MIT,,Learning to See Physics via Visual De-animation
neurips,2017,1,121,Erika,Lu,,University of Oxford,,Learning to See Physics via Visual De-animation
neurips,2017,2,121,Pushmeet,Kohli,,DeepMind,,Learning to See Physics via Visual De-animation
neurips,2017,3,121,Bill,Freeman,,MIT/Google,,Learning to See Physics via Visual De-animation
neurips,2017,4,121,Josh,Tenenbaum,,MIT,,Learning to See Physics via Visual De-animation
neurips,2017,0,2260,Yuchen,Pu,duke,Duke University,yp42@duke.edu,Adversarial Symmetric Variational Autoencoder
neurips,2017,1,2260,Weiyao,Wang,duke,Duke University,ww109@duke.edu,Adversarial Symmetric Variational Autoencoder
neurips,2017,2,2260,Ricardo,Henao,duke,Duke University,r.henao@duke.edu,Adversarial Symmetric Variational Autoencoder
neurips,2017,3,2260,Liqun,Chen,duke,Duke University,lc267@duke.edu,Adversarial Symmetric Variational Autoencoder
neurips,2017,4,2260,Zhe,Gan,duke,Duke University,zg27@duke.edu,Adversarial Symmetric Variational Autoencoder
neurips,2017,5,2260,Chunyuan,Li,duke,Duke University,cl319@duke.edu,Adversarial Symmetric Variational Autoencoder
neurips,2017,6,2260,Lawrence,Carin,duke,Duke University,lcarin@duke.edu,Adversarial Symmetric Variational Autoencoder
neurips,2017,0,1104,Michael,Habeck,gwdg,Max Planck Institute Goettingen,mhabeck@gwdg.de,Model evidence from nonequilibrium simulations
neurips,2017,0,3095,Zhuoran,Yang,,Princeton University,,Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein’s Lemma
neurips,2017,1,3095,Krishnakumar,Balasubramanian,,georgia tech,,Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein’s Lemma
neurips,2017,2,3095,Zhaoran,Wang,,"Princeton, Phd student",,Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein’s Lemma
neurips,2017,3,3095,Han,Liu,,Tencent AI Lab,,Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein’s Lemma
neurips,2017,0,774,Stéphanie,ALLASSONNIERE,,Ecole Polytechnique,,Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data
neurips,2017,1,774,Juliette,Chevallier,,"CMAP, École polytechnique",,Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data
neurips,2017,2,774,Stephane,Oudard,,HEGP,,Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data
neurips,2017,0,2816,Kyuhong,Shim,snu,Seoul National University,skhu20@snu.ac.kr,SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks
neurips,2017,1,2816,Minjae,Lee,snu,Seoul National University,mjlee@dsp.snu.ac.kr,SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks
neurips,2017,2,2816,Iksoo,Choi,snu,Seoul National University,ischoi@dsp.snu.ac.kr,SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks
neurips,2017,3,2816,Yoonho,Boo,snu,Seoul National University,yhboo@dsp.snu.ac.kr,SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks
neurips,2017,4,2816,Wonyong,Sung,snu,Seoul National University,wysung@snu.ac.kr,SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks
neurips,2017,0,26,Constantinos,Daskalakis,,MIT,,Concentration of Multilinear Functions of the Ising Model with Applications to Network Data
neurips,2017,1,26,Nishanth,Dikkala,,MIT,,Concentration of Multilinear Functions of the Ising Model with Applications to Network Data
neurips,2017,2,26,Gautam,Kamath,,MIT,,Concentration of Multilinear Functions of the Ising Model with Applications to Network Data
neurips,2017,0,1481,Alyson,Fletcher,ucla,UCLA,akfletcher@ucla.edu,Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems
neurips,2017,1,1481,Mojtaba,Sahraee-Ardakan,ucla,UCLA,msahraee@ucla.edu,Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems
neurips,2017,2,1481,Sundeep,Rangan,nyu,NYU-Poly,srangan@nyu.edu,Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems
neurips,2017,3,1481,Philip,Schniter,osu,Ohio State University,schniter@ece.osu.edu,Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems
neurips,2017,0,1403,Andrea,Giovannucci,flatironinstitute,"Flatiron Institute, Simons Foundation",agiovannucci@flatironinstitute.org,OnACID: Online Analysis of Calcium Imaging Data in Real Time
neurips,2017,1,1403,Johannes,Friedrich,flatironinstitute,Columbia University,jfriedrich@flatironinstitute.org,OnACID: Online Analysis of Calcium Imaging Data in Real Time
neurips,2017,2,1403,Matt,Kaufman,flatironinstitute,Cold Spring Harbor Laboratory,dchklovskii@flatironinstitute.org,OnACID: Online Analysis of Calcium Imaging Data in Real Time
neurips,2017,3,1403,Anne,Churchland,flatironinstitute,Cold Spring Harbor Laboratory,epnevmatikakis@flatironinstitute.org,OnACID: Online Analysis of Calcium Imaging Data in Real Time
neurips,2017,4,1403,Dmitri,Chklovskii,cshl,Simons Foundation,mkaufman@cshl.edu,OnACID: Online Analysis of Calcium Imaging Data in Real Time
neurips,2017,5,1403,Liam,Paninski,cshl,Columbia University,churchland@cshl.edu,OnACID: Online Analysis of Calcium Imaging Data in Real Time
neurips,2017,6,1403,Eftychios,Pnevmatikakis,columbia,Flatiron Institute,liam@stat.columbia.edu,OnACID: Online Analysis of Calcium Imaging Data in Real Time
neurips,2017,0,3049,Kristjan,Greenewald,harvard,University of Michigan,kgreenewald@fas.harvard.edu,Action Centered Contextual Bandits
neurips,2017,1,3049,Ambuj,Tewari,umich,University of Michigan,tewaria@umich.edu,Action Centered Contextual Bandits
neurips,2017,2,3049,Susan,Murphy,umich,University of Michigan,klasnja@umich.edu,Action Centered Contextual Bandits
neurips,2017,3,3049,Predag,Klasnja,harvard,,samurphy@fas.harvard.edu,Action Centered Contextual Bandits
neurips,2017,0,990,Sven,Peter,uni-heidelberg,University Heidelberg,sven.peter@iwr.uni-heidelberg.de,Cost efficient gradient boosting
neurips,2017,1,990,Ferran,Diego,bosch,Bosch,ferran.diegoandilla@de.bosch.com,Cost efficient gradient boosting
neurips,2017,2,990,Fred,Hamprecht,uni-heidelberg,Heidelberg University,fred.hamprecht@iwr.uni-heidelberg.de,Cost efficient gradient boosting
neurips,2017,3,990,Boaz,Nadler,weizmann,Weizmann Institute of Science,boaz.nadler@weizmann.ac.il,Cost efficient gradient boosting
neurips,2017,0,1314,Surbhi,Goel,utexas,University of Texas at Austin,surbhi@cs.utexas.edu,Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks
neurips,2017,1,1314,Adam,Klivans,utexas,UT Austin,klivans@cs.utexas.edu,Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks
neurips,2017,0,3548,Adarsh,Prasad,cmu,Carnegie Mellon University,adarshp@andrew.cmu.edu,"On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models"
neurips,2017,1,3548,Alexandru,Niculescu-Mizil,nec-labs,NEC Laboratories America,alex@nec-labs.com,"On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models"
neurips,2017,2,3548,Pradeep,Ravikumar,cmu,Carnegie Mellon University,pradeepr@cs.cmu.edu,"On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models"
neurips,2017,0,1942,Evan,Racah,,University of Montreal,,"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events"
neurips,2017,1,1942,Christopher,Beckham,,MILA,,"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events"
neurips,2017,2,1942,Tegan,Maharaj,,"MILA, Polytechnic Montreal",,"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events"
neurips,2017,3,1942,Samira,Ebrahimi Kahou,,Microsoft Research – Maluuba,,"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events"
neurips,2017,4,1942,Mr.,Prabhat,,LBL/NERSC,,"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events"
neurips,2017,5,1942,Chris,Pal,,Montréal Institute for Learning Algorithms,,"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events"
neurips,2017,0,3464,Manasi,Vartak,mit,Massachusetts Institute of Technology,mvartak@csail.mit.edu,A Meta-Learning Perspective on Cold-Start Recommendations for Items
neurips,2017,1,3464,Arvind,Thiagarajan,twitter,Twitter,arvindt@twitter.com,A Meta-Learning Perspective on Cold-Start Recommendations for Items
neurips,2017,2,3464,Conrado,Miranda,twitter,,cmiranda@twitter.com,A Meta-Learning Perspective on Cold-Start Recommendations for Items
neurips,2017,3,3464,Jeshua,Bratman,twitter,Twitter,jbratman@twitter.com,A Meta-Learning Perspective on Cold-Start Recommendations for Items
neurips,2017,4,3464,Hugo,Larochelle,google,Google Brain,hugolarochelle@google.com,A Meta-Learning Perspective on Cold-Start Recommendations for Items
neurips,2017,0,874,Yi,Ouyang,berkeley,"University of California, Berkeley",ouyangyi@berkeley.edu,Learning Unknown Markov Decision Processes: A Thompson Sampling Approach
neurips,2017,1,874,Mukul,Gagrani,usc,University of Southern California,mgagrani@usc.edu,Learning Unknown Markov Decision Processes: A Thompson Sampling Approach
neurips,2017,2,874,Ashutosh,Nayyar,usc,University of Southern California,ashutosn@usc.edu,Learning Unknown Markov Decision Processes: A Thompson Sampling Approach
neurips,2017,3,874,Rahul,Jain,usc,University of Southern California,rahul.jain@usc.edu,Learning Unknown Markov Decision Processes: A Thompson Sampling Approach
neurips,2017,0,2127,Weiyang,Liu,,Georgia Tech,,Deep Hyperspherical Learning
neurips,2017,1,2127,Yan-Ming,Zhang,,"Institute of Automation, Chinese Academy of Sciences",,Deep Hyperspherical Learning
neurips,2017,2,2127,Xingguo,Li,,University of Minnesota,,Deep Hyperspherical Learning
neurips,2017,3,2127,Zhiding,Yu,,Carnegie Mellon University,,Deep Hyperspherical Learning
neurips,2017,4,2127,Bo,Dai,,Georgia Tech,,Deep Hyperspherical Learning
neurips,2017,5,2127,Tuo,Zhao,,Georgia Tech,,Deep Hyperspherical Learning
neurips,2017,6,2127,Le,Song,,Georgia Institute of Technology,,Deep Hyperspherical Learning
neurips,2017,0,1192,Raymond,Yeh,illinois,University of Illinois at Urbana–Champaign,yeh17@illinois.edu,Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts
neurips,2017,1,1192,Jinjun,Xiong,ibm,IBM Research,jinjun@us.ibm.com,Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts
neurips,2017,2,1192,Wen-Mei,Hwu,illinois,,w-hwu@illinois.edu,Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts
neurips,2017,3,1192,Minh,Do,illinois,University of Illinois,minhdo@illinois.edu,Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts
neurips,2017,4,1192,Alexander,Schwing,illinois,University of Illinois at Urbana-Champaign,aschwing@illinois.edu,Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts
neurips,2017,0,2032,Adith,Swaminathan,microsoft,Microsoft Research,adswamin@microsoft.com,Off-policy evaluation for slate recommendation
neurips,2017,1,2032,Akshay,Krishnamurthy,umass,,akshay@cs.umass.edu,Off-policy evaluation for slate recommendation
neurips,2017,2,2032,Alekh,Agarwal,microsoft,Microsoft Research,alekha@microsoft.com,Off-policy evaluation for slate recommendation
neurips,2017,3,2032,Miro,Dudik,microsoft,Microsoft Research,mdudik@microsoft.com,Off-policy evaluation for slate recommendation
neurips,2017,4,2032,John,Langford,microsoft,Microsoft Research New York,jcl@microsoft.com,Off-policy evaluation for slate recommendation
neurips,2017,5,2032,Damien,Jose,microsoft,Microsoft,dajose@microsoft.com,Off-policy evaluation for slate recommendation
neurips,2017,6,2032,Imed,Zitouni,microsoft,Microsoft,izitouni@microsoft.com,Off-policy evaluation for slate recommendation
neurips,2017,0,1748,Michal,Derezinski,ucsc,UC Santa Cruz,mderezin@ucsc.edu,Unbiased estimates for linear regression via volume sampling
neurips,2017,1,1748,Manfred K.,Warmuth,ucsc,Univ. of Calif. at Santa Cruz,manfred@ucsc.edu,Unbiased estimates for linear regression via volume sampling
neurips,2017,0,684,Songbai,Yan,ucsd,"University of California, San Diego",yansongbai@ucsd.edu,Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces
neurips,2017,1,684,Chicheng,Zhang,microsoft,University of California San Diego,chicheng.zhang@microsoft.com,Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces
neurips,2017,0,2739,Joseph,Geumlek,ucsd,UCSD,jgeumlek@cs.ucsd.edu,Renyi Differential Privacy Mechanisms for Posterior Sampling
neurips,2017,1,2739,Shuang,Song,ucsd,UC San Diego,shs037@eng.ucsd.edu,Renyi Differential Privacy Mechanisms for Posterior Sampling
neurips,2017,2,2739,Kamalika,Chaudhuri,ucsd,UCSD,kamalika@cs.ucsd.edu,Renyi Differential Privacy Mechanisms for Posterior Sampling
neurips,2017,0,322,Jalil,Kazemitabar,,"University of California, Los Angeles",,Variable Importance Using Decision Trees
neurips,2017,1,322,Arash,Amini,,UCLA,,Variable Importance Using Decision Trees
neurips,2017,2,322,Adam,Bloniarz,,Google,,Variable Importance Using Decision Trees
neurips,2017,3,322,Ameet,Talwalkar,,CMU,,Variable Importance Using Decision Trees
neurips,2017,0,229,Nisheeth,Srivastava,iitk,IIT Kanpur,nsrivast@cse.iitk.ac.in,A simple model of recognition and recall memory
neurips,2017,1,229,Edward,Vul,ucsd,UCSD,evul@ucsd.edu,A simple model of recognition and recall memory
neurips,2017,0,3116,Suriya,Gunasekar,ttic,TTI Chicago,suriya@ttic.edu,Implicit Regularization in Matrix Factorization
neurips,2017,1,3116,Blake,Woodworth,ttic,Toyota Technological Institute at Chicago,blake@ttic.edu,Implicit Regularization in Matrix Factorization
neurips,2017,2,3116,Srinadh,Bhojanapalli,ttic,Toyota Technological Institute at Chicago,srinadh@ttic.edu,Implicit Regularization in Matrix Factorization
neurips,2017,3,3116,Behnam,Neyshabur,ttic,Institute for Advanced Study,behnam@ttic.edu,Implicit Regularization in Matrix Factorization
neurips,2017,4,3116,Nati,Srebro,ttic,TTI-Chicago,nati@ttic.edu,Implicit Regularization in Matrix Factorization
neurips,2017,0,342,An,Bian,ethz,ETH Zurich,ybian@inf.ethz.ch,Continuous DR-submodular  Maximization: Structure and Algorithms
neurips,2017,1,342,Kfir,Levy,ethz,ETH,yehuda.levy@inf.ethz.ch,Continuous DR-submodular  Maximization: Structure and Algorithms
neurips,2017,2,342,Andreas,Krause,ethz,ETHZ,krausea@ethz.ch,Continuous DR-submodular  Maximization: Structure and Algorithms
neurips,2017,3,342,Joachim,Buhmann,ethz,ETH Zurich,jbuhmann@inf.ethz.ch,Continuous DR-submodular  Maximization: Structure and Algorithms
neurips,2017,0,615,Eran,Malach,huji,Hebrew University Jerusalem Israel,eran.malach@mail.huji.ac.il,"Decoupling ""when to update"" from ""how to update"""
neurips,2017,1,615,Shai,Shalev-Shwartz,huji,Mobileye & HUJI,shais@cs.huji.ac.il,"Decoupling ""when to update"" from ""how to update"""
neurips,2017,0,948,Wataru,Kumagai,riken,RIKEN,wataru.kumagai@riken.jp,Regret Analysis for Continuous Dueling Bandit
neurips,2017,0,514,Sagie,Benaim,,Tel Aviv University,,One-Sided Unsupervised Domain Mapping
neurips,2017,1,514,Lior,Wolf,,Facebook AI Research,,One-Sided Unsupervised Domain Mapping
neurips,2017,0,3176,Maximillian,Nickel,,Facebook,,Poincaré Embeddings for Learning Hierarchical Representations
neurips,2017,1,3176,Douwe,Kiela,,Facebook AI Research,,Poincaré Embeddings for Learning Hierarchical Representations
neurips,2017,0,1710,Hongseok,Namkoong,stanford,Stanford University,hnamk@stanford.edu,Variance-based Regularization with Convex Objectives
neurips,2017,1,1710,John,Duchi,stanford,Stanford,jduchi@stanford.edu,Variance-based Regularization with Convex Objectives
neurips,2017,0,3454,Kevin,Lin,cmu,Carnegie Mellon University,kevinl1@andrew.cmu.edu,"A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening"
neurips,2017,1,3454,James,Sharpnack,ucdavis,UC Davis,jsharpna@ucdavis.edu,"A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening"
neurips,2017,2,3454,Alessandro,Rinaldo,cmu,CMU,arinaldo@stat.cmu.edu,"A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening"
neurips,2017,3,3454,Ryan,Tibshirani,cmu,Carnegie Mellon University,ryantibs@stat.cmu.edu,"A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening"
neurips,2017,0,3435,Neil,Gallagher,,Duke University,,Cross-Spectral Factor Analysis
neurips,2017,1,3435,Kyle,Ulrich,,,,Cross-Spectral Factor Analysis
neurips,2017,2,3435,Austin,Talbot,,Duke University,,Cross-Spectral Factor Analysis
neurips,2017,3,3435,Kafui,Dzirasa,,Duke University,,Cross-Spectral Factor Analysis
neurips,2017,4,3435,Lawrence,Carin,,Duke University,,Cross-Spectral Factor Analysis
neurips,2017,5,3435,David,Carlson,,Duke University,,Cross-Spectral Factor Analysis
neurips,2017,0,617,Günter,Klambauer,jku,LIT AI Lab / University Linz,klambauer@bioinf.jku.at,Self-Normalizing Neural Networks
neurips,2017,1,617,Thomas,Unterthiner,jku,LIT AI Lab / University Linz,unterthiner@bioinf.jku.at,Self-Normalizing Neural Networks
neurips,2017,2,617,Andreas,Mayr,jku,LIT AI Lab / University Linz,mayr@bioinf.jku.at,Self-Normalizing Neural Networks
neurips,2017,3,617,Sepp,Hochreiter,jku,LIT AI Lab / University Linz,hochreit@bioinf.jku.at,Self-Normalizing Neural Networks
neurips,2017,0,2141,Artur,Speiser,,"research center caesar, an associate of the Max Planck Society",,Fast amortized inference of neural activity from calcium imaging data with variational autoencoders
neurips,2017,1,2141,Jinyao,Yan,,Janelia Research Campus,,Fast amortized inference of neural activity from calcium imaging data with variational autoencoders
neurips,2017,2,2141,Evan,Archer,,,,Fast amortized inference of neural activity from calcium imaging data with variational autoencoders
neurips,2017,3,2141,Lars,Buesing,,DeepMind,,Fast amortized inference of neural activity from calcium imaging data with variational autoencoders
neurips,2017,4,2141,Srinivas,Turaga,,"Janelia Research Campus, Howard Hughes Medical Institute",,Fast amortized inference of neural activity from calcium imaging data with variational autoencoders
neurips,2017,5,2141,Jakob,Macke,,"research center caesar, an associate of the Max Planck Society",,Fast amortized inference of neural activity from calcium imaging data with variational autoencoders
neurips,2017,0,2934,Ofer,Meshi,google,Google,meshi@google.com,Asynchronous Parallel Coordinate Minimization for MAP Inference
neurips,2017,1,2934,Alexander,Schwing,illinois,University of Illinois at Urbana-Champaign,aschwing@illinois.edu,Asynchronous Parallel Coordinate Minimization for MAP Inference
neurips,2017,0,671,Will,Hamilton,stanford,Stanford University,wleif@stanford.edu,Inductive Representation Learning on Large Graphs
neurips,2017,1,671,Zhitao,Ying,stanford,Stanford University,rexying@stanford.edu,Inductive Representation Learning on Large Graphs
neurips,2017,2,671,Jure,Leskovec,stanford,Stanford University,jure@cs.stanford.edu,Inductive Representation Learning on Large Graphs
neurips,2017,0,1245,Rowan,McAllister,cam,University of Cambridge,rtm26@cam.ac.uk,Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs
neurips,2017,1,1245,Carl Edward,Rasmussen,cam,University of Cambridge,cer54@cam.ac.uk,Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs
neurips,2017,0,475,Yaoqing,Yang,cmu,Carnegie Mellon University,yyaoqing@andrew.cmu.edu,Coded Distributed Computing for Inverse Problems
neurips,2017,1,475,Pulkit,Grover,cmu,CMU,pgrover@andrew.cmu.edu,Coded Distributed Computing for Inverse Problems
neurips,2017,2,475,Soummya,Kar,cmu,Carnegie Mellon University,soummyak@andrew.cmu.edu,Coded Distributed Computing for Inverse Problems
neurips,2017,0,366,Ryan,Tibshirani,cmu,Carnegie Mellon University,ryantibs@stat.cmu.edu,"Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions"
neurips,2017,0,2373,Ingmar,Kanitscheider,,UT Austin,,Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems
neurips,2017,1,2373,Ila,Fiete,,,,Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems
neurips,2017,0,2442,Zahra,Ghodsi,nyu,New York University,zg451@nyu.edu,SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud
neurips,2017,1,2442,Tianyu,Gu,nyu,NYU,tg1553@nyu.edu,SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud
neurips,2017,2,2442,Siddharth,Garg,nyu,NYU,sg175@nyu.edu,SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud
neurips,2017,0,2330,Dominique,Joncas,google,Google,dominiquep@google.com,Improved Graph Laplacian via Geometric Self-Consistency
neurips,2017,1,2330,Marina,Meila,uw,University of Washington,mmp2@uw.edu,Improved Graph Laplacian via Geometric Self-Consistency
neurips,2017,2,2330,James,McQueen,amazon,University of Washington,jmcq@amazon.com,Improved Graph Laplacian via Geometric Self-Consistency
neurips,2017,0,1832,Alessandro,Rudi,inria,INRIA,alessandro.rudi@inria.fr,Generalization Properties of Learning with Random Features
neurips,2017,1,1832,Lorenzo,Rosasco,mit,University of Genova- MIT - IIT,lrosasco@mit.edu,Generalization Properties of Learning with Random Features
neurips,2017,0,786,Arun,Venkatraman,,Carnegie Mellon University,,Predictive-State Decoders: Encoding the Future into Recurrent Networks
neurips,2017,1,786,Nicholas,Rhinehart,,Carnegie Mellon University,,Predictive-State Decoders: Encoding the Future into Recurrent Networks
neurips,2017,2,786,Wen,Sun,,Carnegie Mellon University,,Predictive-State Decoders: Encoding the Future into Recurrent Networks
neurips,2017,3,786,Lerrel,Pinto,,,,Predictive-State Decoders: Encoding the Future into Recurrent Networks
neurips,2017,4,786,Martial,Hebert,,cmu,,Predictive-State Decoders: Encoding the Future into Recurrent Networks
neurips,2017,5,786,Byron,Boots,,Georgia Tech / Google Brain,,Predictive-State Decoders: Encoding the Future into Recurrent Networks
neurips,2017,6,786,Kris,Kitani,,Carnegie Mellon University,,Predictive-State Decoders: Encoding the Future into Recurrent Networks
neurips,2017,7,786,J.,Bagnell,,Carnegie Mellon University,,Predictive-State Decoders: Encoding the Future into Recurrent Networks
neurips,2017,0,2307,Virginia,Smith,stanford,Stanford University,smithv@stanford.edu,Federated Multi-Task Learning
neurips,2017,1,2307,Chao-Kai,Chiang,usc,University of Southern California,chaokaic@usc.edu,Federated Multi-Task Learning
neurips,2017,2,2307,Maziar,Sanjabi,gmail,"University of California, Los Angeles",maziarsanjabi@gmail.com,Federated Multi-Task Learning
neurips,2017,3,2307,Ameet,Talwalkar,cmu,CMU,talwalkar@cmu.edu,Federated Multi-Task Learning
neurips,2017,0,1724,AmirEmad,Ghassami,illinois,University of Illinois at Urbana–Champaign,ghassam2@illinois.edu,Learning Causal Structures Using Regression Invariance
neurips,2017,1,1724,Saber,Salehkaleybar,illinois,University of Illinois at Urbana-Champaign,sabersk@illinois.edu,Learning Causal Structures Using Regression Invariance
neurips,2017,2,1724,Negar,Kiyavash,illinois,UIUC,kiyavash@illinois.edu,Learning Causal Structures Using Regression Invariance
neurips,2017,3,1724,Kun,Zhang,cmu,CMU,kunz1@cmu.edu,Learning Causal Structures Using Regression Invariance
neurips,2017,0,3315,Søren,Dahlgaard,supwiz,University of Copenhagen,s.dahlgaard@supwiz.com,Practical Hash Functions for Similarity Estimation and Dimensionality Reduction
neurips,2017,1,3315,Mathias,Knudsen,supwiz,University of Copenhagen,m.knudsen@supwiz.com,Practical Hash Functions for Similarity Estimation and Dimensionality Reduction
neurips,2017,2,3315,Mikkel,Thorup,ku,University of Copenhagen,mthorup@di.ku.dk,Practical Hash Functions for Similarity Estimation and Dimensionality Reduction
neurips,2017,0,3101,Tri,Dao,cornell,Stanford University,cdesa@cs.cornell.edu,Gaussian Quadrature for Kernel Features
neurips,2017,1,3101,Christopher,De Sa,stanford,Stanford,trid@stanford.edu,Gaussian Quadrature for Kernel Features
neurips,2017,2,3101,Christopher,Ré,stanford,Stanford,chrismre@cs.stanford.edu,Gaussian Quadrature for Kernel Features
neurips,2017,0,823,Karol,Hausman,usc,University of Southern California,hausman@usc.edu,Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets
neurips,2017,1,823,Yevgen,Chebotar,usc,University of Southern California,ychebota@usc.edu,Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets
neurips,2017,2,823,Stefan,Schaal,usc,MPI-IS and USC,sschaal@usc.edu,Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets
neurips,2017,3,823,Gaurav,Sukhatme,usc,USC,gaurav@usc.edu,Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets
neurips,2017,4,823,Joseph,Lim,usc,University of Southern California,limjj@usc.edu,Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets
neurips,2017,0,519,Francesco,Locatello,ethz,MPI - ETH Zürich,locatelf@ethz.ch,Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees
neurips,2017,1,519,Michael,Tschannen,ethz,ETH Zurich,michaelt@nari.ee.ethz.ch,Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees
neurips,2017,2,519,Gunnar,Raetsch,ethz,ETHZ,raetsch@inf.ethz.ch,Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees
neurips,2017,3,519,Martin,Jaggi,epfl,EPFL,martin.jaggi@epfl.ch,Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees
neurips,2017,0,2252,Arturs,Backurs,mit,MIT,backurs@mit.edu,On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks
neurips,2017,1,2252,Piotr,Indyk,mit,MIT,indyk@mit.edu,On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks
neurips,2017,2,2252,Ludwig,Schmidt,mit,MIT,ludwigs@mit.edu,On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks
neurips,2017,0,3420,Walid,Krichene,google,Google,walidk@google.com,Acceleration and Averaging in Stochastic Descent Dynamics
neurips,2017,1,3420,Peter,Bartlett,berkeley,UC Berkeley,bartlett@cs.berkeley.edu,Acceleration and Averaging in Stochastic Descent Dynamics
neurips,2017,0,1786,Guolin,Ke,pku,Microsoft Research,2qimeng13@pku.edu.cn,LightGBM: A Highly Efficient Gradient Boosting Decision Tree
neurips,2017,1,1786,Qi,Meng,microsoft,Peking University,3tnely@microsoft.com,LightGBM: A Highly Efficient Gradient Boosting Decision Tree
neurips,2017,2,1786,Thomas,Finley,microsoft,,1guolin.ke@microsoft.com,LightGBM: A Highly Efficient Gradient Boosting Decision Tree
neurips,2017,3,1786,Taifeng,Wang,microsoft,Microsoft Research,taifengw@microsoft.com,LightGBM: A Highly Efficient Gradient Boosting Decision Tree
neurips,2017,4,1786,Wei,Chen,microsoft,Microsoft Research,wche@microsoft.com,LightGBM: A Highly Efficient Gradient Boosting Decision Tree
neurips,2017,5,1786,Weidong,Ma,microsoft,Microsoft Research,weima@microsoft.com,LightGBM: A Highly Efficient Gradient Boosting Decision Tree
neurips,2017,6,1786,Qiwei,Ye,microsoft,Microsoft Research,qiwye@microsoft.com,LightGBM: A Highly Efficient Gradient Boosting Decision Tree
neurips,2017,7,1786,Tie-Yan,Liu,microsoft,Microsoft Research,tie-yan.liu@microsoft.com,LightGBM: A Highly Efficient Gradient Boosting Decision Tree
neurips,2017,0,3392,Hongyuan,Mei,jhu,JOHNS HOPKINS UNIVERSITY,hmei@cs.jhu.edu,The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process
neurips,2017,1,3392,Jason,Eisner,jhu,Johns Hopkins University,jason@cs.jhu.edu,The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process
neurips,2017,0,2724,Jian,Wu,,AQR Capital Management,,Bayesian Optimization with Gradients
neurips,2017,1,2724,Matthias,Poloczek,,Cornell University,,Bayesian Optimization with Gradients
neurips,2017,2,2724,Andrew,Wilson,,Cornell University,,Bayesian Optimization with Gradients
neurips,2017,3,2724,Peter,Frazier,,Cornell / Uber,,Bayesian Optimization with Gradients
neurips,2017,0,2079,Paul Hongsuck,Seo,postech,POSTECH,hsseo@postech.ac.kr,Visual Reference Resolution using Attention Memory for Visual Dialog
neurips,2017,1,2079,Andreas,Lehrmann,postech,Disney Research,bhhan@postech.ac.kr,Visual Reference Resolution using Attention Memory for Visual Dialog
neurips,2017,2,2079,Bohyung,Han,disneyresearch,POSTECH,andreas.lehrmann@disneyresearch.com,Visual Reference Resolution using Attention Memory for Visual Dialog
neurips,2017,3,2079,Leonid,Sigal,disneyresearch,Disney Research / University of British Columbia,lsigal@disneyresearch.com,Visual Reference Resolution using Attention Memory for Visual Dialog
neurips,2017,0,2810,Can,Karakus,ucla,UCLA,karakus@ucla.edu,Straggler Mitigation in Distributed Optimization Through Data Encoding
neurips,2017,1,2810,Yifan,Sun,ucla,,suhasdiggavi@ucla.edu,Straggler Mitigation in Distributed Optimization Through Data Encoding
neurips,2017,2,2810,Suhas,Diggavi,technicolor,UCLA,Yifan.Sun@technicolor.com,Straggler Mitigation in Distributed Optimization Through Data Encoding
neurips,2017,3,2810,Wotao,Yin,ucla,"University of California, Los Angeles",wotaoyin@math.ucla.edu,Straggler Mitigation in Distributed Optimization Through Data Encoding
neurips,2017,0,1452,Zhaohan,Guo,cmu,Carnegie Mellon University/Stanford,zguo@cs.cmu.edu,Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation
neurips,2017,1,1452,Philip,Thomas,umass,CMU,pthomas@cs.umass.edu,Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation
neurips,2017,2,1452,Emma,Brunskill,stanford,Stanford University,ebrun@cs.stanford.edu,Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation
neurips,2017,0,47,Rohit,Girdhar,,Carnegie Mellon University,,Attentional Pooling for Action Recognition
neurips,2017,1,47,Deva,Ramanan,,Carnegie Mellon University,,Attentional Pooling for Action Recognition
neurips,2017,0,876,Ho Chung,Law,ox,University Of Oxford,hlaw@stats.ox.ac.uk,Testing and Learning on Distributions with Symmetric Noise Invariance
neurips,2017,1,876,Christopher,Yau,bham,University of Oxford,c.yau@bham.ac.uk,Testing and Learning on Distributions with Symmetric Noise Invariance
neurips,2017,2,876,Dino,Sejdinovic,ox,University of Oxford,dino.sejdinovic@stats.ox.ac.uk,Testing and Learning on Distributions with Symmetric Noise Invariance
neurips,2017,0,789,Antti,Tarvainen,cai,The Curious AI Company,tarvaina@cai.fi,Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results
neurips,2017,1,789,Harri,Valpola,cai,The Curious AI Company,harri@cai.fi,Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results
neurips,2017,0,3193,Ryan,Lowe,,McGill University,,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
neurips,2017,1,3193,YI,WU,,UC Berkeley,,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
neurips,2017,2,3193,Aviv,Tamar,,UC Berkeley,,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
neurips,2017,3,3193,Jean,Harb,,McGill University,,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
neurips,2017,4,3193,OpenAI,Pieter Abbeel,,"OpenAI, UC Berkeley",,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
neurips,2017,5,3193,Igor,Mordatch,,OpenAI,,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
neurips,2017,0,1129,Liangpeng,Zhang,bham,University of Birmingham,lxz472@cs.bham.ac.uk,Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning
neurips,2017,1,1129,Ke,Tang,sustc,Southern University of Science and Technology,tangk3@sustc.edu.cn,Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning
neurips,2017,2,1129,Xin,Yao,sustc,"Southern University of Science and Technology, China",xiny@sustc.edu.cn,Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning
neurips,2017,0,1865,Christos,Louizos,uva,University of Amsterdam,c.louizos@uva.nl,Bayesian Compression for Deep Learning
neurips,2017,1,1865,Karen,Ullrich,uva,University of Amsterdam,k.ullrich@uva.nl,Bayesian Compression for Deep Learning
neurips,2017,2,1865,Max,Welling,uva,University of Amsterdam and University of California Irvine and CIFAR,m.welling@uva.nl,Bayesian Compression for Deep Learning
neurips,2017,0,2312,Cameron,Musco,mit,Massachusetts Institute of Technology,cnmusco@mit.edu,Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?
neurips,2017,1,2312,David,Woodruff,cmu,Carnegie Mellon University,dwoodruf@cs.cmu.edu,Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?
neurips,2017,0,1092,Ziming,Zhang,merl,MERL,zzhang@merl.com,Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks
neurips,2017,1,1092,Matthew,Brand,merl,Mitsubishi Electric Research Labs,brand@merl.com,Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks
neurips,2017,0,1951,Ahmed,Alaa,,,,Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes
neurips,2017,1,1951,Mihaela,van der Schaar,,,,Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes
neurips,2017,0,598,Vatsal,Sharan,stanford,Stanford University,vsharan@stanford.edu,Learning Overcomplete HMMs
neurips,2017,1,598,Sham,Kakade,stanford,University of Washington,pliang@cs.stanford.edu,Learning Overcomplete HMMs
neurips,2017,2,598,Percy,Liang,washington,Stanford University,sham@cs.washington.edu,Learning Overcomplete HMMs
neurips,2017,3,598,Gregory,Valiant,stanford,Stanford University,valiant@stanford.edu,Learning Overcomplete HMMs
neurips,2017,0,3092,Qing,Qu,columbia,Columbia University,qq2105@columbia.edu,Convolutional Phase Retrieval
neurips,2017,1,3092,Yuqian,Zhang,technion,Columbia University,yonina@ee.technion.ac.il,Convolutional Phase Retrieval
neurips,2017,2,3092,Yonina,Eldar,columbia,Israel Institute of Technology,yz2409@columbia.edu,Convolutional Phase Retrieval
neurips,2017,3,3092,John,Wright,columbia,Columbia University,jw2966@columbia.edu,Convolutional Phase Retrieval
neurips,2017,0,2613,Ashok,Cutkosky,stanford,Stanford University,ashokc@cs.stanford.edu,Stochastic and Adversarial Online Learning without Hyperparameters
neurips,2017,1,2613,Kwabena,Boahen,stanford,Stanford University,boahen@stanford.edu,Stochastic and Adversarial Online Learning without Hyperparameters
neurips,2017,0,1368,George,Papamakarios,ed,University of Edinburgh,g.papamakarios@ed.ac.uk,Masked Autoregressive Flow for Density Estimation
neurips,2017,1,1368,Theo,Pavlakou,ed,The University of Edinburgh,theo.pavlakou@ed.ac.uk,Masked Autoregressive Flow for Density Estimation
neurips,2017,2,1368,Iain,Murray,ed,University of Edinburgh,i.murray@ed.ac.uk,Masked Autoregressive Flow for Density Estimation
neurips,2017,0,1063,Dan,Alistarh,ist,IST Austria & ETH Zurich,dan.alistarh@ist.ac.at,QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding
neurips,2017,1,1063,Demjan,Grubic,gmail,ETH Zurich / Google,demjangrubic@gmail.com,QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding
neurips,2017,2,1063,Jerry,Li,mit,MIT,jerryzli@mit.edu,QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding
neurips,2017,3,1063,Ryota,Tomioka,microsoft,Microsoft Research Cambridge,ryoto@microsoft.com,QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding
neurips,2017,4,1063,Milan,Vojnovic,lse,London School of Economics and Political Science (LSE),M.Vojnovic@lse.ac.uk,QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding
neurips,2017,0,3374,Danijar,Hafner,danijar,Google Brain,mail@danijar.com,Learning Hierarchical Information Flow with Recurrent Neural Modules
neurips,2017,1,3374,Alexander,Irpan,google,Google,jcdavidson@google.com,Learning Hierarchical Information Flow with Recurrent Neural Modules
neurips,2017,2,3374,James,Davidson,google,Google Brain,alexirpan@google.com,Learning Hierarchical Information Flow with Recurrent Neural Modules
neurips,2017,3,3374,Nicolas,Heess,google,Google DeepMind,heess@google.com,Learning Hierarchical Information Flow with Recurrent Neural Modules
neurips,2017,0,882,Giulia,Fanti,,Carnegie Mellon University,,Deanonymization in the Bitcoin P2P Network
neurips,2017,1,882,Pramod,Viswanath,,UIUC,,Deanonymization in the Bitcoin P2P Network
neurips,2017,0,356,Yanbo,Fan,ia,"NLPR, CASIA",yanbo.fan@nlpr.ia.ac.cn,Learning with Average Top-k Loss
neurips,2017,1,356,Siwei,Lyu,ia,SUNY at Albany,hubg@nlpr.ia.ac.cn,Learning with Average Top-k Loss
neurips,2017,2,356,Yiming,Ying,albany,State University of New York at Albany,slyu@albany.edu,Learning with Average Top-k Loss
neurips,2017,3,356,Baogang,Hu,albany,Chinese Academy of Sciences,yying@albany.edu,Learning with Average Top-k Loss
neurips,2017,0,250,Yuan-Ting,Hu,,University of Illinois Urbana-Champaign,,MaskRNN: Instance Level Video Object Segmentation
neurips,2017,1,250,Jia-Bin,Huang,,Virginia Tech,,MaskRNN: Instance Level Video Object Segmentation
neurips,2017,2,250,Alexander,Schwing,,University of Illinois at Urbana-Champaign,,MaskRNN: Instance Level Video Object Segmentation
neurips,2017,0,920,Dipan,Pal,cmu,Carnegie Mellon University,dipanp@cmu.edu,Max-Margin Invariant Features from Transformed Unlabelled Data
neurips,2017,1,920,Ashwin,Kannan,cmu,Carnegie Mellon University,aalapakk@cmu.edu,Max-Margin Invariant Features from Transformed Unlabelled Data
neurips,2017,2,920,Gautam,Arakalgud,cmu,Carnegie Mellon University,garakalgud@cmu.edu,Max-Margin Invariant Features from Transformed Unlabelled Data
neurips,2017,3,920,Marios,Savvides,cmu,Carnegie Mellon University,marioss@cmu.edu,Max-Margin Invariant Features from Transformed Unlabelled Data
neurips,2017,0,1466,Greg,Van Buskirk,utdallas,UT Dallas,greg.vanbuskirk@utdallas.edu,Sparse Approximate Conic Hulls
neurips,2017,1,1466,Benjamin,Raichel,utdallas,UT Dallas,benjamin.raichel@utdallas.edu,Sparse Approximate Conic Hulls
neurips,2017,2,1466,Nicholas,Ruozzi,utdallas,UTDallas,nicholas.ruozzi@utdallas.edu,Sparse Approximate Conic Hulls
neurips,2017,0,557,Wei,Shen,gmail,Shanghai University,shenwei1231@gmail.com,Label Distribution Learning Forests
neurips,2017,1,557,KAI,ZHAO,gmail,Nankai University,zhaok1206@gmail.com,Label Distribution Learning Forests
neurips,2017,2,557,Yilu,Guo,gmail,Shanghai University,gyl.luan0@gmail.com,Label Distribution Learning Forests
neurips,2017,3,557,Alan,Yuille,gmail,Johns Hopkins University,alan.l.yuille@gmail.com,Label Distribution Learning Forests
neurips,2017,0,2163,Shinji,Ito,nec,NEC Corporation,s-ito@me.jp.nec.com,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation
neurips,2017,1,2163,Daisuke,Hatano,nii,National Institute of Informatics,hatano@nii.ac.jp,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation
neurips,2017,2,2163,Hanna,Sumita,nii,National Institute of Informatics,sumita@nii.ac.jp,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation
neurips,2017,3,2163,Akihiro,Yabe,nec,,a-yabe@cq.jp.nec.com,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation
neurips,2017,4,2163,Takuro,Fukunaga,nii,National Institute of Informatics,takuro@nii.ac.jp,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation
neurips,2017,5,2163,Naonori,Kakimura,keio,,kakimura@math.keio.ac.jp,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation
neurips,2017,6,2163,Ken-Ichi,Kawarabayashi,nii,,k-keniti@nii.ac.jp,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation
neurips,2017,0,2518,Yuanyuan,Liu,cuhk,The Chinese University of Hong Kong,yyliu@cse.cuhk.edu.hk,Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds
neurips,2017,1,2518,Fanhua,Shang,cuhk,The Chinese University of Hong Kong,fhshang@cse.cuhk.edu.hk,Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds
neurips,2017,2,2518,James,Cheng,cuhk,The Chinese University of Hong Kong,jcheng@cse.cuhk.edu.hk,Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds
neurips,2017,3,2518,Hong,Cheng,cuhk,The Chinese University of Hong Kong,hcheng@se.cuhk.edu.hk,Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds
neurips,2017,4,2518,Licheng,Jiao,xidian,Xidian University,lchjiao@mail.xidian.edu.cn,Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds
neurips,2017,0,2844,Dustin,Tran,,Columbia University & OpenAI,,Hierarchical Implicit Models and Likelihood-Free Variational Inference
neurips,2017,1,2844,Rajesh,Ranganath,,Princeton University,,Hierarchical Implicit Models and Likelihood-Free Variational Inference
neurips,2017,2,2844,David,Blei,,Columbia University,,Hierarchical Implicit Models and Likelihood-Free Variational Inference
neurips,2017,0,753,Mainak,Jas,,Télécom ParisTech,,Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding
neurips,2017,1,753,Tom,Dupré la Tour,,Télécom ParisTech,,Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding
neurips,2017,2,753,Umut,Simsekli,,Bogazici University,,Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding
neurips,2017,3,753,Alexandre,Gramfort,,"LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay",,Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding
neurips,2017,0,3302,Harm,de Vries,harmdevries,Université de Montréal,mail@harmdevries.com,Modulating early visual processing by language
neurips,2017,1,3302,Florian,Strub,inria,University of Lille,florian.strub@inria.fr,Modulating early visual processing by language
neurips,2017,2,3302,Jeremie,Mary,univ-lille3,INRIA / Univ. Lille,jeremie.mary@univ-lille3.fr,Modulating early visual processing by language
neurips,2017,3,3302,Hugo,Larochelle,google,Google Brain,hugolarochelle@google.com,Modulating early visual processing by language
neurips,2017,4,3302,Olivier,Pietquin,google,DeepMind,pietquin@google.com,Modulating early visual processing by language
neurips,2017,5,3302,Aaron,Courville,gmail,U. Montreal,aaron.courville@gmail.com,Modulating early visual processing by language
neurips,2017,0,2892,Vitaly,Kuznetsov,nyu,Google Research,vitaly@cims.nyu.edu,Discriminative State Space Models
neurips,2017,1,2892,Mehryar,Mohri,nyu,Courant Institute and Google,mohri@cims.nyu.edu,Discriminative State Space Models
neurips,2017,0,1290,Serhii,Havrylov,ed,University of Edinburgh,s.havrylov@inf.ed.ac.uk,Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols
neurips,2017,1,1290,Ivan,Titov,ed,University of Edinburgh / University of Amsterdam,ititov@inf.ed.ac.uk,Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols
neurips,2017,0,17,Zhen,He,,University College London,,"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning"
neurips,2017,1,17,Shaobing,Gao,,Sichuan University,,"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning"
neurips,2017,2,17,Liang,Xiao,,National University of Defense Technology,,"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning"
neurips,2017,3,17,Daxue,Liu,,National University of Defense Technology,,"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning"
neurips,2017,4,17,Hangen,He,,National University of Defense Technology,,"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning"
neurips,2017,5,17,David,Barber,,University College London,,"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning"
neurips,2017,0,1728,Zheng,Wen,adobe,Adobe Research,zwen@adobe.com,Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback
neurips,2017,1,1728,Branislav,Kveton,adobe,Adobe Research,kveton@adobe.com,Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback
neurips,2017,2,1728,Michal,Valko,inria,Inria Lille - Nord Europe,michal.valko@inria.fr,Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback
neurips,2017,3,1728,Sharan,Vaswani,ubc,University of British Columbia,sharanv@cs.ubc.ca,Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback
neurips,2017,0,2998,Ahmet,Alacaoglu,epfl,EPFL,ahmet.alacaoglu@epfl.ch,Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization
neurips,2017,1,2998,Quoc,Tran Dinh,epfl,"Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, North Carolina",volkan.cevher@epfl.ch,Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization
neurips,2017,2,2998,Olivier,Fercoq,unc,Telecom ParisTech,quoctd@email.unc.edu,Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization
neurips,2017,3,2998,Volkan,Cevher,telecom-paristech,EPFL,olivier.fercoq@telecom-paristech.fr,Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization
neurips,2017,0,815,Carl,Jidling,uu,Uppsala University,carl.jidling@it.uu.se,Linearly constrained Gaussian processes
neurips,2017,1,815,Niklas,Wahlström,uu,Uppsala University,niklas.wahlstrom@it.uu.se,Linearly constrained Gaussian processes
neurips,2017,2,815,Adrian,Wills,newcastle,"University of Newcastle, Australia",adrian.wills@newcastle.edu.au,Linearly constrained Gaussian processes
neurips,2017,3,815,Thomas,Schön,uu,Uppsala University,thomas.schon@it.uu.se,Linearly constrained Gaussian processes
neurips,2017,0,3285,Michael,Eickenberg,,UC Berkeley,,Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities
neurips,2017,1,3285,Georgios,Exarchakis,,École Normale Supérieure,,Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities
neurips,2017,2,3285,Matthew,Hirn,,Michigan State University,,Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities
neurips,2017,3,3285,Stephane,Mallat,,Ecole normale superieure,,Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities
neurips,2017,0,3299,Jacob,Abernethy,gatech,University of Michigan,prof@gatech.edu,On Frank-Wolfe and Equilibrium Computation
neurips,2017,1,3299,Jun-Kun,Wang,gatech,Georgia Institute of Technology,jimwang@gatech.edu,On Frank-Wolfe and Equilibrium Computation
neurips,2017,0,3170,Roderich,Gross,sheffield,The University of Sheffield,r.gross@sheffield.ac.uk,Generalizing GANs: A Turing Perspective
neurips,2017,1,3170,Yue,Gu,sheffield,The University of Sheffield,ygu16@sheffield.ac.uk,Generalizing GANs: A Turing Perspective
neurips,2017,2,3170,Wei,Li,york,University of York,wei.li@york.ac.uk,Generalizing GANs: A Turing Perspective
neurips,2017,3,3170,Melvin,Gauci,harvard,Harvard University,mgauci@g.harvard.edu,Generalizing GANs: A Turing Perspective
neurips,2017,0,3470,Xiaojie,Jin,,National University of Singapore & Snap Research,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,1,3470,Huaxin,Xiao,,NUDT,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,2,3470,Xiaohui,Shen,,Adobe,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,3,3470,Jimei,Yang,,Adobe Research,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,4,3470,Zhe,Lin,,Adobe,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,5,3470,Yunpeng,Chen,,National University of Singapore,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,6,3470,Zequn,Jie,,Tencent AI Lab,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,7,3470,Jiashi,Feng,,National University of Singapore,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,8,3470,Shuicheng,Yan,,Qihoo 360 AI Institute,,Predicting Scene Parsing and Motion Dynamics in the Future
neurips,2017,0,477,Zhaobin,Kuang,wisc,"University of Wisconsin, Madison",zkuang@wisc.edu1,A Screening Rule for l1-Regularized Ising Model Estimation
neurips,2017,1,477,Sinong,Geng,wisc,University of Wisconsin Madison,sgeng2@wisc.edu2,A Screening Rule for l1-Regularized Ising Model Estimation
neurips,2017,2,477,David,Page,wisc,UW-Madison,page@biostat.wisc.edu3,A Screening Rule for l1-Regularized Ising Model Estimation
neurips,2017,0,2272,Thomas,Bonald,telecom-paristech,Telecom ParisTech,thomas.bonald@telecom-paristech.fr,A Minimax Optimal Algorithm for Crowdsourcing
neurips,2017,1,2272,Richard,Combes,supelec,Centrale-Supelec,richard.combes@supelec.fr,A Minimax Optimal Algorithm for Crowdsourcing
neurips,2017,0,3202,Ilias,Diakonikolas,usc,USC,diakonik@usc.edu,Communication-Efficient Distributed Learning of Discrete Distributions
neurips,2017,1,3202,Elena,Grigorescu,purdue,Purdue University,nataraj2@purdue.edu,Communication-Efficient Distributed Learning of Discrete Distributions
neurips,2017,2,3202,Jerry,Li,purdue,MIT,elena-g@purdue.edu,Communication-Efficient Distributed Learning of Discrete Distributions
neurips,2017,3,3202,Abhiram,Natarajan,mit,Purdue University,jerryzli@mit.edu,Communication-Efficient Distributed Learning of Discrete Distributions
neurips,2017,4,3202,Krzysztof,Onak,ibm,IBM T.J. Watson Research Center,konak@us.ibm.com,Communication-Efficient Distributed Learning of Discrete Distributions
neurips,2017,5,3202,Ludwig,Schmidt,mit,MIT,ludwigs@mit.edu,Communication-Efficient Distributed Learning of Discrete Distributions
neurips,2017,0,1535,Yedid,Hoshen,fb,Facebook AI Research,yedidh@fb.com,VAIN: Attentional Multi-agent Predictive Modeling
neurips,2017,0,1739,Adam,Kosiorek,ox,University of Oxford,adamk@robots.ox.ac.uk,Hierarchical Attentive Recurrent Tracking
neurips,2017,1,1739,Alex,Bewley,ox,University of Oxford,bewley@robots.ox.ac.uk,Hierarchical Attentive Recurrent Tracking
neurips,2017,2,1739,Ingmar,Posner,ox,Oxford University,ingmar@robots.ox.ac.uk,Hierarchical Attentive Recurrent Tracking
neurips,2017,0,2246,Wojciech,Czarnecki,google,DeepMind,lejlot@google.com,Sobolev Training for Neural Networks
neurips,2017,1,2246,Simon,Osindero,google,DeepMind,osindero@google.com,Sobolev Training for Neural Networks
neurips,2017,2,2246,Max,Jaderberg,google,DeepMind,jaderberg@google.com,Sobolev Training for Neural Networks
neurips,2017,3,2246,Grzegorz,Swirszcz,google,DeepMind @ Google,swirszcz@google.com,Sobolev Training for Neural Networks
neurips,2017,4,2246,Razvan,Pascanu,google,Google DeepMind,razp@google.com,Sobolev Training for Neural Networks
neurips,2017,0,429,Tomoya,Murata,msi,NTT DATA Mathematical Systems Inc.,murata@msi.co.jp,Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization
neurips,2017,1,429,Taiji,Suzuki,u-tokyo,taiji@mist.i.u-tokyo.ac.jp,taiji@mist.i.u-tokyo.ac.jp,Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization
neurips,2017,0,914,Bo-Jian,Hou,nju,LAMDA Group,houbj@lamda.nju.edu.cn,Learning with Feature Evolvable Streams
neurips,2017,1,914,Lijun,Zhang,nju,Nanjing University (NJU),zhanglj@lamda.nju.edu.cn,Learning with Feature Evolvable Streams
neurips,2017,2,914,Zhi-Hua,Zhou,nju,Nanjing University,zhouzh@lamda.nju.edu.cn,Learning with Feature Evolvable Streams
neurips,2017,0,587,Felix,Berkenkamp,ethz,ETH Zurich,befelix@inf.ethz.ch,Safe Model-based Reinforcement Learning with Stability Guarantees
neurips,2017,1,587,Matteo,Turchetta,utoronto,ETH Zurich,schoellig@utias.utoronto.ca,Safe Model-based Reinforcement Learning with Stability Guarantees
neurips,2017,2,587,Angela,Schoellig,ethz,,matteotu@inf.ethz.ch,Safe Model-based Reinforcement Learning with Stability Guarantees
neurips,2017,3,587,Andreas,Krause,ethz,ETHZ,krausea@ethz.ch,Safe Model-based Reinforcement Learning with Stability Guarantees
neurips,2017,0,2986,Kristjan,Greenewald,,University of Michigan,,"Time-dependent spatially varying graphical models, with application to brain fMRI data analysis"
neurips,2017,1,2986,Seyoung,Park,,Yale University,,"Time-dependent spatially varying graphical models, with application to brain fMRI data analysis"
neurips,2017,2,2986,Shuheng,Zhou,,University of Michigan,,"Time-dependent spatially varying graphical models, with application to brain fMRI data analysis"
neurips,2017,3,2986,Alexander,Giessing,,University of Michigan,,"Time-dependent spatially varying graphical models, with application to brain fMRI data analysis"
neurips,2017,0,2590,Andrei-Cristian,Barbos,u-bordeaux,University of Bordeaux,andbarbos@u-bordeaux.fr,Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling
neurips,2017,1,2590,Francois,Caron,ims-bordeaux,,giova@ims-bordeaux.fr,Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling
neurips,2017,2,2590,Jean-François,Giovannelli,ox,University of Bordeaux,caron@stats.ox.ac.uk,Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling
neurips,2017,3,2590,Arnaud,Doucet,ox,Oxford,doucet@stats.ox.ac.uk,Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling
neurips,2017,0,2504,Liping,Liu,,Tufts University,,Context Selection for Embedding Models
neurips,2017,1,2504,Francisco,Ruiz,,,,Context Selection for Embedding Models
neurips,2017,2,2504,Susan,Athey,,Stanford University,,Context Selection for Embedding Models
neurips,2017,3,2504,David,Blei,,Columbia University,,Context Selection for Embedding Models
neurips,2017,0,717,Kristofer,Bouchard,,Lawrence Berkeley National Laboratory,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,1,717,Alejandro,Bujan,,UC Berkeley,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,2,717,Fred,Roosta,,University of California Berkeley,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,3,717,Shashanka,Ubaru,,University of Minnesota,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,4,717,Mr.,Prabhat,,LBL/NERSC,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,5,717,Antoine,Snijders,,,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,6,717,Jian-Hua,Mao,,,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,7,717,Edward,Chang,,,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,8,717,Michael,Mahoney,,UC Berkeley,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,9,717,Sharmodeep,Bhattacharya,,Oregon State University,,Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction
neurips,2017,0,3272,Zihang,Dai,cmu,Carnegie Mellon University,dzihang@cs.cmu.edu,Good Semi-supervised Learning That Requires a Bad GAN
neurips,2017,1,3272,Zhilin,Yang,cmu,Carnegie Mellon University,zhiliny@cs.cmu.edu,Good Semi-supervised Learning That Requires a Bad GAN
neurips,2017,2,3272,Fan,Yang,cmu,Carnegie Mellon University,fanyang1@cs.cmu.edu,Good Semi-supervised Learning That Requires a Bad GAN
neurips,2017,3,3272,William,Cohen,cmu,Carnegie Mellon University,wcohen@cs.cmu.edu,Good Semi-supervised Learning That Requires a Bad GAN
neurips,2017,4,3272,Russ,Salakhutdinov,cmu,,rsalakhu@cs.cmu.edu,Good Semi-supervised Learning That Requires a Bad GAN
neurips,2017,0,2420,Yitong,Li,,Duke University,,Targeting EEG/LFP Synchrony with Neural Nets
neurips,2017,1,2420,michael,Murias,,Duke University,,Targeting EEG/LFP Synchrony with Neural Nets
neurips,2017,2,2420,samantha,Major,,,,Targeting EEG/LFP Synchrony with Neural Nets
neurips,2017,3,2420,geraldine,Dawson,,,,Targeting EEG/LFP Synchrony with Neural Nets
neurips,2017,4,2420,Kafui,Dzirasa,,Duke University,,Targeting EEG/LFP Synchrony with Neural Nets
neurips,2017,5,2420,Lawrence,Carin,,Duke University,,Targeting EEG/LFP Synchrony with Neural Nets
neurips,2017,6,2420,David,Carlson,,Duke University,,Targeting EEG/LFP Synchrony with Neural Nets
neurips,2017,0,2891,Anton,Mallasto,ku,University of Copenhagen,mallasto@di.ku.dk,Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes
neurips,2017,1,2891,Aasa,Feragen,ku,University of Copenhagen,aasa@di.ku.dk,Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes
neurips,2017,0,1611,Holakou,Rahmanian,ucsc,University of California at Santa Cruz,holakou@ucsc.edu,Online Dynamic Programming
neurips,2017,1,1611,Manfred K.,Warmuth,ucsc,Univ. of Calif. at Santa Cruz,manfred@ucsc.edu,Online Dynamic Programming
neurips,2017,0,3169,Aaron,van den Oord,google,Google Deepmind,avdnoord@google.com,Neural Discrete Representation Learning
neurips,2017,1,3169,Oriol,Vinyals,google,Google DeepMind,vinyals@google.com,Neural Discrete Representation Learning
neurips,2017,2,3169,koray,kavukcuoglu,google,Google DeepMind,korayk@google.com,Neural Discrete Representation Learning
neurips,2017,0,993,Haizi,Yu,illinois,University of Illinois at Urbana-Champaign,haiziyu7@illinois.edu,Probabilistic Rule Realization and Selection
neurips,2017,1,993,Tianxi,Li,umich,University of Michigan,tianxili@umich.edu,Probabilistic Rule Realization and Selection
neurips,2017,2,993,Lav,Varshney,illinois,University of Illinois at Urbana-Champaign,varshney@illinois.edu,Probabilistic Rule Realization and Selection
neurips,2017,0,2020,Marco,Fraccaro,,Technical University of Denmark (DTU),,A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning
neurips,2017,1,2020,Simon,Kamronn,,Technical University of Denmark,,A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning
neurips,2017,2,2020,Ulrich,Paquet,,,,A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning
neurips,2017,3,2020,Ole,Winther,,Technical University of Denmark,,A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning
neurips,2017,0,1236,Kevin,Roth,ethz,ETH,kevin.roth@inf.ethz.ch,Stabilizing Training of Generative Adversarial Networks through Regularization
neurips,2017,1,1236,Aurelien,Lucchi,ethz,ETH Zurich,aurelien.lucchi@inf.ethz.ch,Stabilizing Training of Generative Adversarial Networks through Regularization
neurips,2017,2,1236,Sebastian,Nowozin,microsoft,Microsoft Research Cambridge,sebastian.Nowozin@microsoft.com,Stabilizing Training of Generative Adversarial Networks through Regularization
neurips,2017,3,1236,Thomas,Hofmann,ethz,ETH Zurich,thomas.hofmann@inf.ethz.ch,Stabilizing Training of Generative Adversarial Networks through Regularization
neurips,2017,0,1293,Francesco,Orabona,orabona,Stony Brook University,francesco@orabona.com,Training Deep Networks without Learning Rates Through Coin Betting
neurips,2017,1,1293,Tatiana,Tommasi,uniroma1,University of Rome La Sapienza,tommasi@dis.uniroma1.it,Training Deep Networks without Learning Rates Through Coin Betting
neurips,2017,0,70,Jian,Zhao,nus,National University of Singapore,zhaojian90@u.nus.edu,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,1,70,Lin,Xiong,nus,Panasonic R&D Center Singapore,jianshu@u.nus.edu,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,2,70,Panasonic,Karlekar Jayashree,panasonic,"Panasonic, Singapore",lin.xiong@sg.panasonic.com,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,3,70,Jianshu,Li,panasonic,National University of Singapore,karlekar.jayashree@sg.panasonic.com,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,4,70,Fang,Zhao,panasonic,National University of Singapore,sugiri.pranata@sg.panasonic.com,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,5,70,Zhecan,Wang,panasonic,Franklin. W. Olin College of Engineering,shengmei.shen@sg.panasonic.com,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,6,70,Panasonic,Sugiri Pranata,olin,"Panasonic, Singapore",zhecan.wang@students.olin.edu,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,7,70,Panasonic,Shengmei Shen,nus,"Panasonic, Singapore",elezhf@u.nus.edu,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,8,70,Shuicheng,Yan,nus,National University of Singapore,eleyans@u.nus.edu,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,9,70,Jiashi,Feng,nus,National University of Singapore,elefjia@u.nus.edu,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis
neurips,2017,0,2470,Christian,Borgs,microsoft,Microsoft Research New England,borgs@microsoft.com,Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation
neurips,2017,1,2470,Jennifer,Chayes,microsoft,Microsoft Research,jchayes@microsoft.com,Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation
neurips,2017,2,2470,Christina,Lee,mit,Microsoft Research,celee@mit.edu,Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation
neurips,2017,3,2470,Devavrat,Shah,mit,Massachusetts Institute of Technology,devavrat@mit.edu,Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation
neurips,2017,0,1048,Ryuichi,Kiryo,,UTokyo/RIKEN,,Positive-Unlabeled Learning with Non-Negative Risk Estimator
neurips,2017,1,1048,Gang,Niu,,The University of Tokyo / RIKEN,,Positive-Unlabeled Learning with Non-Negative Risk Estimator
neurips,2017,2,1048,Marthinus,du Plessis,,The University of Tokyo,,Positive-Unlabeled Learning with Non-Negative Risk Estimator
neurips,2017,3,1048,Masashi,Sugiyama,,RIKEN / University of Tokyo,,Positive-Unlabeled Learning with Non-Negative Risk Estimator
neurips,2017,0,2872,Vaishnavh,Nagarajan,cmu,Carnegie Mellon University,vaishnavh@cs.cmu.edu,Gradient descent GAN optimization is locally stable
neurips,2017,1,2872,J. Zico,Kolter,cmu,Carnegie Mellon University,zkolter@cs.cmu.edu,Gradient descent GAN optimization is locally stable
neurips,2017,0,2334,Cong,Fang,pku,Peking University,fangcong@pku.edu.cn,Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers
neurips,2017,1,2334,Feng,Cheng,pku,Peking University,fengcheng@pku.edu.cn,Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers
neurips,2017,2,2334,Zhouchen,Lin,pku,Peking University,zlin@pku.edu.cn,Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers
neurips,2017,0,166,Hong,Chen,hzau,University of Pittsburgh,chenh@mail.hzau.edu.cn,Group Sparse Additive Machine
neurips,2017,1,166,Xiaoqian,Wang,gmail,University of Pittsburgh,xqwang1991@gmail.com,Group Sparse Additive Machine
neurips,2017,2,166,Cheng,Deng,xidian,"School of Electronic Engineering, Xidian University, China",chdeng@mail.xidian.edu.cn,Group Sparse Additive Machine
neurips,2017,3,166,Heng,Huang,pitt,University of Pittsburgh,heng.huang@pitt.edu,Group Sparse Additive Machine
neurips,2017,0,1209,Alireza,Makhzani,toronto,University of Toronto,makhzani@psi.toronto.edu,PixelGAN Autoencoders
neurips,2017,1,1209,Brendan,Frey,toronto,"Deep Genomics, Vector Institute, Univ. Toronto",frey@psi.toronto.edu,PixelGAN Autoencoders
neurips,2017,0,2667,Rishit,Sheth,tufts,Tufts University,rishit.sheth@tufts.edu,Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models
neurips,2017,1,2667,Roni,Khardon,tufts,Tufts University,roni@cs.tufts.edu,Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models
neurips,2017,0,2889,Aaditya,Ramdas,,"University of California, Berkeley",,Online control of the false discovery rate with decaying memory
neurips,2017,1,2889,Fanny,Yang,,"University of California, Berkeley",,Online control of the false discovery rate with decaying memory
neurips,2017,2,2889,Martin,Wainwright,,UC Berkeley,,Online control of the false discovery rate with decaying memory
neurips,2017,3,2889,Michael,Jordan,,UC Berkeley,,Online control of the false discovery rate with decaying memory
neurips,2017,0,459,Noam,Brown,cmu,Carnegie Mellon University,noamb@cs.cmu.edu,Safe and Nested Subgame Solving for Imperfect-Information Games
neurips,2017,1,459,Tuomas,Sandholm,cmu,Carnegie Mellon University,sandholm@cs.cmu.edu,Safe and Nested Subgame Solving for Imperfect-Information Games
neurips,2017,0,1687,Ben,London,amazon,Amazon,blondon@amazon.com,A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent
neurips,2017,0,95,El Mahdi,El Mhamdi,epfl,EPFL,elmahdi.elmhamdi@epfl.ch,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning
neurips,2017,1,95,Rachid,Guerraoui,epfl,,rachid.guerraoui@epfl.ch,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning
neurips,2017,2,95,Hadrien,Hendrikx,gmail,EPFL,hadrien.hendrikx@gmail.com,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning
neurips,2017,3,95,Alexandre,Maurer,epfl,EPFL,alexandre.maurer@epfl.ch,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning
neurips,2017,0,338,Jun-Yan,Zhu,,UC Berkeley,,Toward Multimodal Image-to-Image Translation
neurips,2017,1,338,Richard,Zhang,,"University of California, Berkeley",,Toward Multimodal Image-to-Image Translation
neurips,2017,2,338,Deepak,Pathak,,UC Berkeley,,Toward Multimodal Image-to-Image Translation
neurips,2017,3,338,Trevor,Darrell,,UC Berkeley,,Toward Multimodal Image-to-Image Translation
neurips,2017,4,338,Alexei,Efros,,UC Berkeley,,Toward Multimodal Image-to-Image Translation
neurips,2017,5,338,Oliver,Wang,,Adobe Research,,Toward Multimodal Image-to-Image Translation
neurips,2017,6,338,Eli,Shechtman,,,,Toward Multimodal Image-to-Image Translation
neurips,2017,0,2186,Ashia,Wilson,berkeley,UC Berkeley,ashia@berkeley.edu,The Marginal Value of Adaptive Gradient Methods in Machine Learning
neurips,2017,1,2186,Rebecca,Roelofs,berkeley,UC Berkeley,roelofs@berkeley.edu,The Marginal Value of Adaptive Gradient Methods in Machine Learning
neurips,2017,2,2186,Mitchell,Stern,berkeley,UC Berkeley,mitchell@berkeley.edu,The Marginal Value of Adaptive Gradient Methods in Machine Learning
neurips,2017,3,2186,Nati,Srebro,ttic,TTI-Chicago,nati@ttic.edu,The Marginal Value of Adaptive Gradient Methods in Machine Learning
neurips,2017,4,2186,Benjamin,Recht,berkeley,UC Berkeley,brecht@berkeley.edu,The Marginal Value of Adaptive Gradient Methods in Machine Learning
neurips,2017,0,1642,Ge,Yang,,Harvard University,,Mean Field Residual Networks: On the Edge of Chaos
neurips,2017,1,1642,Samuel,Schoenholz,,Google Brain,,Mean Field Residual Networks: On the Edge of Chaos
neurips,2017,0,1371,Lihua,Lei,berkeley,UC Berkeley,lihua.lei@berkeley.edu,Non-convex Finite-Sum Optimization Via SCSG Methods
neurips,2017,1,1371,Cheng,Ju,berkeley,"University of California, Berkeley",cju@berkeley.edu,Non-convex Finite-Sum Optimization Via SCSG Methods
neurips,2017,2,1371,Jianbo,Chen,berkeley,"University of California, Berkeley",jianbochen@berkeley.edu,Non-convex Finite-Sum Optimization Via SCSG Methods
neurips,2017,3,1371,Michael,Jordan,berkeley,UC Berkeley,jordan@stat.berkeley.edu,Non-convex Finite-Sum Optimization Via SCSG Methods
neurips,2017,0,1250,Aryan,Mokhtari,upenn,University of Pennsylvania,aryanm@seas.upenn.edu,First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization
neurips,2017,1,1250,Alejandro,Ribeiro,upenn,University of Pennsylvania,aribeiro@seas.upenn.edu,First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization
neurips,2017,0,2404,Hugh,Salimbeni,ic,Imperial College London,hrs13@ic.ac.uk,Doubly Stochastic Variational Inference for Deep Gaussian Processes
neurips,2017,1,2404,Marc,Deisenroth,imperial,Imperial College London,m.deisenroth@imperial.ac.uk,Doubly Stochastic Variational Inference for Deep Gaussian Processes
neurips,2017,0,175,Muhammad Bilal,Zafar,mpi-sws,MPI-SWS,mzafar@mpi-sws.org,From Parity to Preference-based Notions of Fairness in Classification
neurips,2017,1,175,Isabel,Valera,mpg,MPI for Intelligent Systems,isabel.valera@tue.mpg.de,From Parity to Preference-based Notions of Fairness in Classification
neurips,2017,2,175,Manuel,Rodriguez,mpi-sws,MPI SWS,manuelgr@mpi-sws.org,From Parity to Preference-based Notions of Fairness in Classification
neurips,2017,3,175,Krishna,Gummadi,mpi-sws,Max Planck Institute for Software Systems,gummadi@mpi-sws.org,From Parity to Preference-based Notions of Fairness in Classification
neurips,2017,4,175,Adrian,Weller,cam,University of Cambridge,aw665@cam.ac.uk,From Parity to Preference-based Notions of Fairness in Classification
neurips,2017,0,455,Ilja,Kuzborskij,gmail,EPFL / Google Brain Intern,ilja.kuzborskij@gmail.com,Nonparametric Online Regression while Learning the Metric
neurips,2017,1,455,Nicolò,Cesa-Bianchi,unimi,"Università degli Studi di Milano, Italy",nicolo.cesa-bianchi@unimi.it,Nonparametric Online Regression while Learning the Metric
neurips,2017,0,1025,Alberto,Bietti,inria,Inria,alberto.bietti@inria.fr,Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure
neurips,2017,1,1025,Julien,Mairal,inria,Inria,julien.mairal@inria.fr,Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure
neurips,2017,0,2512,Anastasiia,Mishchuk,gmail,"Szkocka Research Group, Ukraine",anastasiya.mishchuk@gmail.com,Working hard to know your neighbor's margins: Local descriptor learning loss
neurips,2017,1,2512,Dmytro,Mishkin,cvut,Czech Technical University in Prague,mishkdmy@cmp.felk.cvut.cz,Working hard to know your neighbor's margins: Local descriptor learning loss
neurips,2017,2,2512,Filip,Radenovic,cvut,"Visual Recognition Group, CTU in Prague",filip.radenovic@cmp.felk.cvut.cz,Working hard to know your neighbor's margins: Local descriptor learning loss
neurips,2017,3,2512,Jiri,Matas,cvut,Czech Technical University,matas@cmp.felk.cvut.cz,Working hard to know your neighbor's margins: Local descriptor learning loss
neurips,2017,0,1251,Shumeet,Baluja,google,"Google, Inc.",shumeet@google.com,Hiding Images in Plain Sight: Deep Steganography
neurips,2017,0,1177,Remi,Lam,mit,MIT,rlam@mit.edu,Lookahead  Bayesian Optimization with Inequality Constraints
neurips,2017,1,1177,Karen,Willcox,mit,MIT,kwillcox@mit.edu,Lookahead  Bayesian Optimization with Inequality Constraints
neurips,2017,0,2683,Mehryar,Mohri,nyu,Courant Institute and Google,mohri@cims.nyu.edu,Online Learning with Transductive Regret
neurips,2017,1,2683,Scott,Yang,nyu,D. E. Shaw & Co.,yangs@cims.nyu.edu,Online Learning with Transductive Regret
neurips,2017,0,1294,Alejandro,Newell,umich,University of Michigan,alnewell@umich.edu,Pixels to Graphs by Associative Embedding
neurips,2017,1,1294,Jia,Deng,umich,University of Michigan,jiadeng@umich.edu,Pixels to Graphs by Associative Embedding
neurips,2017,0,2513,Chaobing,Song,tsinghua,Tsinghua University,songcb16@mails.tsinghua.edu.cn,Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex
neurips,2017,1,2513,Shaobo,Cui,tsinghua,Tsinghua University,shaobocui16@mails.tsinghua.edu.cn,Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex
neurips,2017,2,2513,Yong,Jiang,tsinghua,Tsinghua-Berkeley Shenzhen Institute,jiangy@sz.tsinghua.edu.cn,Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex
neurips,2017,3,2513,Shu-Tao,Xia,tsinghua,Tsinghua University,xiast@sz.tsinghua.edu.cn,Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex
neurips,2017,0,1738,Aurko,Roy,google,Google,aurkor@google.com,Reinforcement Learning under Model Mismatch
neurips,2017,1,1738,Huan,Xu,gatech,,huan.xu@isye.gatech.edu,Reinforcement Learning under Model Mismatch
neurips,2017,2,1738,Sebastian,Pokutta,gatech,Georgia Institute of Technology,sebastian.pokutta@isye.gatech.edu,Reinforcement Learning under Model Mismatch
neurips,2017,0,2018,Yarin,Gal,cam,University of Oxford,yarin.gal@eng.cam.ac.uk,Concrete Dropout
neurips,2017,1,2018,Jiri,Hron,cam,University of Cambridge,jh2084@cam.ac.uk,Concrete Dropout
neurips,2017,2,2018,Alex,Kendall,cam,University of Cambridge,agk34@cam.ac.uk,Concrete Dropout
neurips,2017,0,2082,Yi,Ding,uchicago,University of Chicago,dingy@uchicago.edu,Multiresolution Kernel Approximation for Gaussian Process Regression
neurips,2017,1,2082,Risi,Kondor,uchicago,The University of Chicago,risi@uchicago.edu,Multiresolution Kernel Approximation for Gaussian Process Regression
neurips,2017,2,2082,Jonathan,Eskreis-Winkler,uchicago,University of Chicago,eskreiswinkler@uchicago.edu,Multiresolution Kernel Approximation for Gaussian Process Regression
neurips,2017,0,1730,Yasin,Abbasi Yadkori,,Adobe Research,,Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem
neurips,2017,1,1730,Peter,Bartlett,,UC Berkeley,,Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem
neurips,2017,2,1730,Victor,Gabillon,,QUT - ACEMS,,Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem
neurips,2017,0,1119,Chris,Metzler,rice,Rice University,chris.metzler@rice.edu,Learned D-AMP: Principled Neural Network based Compressive Image Recovery
neurips,2017,1,1119,Ali,Mousavi,rice,Rice University,ali.mousavi@rice.edu,Learned D-AMP: Principled Neural Network based Compressive Image Recovery
neurips,2017,2,1119,Richard,Baraniuk,rice,Rice University,richb@rice.edu,Learned D-AMP: Principled Neural Network based Compressive Image Recovery
neurips,2017,0,3449,Tatsunori,Hashimoto,stanford,Massachusetts Institute of Technology,thashim@cs.stanford.edu,Unsupervised Transformation Learning via Convex Relaxations
neurips,2017,1,3449,Percy,Liang,stanford,Stanford University,jduchi@cs.stanford.edu,Unsupervised Transformation Learning via Convex Relaxations
neurips,2017,2,3449,John,Duchi,stanford,Stanford,pliang@cs.stanford.edu,Unsupervised Transformation Learning via Convex Relaxations
neurips,2017,0,773,Eirikur,Agustsson,ethz,ETH Zurich,aeirikur@vision.ee.ethz.ch,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations
neurips,2017,1,773,Fabian,Mentzer,ethz,ETH Zurich,mentzerf@vision.ee.ethz.ch,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations
neurips,2017,2,773,Michael,Tschannen,ethz,ETH Zurich,michaelt@nari.ee.ethz.ch,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations
neurips,2017,3,773,Lukas,Cavigelli,ethz,ETH Zurich,cavigelli@iis.ee.ethz.ch,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations
neurips,2017,4,773,Radu,Timofte,ethz,ETH Zurich,timofter@vision.ee.ethz.ch,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations
neurips,2017,5,773,Luca,Benini,ethz,ETH Zurich,benini@iis.ee.ethz.ch,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations
neurips,2017,6,773,Luc,Gool,ethz,"Computer Vision Lab, ETH Zurich",vangool@vision.ee.ethz.ch,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations
neurips,2017,0,1486,Katrina,Ligett,,Hebrew University,,Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM
neurips,2017,1,1486,Seth,Neel,,University of Pennsylvania,,Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM
neurips,2017,2,1486,Aaron,Roth,,University of Pennsylvania,,Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM
neurips,2017,3,1486,Bo,Waggoner,,,,Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM
neurips,2017,4,1486,Steven,Wu,,Microsoft,,Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM
neurips,2017,0,2162,Chongxuan,LI,tsinghua,Tsinghua University,licx14@mails.tsinghua.edu.cn,Triple Generative Adversarial Nets
neurips,2017,1,2162,Taufik,Xu,tsinghua,Tsinghua University,xu-k16@mails.tsinghua.edu.cn,Triple Generative Adversarial Nets
neurips,2017,2,2162,Jun,Zhu,tsinghua,Tsinghua University,dcszj@mail.tsinghua.edu.cn,Triple Generative Adversarial Nets
neurips,2017,3,2162,Bo,Zhang,tsinghua,Tsinghua University,dcszb@mail.tsinghua.edu.cn,Triple Generative Adversarial Nets
neurips,2017,0,1027,Christoph,Hofer,sbg,University of Salzburg,chofer@cosy.sbg.ac.at,Deep Learning with Topological Signatures
neurips,2017,1,1027,Roland,Kwitt,sbg,University of Salzburg,Roland.Kwitt@sbg.ac.at,Deep Learning with Topological Signatures
neurips,2017,2,1027,Marc,Niethammer,unc,UNC Chapel Hill,mn@cs.unc.edu,Deep Learning with Topological Signatures
neurips,2017,3,1027,Andreas,Uhl,sbg,University of Salzburg,uhl@cosy.sbg.ac.at,Deep Learning with Topological Signatures
neurips,2017,0,1160,Andres,Munoz,,,,Revenue Optimization with Approximate Bid Predictions
neurips,2017,1,1160,Sergei,Vassilvitskii,,Google,,Revenue Optimization with Approximate Bid Predictions
neurips,2017,0,2165,Mali,Sundaresan,gmail,"Indian Institute of Science, Bangalore",s.malisundar@gmail.com,Mapping distinct timescales of functional interactions among brain networks
neurips,2017,1,2165,Arshed,Nabeel,iisc,"Indian Institute of Science, Bangalore",arshed@iisc.ac.in,Mapping distinct timescales of functional interactions among brain networks
neurips,2017,2,2165,Devarajan,Sridharan,iisc,"Indian Institute of Science, Bangalore",sridhar@iisc.ac.in,Mapping distinct timescales of functional interactions among brain networks
neurips,2017,0,2945,Ishaan,Gulrajani,umontreal,Google,faruk.ahmed@umontreal.ca,Improved Training of Wasserstein GANs
neurips,2017,1,2945,Faruk,Ahmed,umontreal,MILA,vincent.dumoulin@umontreal.ca,Improved Training of Wasserstein GANs
neurips,2017,2,2945,Martin,Arjovsky,umontreal,New York University,aaron.courville@umontreal.ca,Improved Training of Wasserstein GANs
neurips,2017,3,2945,Vincent,Dumoulin,nyu,Université de Montréal,ma4371@nyu.edu,Improved Training of Wasserstein GANs
neurips,2017,4,2945,Aaron,Courville,gmail,U. Montreal,igul222@gmail.com,Improved Training of Wasserstein GANs
neurips,2017,0,901,Benjamin,Cowley,cmu,Carnegie Mellon University,bcowley@cs.cmu.edu,Adaptive stimulus selection for optimizing neural population responses
neurips,2017,1,901,Ryan,Williamson,pitt,Carnegie Mellon University,rcw30@pitt.edu,Adaptive stimulus selection for optimizing neural population responses
neurips,2017,2,901,Katerina,Clemens,pitt,University of Pittsburgh,kac216@pitt.edu,Adaptive stimulus selection for optimizing neural population responses
neurips,2017,3,901,Matthew,Smith,pitt,University of Pittsburgh,smithma@pitt.edu,Adaptive stimulus selection for optimizing neural population responses
neurips,2017,4,901,Byron,Yu,cmu,Carnegie Mellon University,byronyu@cmu.edu,Adaptive stimulus selection for optimizing neural population responses
neurips,2017,0,3214,Ashish,Khetan,,University of Illinois Urbana Champaign,,Matrix Norm Estimation from a Few Entries
neurips,2017,1,3214,Sewoong,Oh,,UIUC,,Matrix Norm Estimation from a Few Entries
neurips,2017,0,325,Simon,Du,cmu,Carnegie Mellon University,ssdu@cs.cmu.edu,On the Power of Truncated SVD for General High-rank Matrix Estimation Problems
neurips,2017,1,325,Yining,Wang,cmu,Carnegie Mellon University,yiningwa@cs.cmu.edu,On the Power of Truncated SVD for General High-rank Matrix Estimation Problems
neurips,2017,2,325,Aarti,Singh,cmu,CMU,aartisingh@cmu.edu,On the Power of Truncated SVD for General High-rank Matrix Estimation Problems
neurips,2017,0,970,Wei,Wen,duke,Duke University,1wei.wen@duke.edu,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning
neurips,2017,1,970,Cong,Xu,duke,Hewlett Packard Labs,chunpeng.wu@duke.edu,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning
neurips,2017,2,970,Feng,Yan,duke,"University of Nevada, Reno",yiran.chen@duke.edu,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning
neurips,2017,3,970,Chunpeng,Wu,duke,Duke University,hai.li@duke.edu,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning
neurips,2017,4,970,Yandan,Wang,hpe,University of Pittsburgh,2cong.xu@hpe.com,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning
neurips,2017,5,970,Yiran,Chen,unr,Duke University,3fyan@unr.edu,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning
neurips,2017,6,970,Hai,Li,pitt,Duke University,4yaw46@pitt.edu,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning
neurips,2017,0,3319,Martin,Heusel,jku,LIT AI Lab / University Linz,mhe@bioinf.jku.at,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium
neurips,2017,1,3319,Hubert,Ramsauer,jku,LIT AI Lab / University Linz,ramsauer@bioinf.jku.at,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium
neurips,2017,2,3319,Thomas,Unterthiner,jku,LIT AI Lab / University Linz,unterthiner@bioinf.jku.at,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium
neurips,2017,3,3319,Bernhard,Nessler,jku,Johannes Kepler University Linz,nessler@bioinf.jku.at,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium
neurips,2017,4,3319,Sepp,Hochreiter,jku,LIT AI Lab / University Linz,hochreit@bioinf.jku.at,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium
neurips,2017,0,2493,Scott,Lundberg,washington,University of Washington,slund1@cs.washington.edu,A Unified Approach to Interpreting Model Predictions
neurips,2017,1,2493,Su-In,Lee,washington,University of Washington,suinlee@cs.washington.edu,A Unified Approach to Interpreting Model Predictions
neurips,2017,0,913,Emmanuel,Abbe,princeton,Princeton University,eabbe@princeton.edu,Nonbacktracking Bounds on the Influence in Independent Cascade Models
neurips,2017,1,913,Sanjeev,Kulkarni,princeton,Princeton University,kulkarni@princeton.edu,Nonbacktracking Bounds on the Influence in Independent Cascade Models
neurips,2017,2,913,Eun Jee,Lee,princeton,Princeton University,ejlee@princeton.edu,Nonbacktracking Bounds on the Influence in Independent Cascade Models
neurips,2017,0,3135,Zeyuan,Allen-Zhu,mit,Microsoft Research,zeyuan@csail.mit.edu,Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls
neurips,2017,1,3135,Elad,Hazan,princeton,Princeton University,huwei@cs.princeton.edu,Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls
neurips,2017,2,3135,Wei,Hu,princeton,Princeton University,ehazan@cs.princeton.edu,Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls
neurips,2017,3,3135,Yuanzhi,Li,princeton,Princeton University,yuanzhil@cs.princeton.edu,Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls
neurips,2017,0,1706,Roel,Dobbe,berkeley,UC Berkeley,dobbe@eecs.berkeley.edu,Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach
neurips,2017,1,1706,David,Fridovich-Keil,berkeley,UC Berkeley,dfk@eecs.berkeley.edu,Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach
neurips,2017,2,1706,Claire,Tomlin,berkeley,UC Berkeley,tomlin@eecs.berkeley.edu,Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach
neurips,2017,0,1986,David,Klindt,gmail,University of Tübingen,klindt.david@gmail.com,Neural system identification for large populations separating “what” and “where”
neurips,2017,1,1986,Alexander,Ecker,uni-tuebingen,University of Tuebingen,alexander.ecker@uni-tuebingen.de,Neural system identification for large populations separating “what” and “where”
neurips,2017,2,1986,Thomas,Euler,uni-tuebingen,University of Tübingen,thomas.euler@cin.uni-tuebingen.de,Neural system identification for large populations separating “what” and “where”
neurips,2017,3,1986,Matthias,Bethge,bethgelab,"CIN, University Tübingen",matthias.bethge@bethgelab.org,Neural system identification for large populations separating “what” and “where”
neurips,2017,0,2224,Ksenia,Konyushkova,epfl,EPFL,ksenia.konyushkova@epfl.ch,Learning Active Learning from Data
neurips,2017,1,2224,Raphael,Sznitman,unibe,University of Bern,raphael.sznitman@artorg.unibe.ch,Learning Active Learning from Data
neurips,2017,2,2224,Pascal,Fua,epfl,,pascal.fua@epfl.ch,Learning Active Learning from Data
neurips,2017,0,406,Qizhe,Xie,cmu,Carnegie Mellon University,qizhex@cs.cmu.edu,Controllable Invariance through Adversarial Feature Learning
neurips,2017,1,406,Zihang,Dai,cmu,Carnegie Mellon University,dzihang@cs.cmu.edu,Controllable Invariance through Adversarial Feature Learning
neurips,2017,2,406,Yulun,Du,cmu,Carnegie Mellon University,yulund@cs.cmu.edu,Controllable Invariance through Adversarial Feature Learning
neurips,2017,3,406,Eduard,Hovy,cmu,CMU,hovy@cs.cmu.edu,Controllable Invariance through Adversarial Feature Learning
neurips,2017,4,406,Graham,Neubig,cmu,Carnegie Mellon University,gneubig@cs.cmu.edu,Controllable Invariance through Adversarial Feature Learning
neurips,2017,0,2374,Nicholas,Watters,,Google DeepMind,,Visual Interaction Networks: Learning a Physics Simulator from Video
neurips,2017,1,2374,Daniel,Zoran,,DeepMind,,Visual Interaction Networks: Learning a Physics Simulator from Video
neurips,2017,2,2374,Theophane,Weber,,DeepMind,,Visual Interaction Networks: Learning a Physics Simulator from Video
neurips,2017,3,2374,Peter,Battaglia,,DeepMind,,Visual Interaction Networks: Learning a Physics Simulator from Video
neurips,2017,4,2374,Razvan,Pascanu,,Google DeepMind,,Visual Interaction Networks: Learning a Physics Simulator from Video
neurips,2017,5,2374,Andrea,Tacchetti,,MIT,,Visual Interaction Networks: Learning a Physics Simulator from Video
neurips,2017,0,1142,Kareem,Amin,google,Google Research,kamin@google.com,Repeated Inverse Reinforcement Learning
neurips,2017,1,1142,Nan,Jiang,umich,Microsoft Research,nanjiang@umich.edu,Repeated Inverse Reinforcement Learning
neurips,2017,2,1142,Satinder,Singh,umich,University of Michigan,baveja@umich.edu,Repeated Inverse Reinforcement Learning
neurips,2017,0,319,Murat,Erdogdu,toronto,Microsoft Research,erdogdu@cs.toronto.edu,Inference in Graphical Models via Semidefinite Programming Hierarchies
neurips,2017,1,319,Yash,Deshpande,mit,MIT,yash@mit.edu,Inference in Graphical Models via Semidefinite Programming Hierarchies
neurips,2017,2,319,Andrea,Montanari,stanford,Stanford,montanari@stanford.edu,Inference in Graphical Models via Semidefinite Programming Hierarchies
neurips,2017,0,1647,Sung-Soo,Ahn,kaist,KAIST,sungsoo.ahn@kaist.ac.kr,Gauging Variational Inference
neurips,2017,1,1647,Michael,Chertkov,kaist,Los Alamos National Laboratory,jinwoos@kaist.ac.kr,Gauging Variational Inference
neurips,2017,2,1647,Jinwoo,Shin,lanl,KAIST,chertkov@lanl.gov,Gauging Variational Inference
neurips,2017,0,2623,huan,ling,toronto,university of toronto,linghuan@cs.toronto.edu,Teaching Machines to Describe Images with Natural Language Feedback
neurips,2017,1,2623,Sanja,Fidler,toronto,University of Toronto,fidler@cs.toronto.edu,Teaching Machines to Describe Images with Natural Language Feedback
neurips,2017,0,1334,Alejandro,Newell,umich,University of Michigan,alnewell@umich.edu,Associative Embedding: End-to-End Learning for Joint Detection and Grouping
neurips,2017,1,1334,Zhiao,Huang,tsinghua,"IIIS, Tsinghua University",hza14@mails.tsinghua.edu.cn,Associative Embedding: End-to-End Learning for Joint Detection and Grouping
neurips,2017,2,1334,Jia,Deng,umich,University of Michigan,jiadeng@umich.edu,Associative Embedding: End-to-End Learning for Joint Detection and Grouping
neurips,2017,0,1443,Linus,Hamilton,,MIT,,"Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications"
neurips,2017,1,1443,Frederic,Koehler,,MIT,,"Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications"
neurips,2017,2,1443,Ankur,Moitra,,,,"Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications"
neurips,2017,0,675,Ehsan,Elhamifar,neu,Northeastern University,eelhami@ccs.neu.edu,Subset Selection and Summarization in Sequential Data
neurips,2017,1,675,M. Clara,De Paolis Kaluza,neu,Northeastern University,clara@ccs.neu.edu,Subset Selection and Summarization in Sequential Data
neurips,2017,0,3370,Anirudh Goyal,ALIAS PARTH GOYAL,,Université de Montréal,,Z-Forcing: Training Stochastic Recurrent Networks
neurips,2017,1,3370,Alessandro,Sordoni,,Microsoft Maluuba,,Z-Forcing: Training Stochastic Recurrent Networks
neurips,2017,2,3370,Marc-Alexandre,Côté,,Microsoft Research,,Z-Forcing: Training Stochastic Recurrent Networks
neurips,2017,3,3370,Nan Rosemary,Ke,,"MILA, École Polytechnique de Montréal",,Z-Forcing: Training Stochastic Recurrent Networks
neurips,2017,4,3370,Yoshua,Bengio,,U. Montreal,,Z-Forcing: Training Stochastic Recurrent Networks
neurips,2017,0,1796,Ronan,Fruit,inria,Inria Lille,ronan.fruit@inria.fr,Regret Minimization in MDPs with Options without Prior Knowledge
neurips,2017,1,1796,Matteo,Pirotta,inria,INRIA Lille-Nord Europe,matteo.pirotta@inria.fr,Regret Minimization in MDPs with Options without Prior Knowledge
neurips,2017,2,1796,Alessandro,Lazaric,inria,INRIA Lille-Nord Europe,alessandro.lazaric@inria.fr,Regret Minimization in MDPs with Options without Prior Knowledge
neurips,2017,3,1796,Emma,Brunskill,stanford,CMU,ebrun@cs.stanford.edu,Regret Minimization in MDPs with Options without Prior Knowledge
neurips,2017,0,3226,Asish,Ghoshal,purdue,Purdue University,aghoshal@purdue.edu,Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity
neurips,2017,1,3226,Jean,Honorio,purdue,Purdue University,jhonorio@purdue.edu,Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity
neurips,2017,0,3001,Arthur,Mensch,m4x,Inria Parietal,arthur.mensch@m4x.org,Learning Neural Representations of Human Cognition across Many fMRI Studies
neurips,2017,1,3001,Julien,Mairal,inria,Inria,julien.mairal@inria.fr,Learning Neural Representations of Human Cognition across Many fMRI Studies
neurips,2017,2,3001,Danilo,Bzdok,rwth-aachen,RWTH Aachen University,danilo.bzdok@rwth-aachen.de,Learning Neural Representations of Human Cognition across Many fMRI Studies
neurips,2017,3,3001,Bertrand,Thirion,inria,INRIA,bertrand.thirion@inria.fr,Learning Neural Representations of Human Cognition across Many fMRI Studies
neurips,2017,4,3001,Gael,Varoquaux,inria,"Parietal Team, INRIA",gael.varoquaux@inria.fr,Learning Neural Representations of Human Cognition across Many fMRI Studies
neurips,2017,0,2107,Mikhail,Yurochkin,umich,University of Michigan,moonfolk@umich.edu,Conic Scan-and-Cover algorithms for nonparametric topic modeling
neurips,2017,1,2107,Aritra,Guha,umich,University of Michigan,aritra@umich.edu,Conic Scan-and-Cover algorithms for nonparametric topic modeling
neurips,2017,2,2107,XuanLong,Nguyen,umich,University of Michigan,xuanlong@umich.edu,Conic Scan-and-Cover algorithms for nonparametric topic modeling
neurips,2017,0,2546,Yingxiang,Yang,,UIUC,,Online Learning for Multivariate Hawkes Processes
neurips,2017,1,2546,Jalal,Etesami,,UIUC,,Online Learning for Multivariate Hawkes Processes
neurips,2017,2,2546,Niao,He,,UIUC,,Online Learning for Multivariate Hawkes Processes
neurips,2017,3,2546,Negar,Kiyavash,,UIUC,,Online Learning for Multivariate Hawkes Processes
neurips,2017,0,1562,Maximilian,Alber,,TU Berlin,,An Empirical Study on The Properties of Random Bases for Kernel Methods
neurips,2017,1,1562,Pieter-Jan,Kindermans,,Google AI Resident,,An Empirical Study on The Properties of Random Bases for Kernel Methods
neurips,2017,2,1562,Kristof,Schütt,,TU Berlin,,An Empirical Study on The Properties of Random Bases for Kernel Methods
neurips,2017,3,1562,Klaus-Robert,Müller,,TU Berlin,,An Empirical Study on The Properties of Random Bases for Kernel Methods
neurips,2017,4,1562,Fei,Sha,,University of Southern California (USC),,An Empirical Study on The Properties of Random Bases for Kernel Methods
neurips,2017,0,994,Aryeh,Kontorovich,bgu,Ben Gurion University,karyeh@cs.bgu.ac.il,"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions"
neurips,2017,1,994,Sivan,Sabato,bgu,Ben Gurion University,sabatos@bgu.ac.il,"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions"
neurips,2017,2,994,Roi,Weiss,weizmann,Weizmann institute of science,roiw@weizmann.ac.il,"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions"
neurips,2017,0,3219,Christos,Louizos,uva,University of Amsterdam,c.louizos@uva.nl,Causal Effect Inference with Deep Latent-Variable Models
neurips,2017,1,3219,Uri,Shalit,nyu,,uas1@nyu.edu,Causal Effect Inference with Deep Latent-Variable Models
neurips,2017,2,3219,Joris,Mooij,uva,University of Amsterdam,j.m.mooij@uva.nl,Causal Effect Inference with Deep Latent-Variable Models
neurips,2017,3,3219,David,Sontag,mit,MIT,dsontag@mit.edu,Causal Effect Inference with Deep Latent-Variable Models
neurips,2017,4,3219,Richard,Zemel,toronto,University of Toronto,zemel@cs.toronto.edu,Causal Effect Inference with Deep Latent-Variable Models
neurips,2017,5,3219,Max,Welling,uva,University of Amsterdam and University of California Irvine and CIFAR,m.welling@uva.nl,Causal Effect Inference with Deep Latent-Variable Models
neurips,2017,0,2274,Emmanouil,Platanios,cmu,Carnegie Mellon University,e.a.platanios@cs.cmu.edu,Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach
neurips,2017,1,2274,Hoifung,Poon,cmu,Microsoft Research,tom.mitchell@cs.cmu.edu,Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach
neurips,2017,2,2274,Tom,Mitchell,microsoft,Carnegie Mellon University,hoifung@microsoft.com,Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach
neurips,2017,3,2274,Eric,Horvitz,microsoft,Microsoft Research,horvitz@microsoft.com,Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach
neurips,2017,0,2279,Miro,Dudik,microsoft,Microsoft Research,mdudik@microsoft.com,A Decomposition of Forecast Error in Prediction Markets
neurips,2017,1,2279,Sebastien,Lahaie,gmail,Google,rrogers386@gmail.com,A Decomposition of Forecast Error in Prediction Markets
neurips,2017,2,2279,Ryan,Rogers,google,University of Pennsylvania,slahaie@google.com,A Decomposition of Forecast Error in Prediction Markets
neurips,2017,3,2279,Jennifer,Wortman Vaughan,microsoft,Microsoft Research,jenn@microsoft.com,A Decomposition of Forecast Error in Prediction Markets
neurips,2017,0,2407,Stéphan,Clémençon,,Telecom ParisTech,,Ranking Data with Continuous Labels through Oriented Recursive Partitions
neurips,2017,1,2407,Mastane,Achab,,Télécom ParisTech,,Ranking Data with Continuous Labels through Oriented Recursive Partitions
neurips,2017,0,3171,Kun,Dong,,Cornell University,,Scalable Log Determinants for Gaussian Process Kernel Learning
neurips,2017,1,3171,David,Eriksson,,Cornell University,,Scalable Log Determinants for Gaussian Process Kernel Learning
neurips,2017,2,3171,Hannes,Nickisch,,Philips Research,,Scalable Log Determinants for Gaussian Process Kernel Learning
neurips,2017,3,3171,David,Bindel,,Cornell University,,Scalable Log Determinants for Gaussian Process Kernel Learning
neurips,2017,4,3171,Andrew,Wilson,,Cornell University,,Scalable Log Determinants for Gaussian Process Kernel Learning
neurips,2017,0,2591,Flavio,Chierichetti,,Sapienza University,,Fair Clustering Through Fairlets
neurips,2017,1,2591,Ravi,Kumar,,Google,,Fair Clustering Through Fairlets
neurips,2017,2,2591,Silvio,Lattanzi,,Google,,Fair Clustering Through Fairlets
neurips,2017,3,2591,Sergei,Vassilvitskii,,Google,,Fair Clustering Through Fairlets
neurips,2017,0,213,Wittawat,Jitkrittum,gmail,"Gatsby unit, University College London",wittawatj@gmail.com,A Linear-Time Kernel Goodness-of-Fit Test
neurips,2017,1,213,Wenkai,Xu,ucl,"Gatsby Unit, UCL",wenkaix@gatsby.ucl.ac.uk,A Linear-Time Kernel Goodness-of-Fit Test
neurips,2017,2,213,Zoltan,Szabo,polytechnique,Ecole Polytechnique,zoltan.szabo@polytechnique.edu,A Linear-Time Kernel Goodness-of-Fit Test
neurips,2017,3,213,Kenji,Fukumizu,ism,Institute of Statistical Mathematics,fukumizu@ism.ac.jp,A Linear-Time Kernel Goodness-of-Fit Test
neurips,2017,4,213,Arthur,Gretton,gmail,"Gatsby Unit, UCL",arthur.gretton@gmail.com,A Linear-Time Kernel Goodness-of-Fit Test
neurips,2017,0,1745,Nir,Levine,gmail,Technion - Israel Institute of Technology,levin.nir1@gmail.com,Rotting Bandits
neurips,2017,1,1745,Koby,Crammer,technion,Technion,koby@ee.technion.ac.il,Rotting Bandits
neurips,2017,2,1745,Shie,Mannor,technion,Technion,shie@ee.technion.ac.il,Rotting Bandits
neurips,2017,0,3164,Ga,Wu,utoronto,University of Toronto,wuga@mie.utoronto.ca,Scalable Planning with Tensorflow for Hybrid Nonlinear Domains
neurips,2017,1,3164,Buser,Say,utoronto,University of Toronto,bsay@mie.utoronto.ca,Scalable Planning with Tensorflow for Hybrid Nonlinear Domains
neurips,2017,2,3164,Scott,Sanner,utoronto,University of Toronto,ssanner@mie.utoronto.ca,Scalable Planning with Tensorflow for Hybrid Nonlinear Domains
neurips,2017,0,93,Chris,Oates,,Newcastle University,,Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models
neurips,2017,1,93,Steven,Niederer,,Kings College London,,Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models
neurips,2017,2,93,Angela,Lee,,King's College London,,Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models
neurips,2017,3,93,François-Xavier,Briol,,University of Warwick,,Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models
neurips,2017,4,93,Mark,Girolami,,Imperial College London,,Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models
neurips,2017,0,1279,Julien,Audiffren,gmail,"Exascale Infolab, Fribourg University",julien.audiffren@gmail.com,Bandits Dueling on Partially Ordered Sets
neurips,2017,1,1279,Liva,Ralaivola,univ-mrs,"LIF, IUF, Aix-Marseille University, CNRS",liva.ralaivola@lif.univ-mrs.fr,Bandits Dueling on Partially Ordered Sets
neurips,2017,0,1531,Mohammad Ali,Bashiri,uic,University of Illinois at Chicago,mbashi4@uic.edu,Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search
neurips,2017,1,1531,Xinhua,Zhang,uic,University of Illinois at Chicago (UIC),zhangx@uic.edu,Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search
neurips,2017,0,567,Daniel,Milstein,brown,Brown University,daniel_milstein@alumni.brown.edu,Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces
neurips,2017,1,567,Jason,Pacheco,mit,Brown University,pachecoj@mit.edu,Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces
neurips,2017,2,567,Leigh,Hochberg,brown,"Brown, MGH, VA, Harvard",leigh_hochberg@brown.edu,Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces
neurips,2017,3,567,John,Simeral,brown,Brown University,john_simeral@brown.edu,Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces
neurips,2017,4,567,Beata,Jarosiewicz,stanford,Stanford University,beataj@stanford.edu,Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces
neurips,2017,5,567,Erik,Sudderth,uci,"University of California, Irvine",sudderth@uci.edu,Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces
neurips,2017,0,1417,Jeffrey,Regier,berkeley,UC Berkeley,jregier@cs.berkeley.edu,Fast Black-box Variational Inference through Stochastic Trust-Region Optimization
neurips,2017,1,1417,Michael,Jordan,berkeley,UC Berkeley,jordan@cs.berkeley.edu,Fast Black-box Variational Inference through Stochastic Trust-Region Optimization
neurips,2017,2,1417,Jon,McAuliffe,berkeley,UC Berkeley,jon@stat.berkeley.edu,Fast Black-box Variational Inference through Stochastic Trust-Region Optimization
neurips,2017,0,1193,Lixin,Fan,nokia,Nokia Technologies,lixin.fan@nokia.com,Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network
neurips,2017,0,2135,Flavio,Calmon,harvard,Harvard University,flavio@seas.harvard.edu,Optimized Pre-Processing for Discrimination Prevention
neurips,2017,1,2135,Dennis,Wei,ibm,IBM Research,dwei@us.ibm.com,Optimized Pre-Processing for Discrimination Prevention
neurips,2017,2,2135,Bhanukiran,Vinzamuri,ibm,IBM Research,bhanu.vinzamuri@ibm.com,Optimized Pre-Processing for Discrimination Prevention
neurips,2017,3,2135,Karthikeyan,Natesan Ramamurthy,ibm,IBM Research,knatesa@us.ibm.com,Optimized Pre-Processing for Discrimination Prevention
neurips,2017,4,2135,Kush,Varshney,ibm,IBM Research,krvarshn@us.ibm.com,Optimized Pre-Processing for Discrimination Prevention
neurips,2017,0,1421,Jinfeng,Yi,gmail,Tencent AI Lab/IBM TJ Watson Research Center,jinfengyi.ustc@gmail.com,Scalable Demand-Aware Recommendation
neurips,2017,1,1421,Cho-Jui,Hsieh,ucdavis,UC Davis,chohsieh@ucdavis.edu,Scalable Demand-Aware Recommendation
neurips,2017,2,1421,Kush,Varshney,ibm,IBM Research,krvarshn@us.ibm.com,Scalable Demand-Aware Recommendation
neurips,2017,3,1421,Lijun,Zhang,nju,Nanjing University (NJU),zhanglj@lamda.nju.edu.cn,Scalable Demand-Aware Recommendation
neurips,2017,4,1421,Yao,Li,ucdavis,"University of California, Davis",yaoli@ucdavis.edu,Scalable Demand-Aware Recommendation
neurips,2017,0,291,Abhishek,Kar,berkeley,UC Berkeley,akar@berkeley.edu,Learning a Multi-View Stereo Machine
neurips,2017,1,291,Christian,Häne,berkeley,UC Berkeley,chaene@berkeley.edu,Learning a Multi-View Stereo Machine
neurips,2017,2,291,Jitendra,Malik,berkeley,,malik@berkeley.edu,Learning a Multi-View Stereo Machine
neurips,2017,0,3273,Krzysztof,Choromanski,google,Google Brain Robotics,kchoro@google.com,On Blackbox Backpropagation and Jacobian Sensing
neurips,2017,1,3273,Vikas,Sindhwani,google,,sindhwani@google.com,On Blackbox Backpropagation and Jacobian Sensing
neurips,2017,0,3021,Siddharth,N,ox,University of Oxford,nsid@robots.ox.ac.uk,Learning Disentangled Representations with Semi-Supervised Deep Generative Models
neurips,2017,1,3021,Brooks,Paige,turing,Alan Turing Institute,bpaige@turing.ac.uk,Learning Disentangled Representations with Semi-Supervised Deep Generative Models
neurips,2017,2,3021,Jan-Willem,van de Meent,northeastern,Northeastern University,j.vandemeent@northeastern.edu,Learning Disentangled Representations with Semi-Supervised Deep Generative Models
neurips,2017,3,3021,Alban,Desmaison,ox,Oxford University,alban@robots.ox.ac.uk,Learning Disentangled Representations with Semi-Supervised Deep Generative Models
neurips,2017,4,3021,Noah,Goodman,stanford,Stanford University,ngoodman@stanford.edu,Learning Disentangled Representations with Semi-Supervised Deep Generative Models
neurips,2017,5,3021,Pushmeet,Kohli,google,Microsoft Research,pushmeet@google.com,Learning Disentangled Representations with Semi-Supervised Deep Generative Models
neurips,2017,6,3021,Frank,Wood,ox,University of Oxford,fwood@robots.ox.ac.uk,Learning Disentangled Representations with Semi-Supervised Deep Generative Models
neurips,2017,7,3021,Philip,Torr,ox,University of Oxford,philip.torr@eng.ox.ac.uk,Learning Disentangled Representations with Semi-Supervised Deep Generative Models
neurips,2017,0,604,Luca,Ambrogioni,donders,Donders Institute,l.ambrogioni@donders.ru.nl,GP CaKe: Effective brain connectivity with causal kernels
neurips,2017,1,604,Max,Hinne,donders,Radboud University,m.vangerven@donders.ru.nl,GP CaKe: Effective brain connectivity with causal kernels
neurips,2017,2,604,Marcel,Van Gerven,donders,Radboud University,m.hinne@donders.ru.nl,GP CaKe: Effective brain connectivity with causal kernels
neurips,2017,3,604,Eric,Maris,donders,Donders Institute,e.maris@donders.ru.nl,GP CaKe: Effective brain connectivity with causal kernels
neurips,2017,0,1988,Jacob,Steinhardt,stanford,Stanford University,jsteinha@stanford.edu,Certified Defenses for Data Poisoning Attacks
neurips,2017,1,1988,Pang Wei,Koh,stanford,Stanford University,pangwei@cs.stanford.edu,Certified Defenses for Data Poisoning Attacks
neurips,2017,2,1988,Percy,Liang,stanford,Stanford University,pliang@cs.stanford.edu,Certified Defenses for Data Poisoning Attacks
neurips,2017,0,3286,Aravind,Rajeswaran,,University of Washington,,Towards Generalization and Simplicity in Continuous Control
neurips,2017,1,3286,Kendall,Lowrey,,University of Washington,,Towards Generalization and Simplicity in Continuous Control
neurips,2017,2,3286,Emanuel,Todorov,,University of Washington,,Towards Generalization and Simplicity in Continuous Control
neurips,2017,3,3286,Sham,Kakade,,University of Washington,,Towards Generalization and Simplicity in Continuous Control
neurips,2017,0,2906,Sébastien,Racanière,,Google DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,1,2906,Theophane,Weber,,DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,2,2906,David,Reichert,,DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,3,2906,Lars,Buesing,,DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,4,2906,Arthur,Guez,,Google,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,5,2906,Danilo,Jimenez Rezende,,Google DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,6,2906,Adrià,Puigdomènech Badia,,Google DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,7,2906,Oriol,Vinyals,,Google DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,8,2906,Nicolas,Heess,,Google DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,9,2906,Yujia,Li,,DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,10,2906,Razvan,Pascanu,,Google DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,11,2906,Peter,Battaglia,,DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,12,2906,Demis,Hassabis,,DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,13,2906,David,Silver,,DeepMind,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,14,2906,Daan,Wierstra,,DeepMind Technologies,,Imagination-Augmented Agents for Deep Reinforcement Learning
neurips,2017,0,3205,Balaji,Lakshminarayanan,google,Google Deepmind,balajiln@google.com,Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
neurips,2017,1,3205,Alexander,Pritzel,google,Google Deepmind,apritzel@google.com,Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
neurips,2017,2,3205,Charles,Blundell,google,DeepMind,cblundell@google.com,Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
neurips,2017,0,2143,Fabio,Cecchi,tue,Eindhoven University of Technology,f.cecchi@tue.nl,Adaptive Active Hypothesis Testing under Limited Information
neurips,2017,1,2143,Nidhi,Hegde,nokia-bell-labs,Nokia Bell Labs,nidhi.hegde@nokia-bell-labs.com,Adaptive Active Hypothesis Testing under Limited Information
neurips,2017,0,931,Xiangru,Huang,utexas,University of Texas at Austin,xrhuang@cs.utexas.edu,Translation Synchronization via Truncated Least Squares
neurips,2017,1,931,Zhenxiao,Liang,tsinghua,Tsinghua University,liangzx14@mails.tsinghua.edu.cn,Translation Synchronization via Truncated Least Squares
neurips,2017,2,931,Chandrajit,Bajaj,utexas,The University of Texas at Austin,bajaj@cs.utexas.edu,Translation Synchronization via Truncated Least Squares
neurips,2017,3,931,Qixing,Huang,utexas,The University of Texas at Austin,huangqx@cs.utexas.edu,Translation Synchronization via Truncated Least Squares
neurips,2017,0,2007,Yossi,Arjevani,weizmann,The Weizmann Institute,yossi.arjevani@weizmann.ac.il,Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums Optimization
neurips,2017,0,1099,Urs,Köster,,Intel Corporation,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,1,1099,Tristan,Webb,,Intel / Nervana,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,2,1099,Xin,Wang,,Intel Corporation,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,3,1099,Marcel,Nassar,,Intel Corporation,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,4,1099,Arjun,Bansal,,Intel Nervana,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,5,1099,William,Constable,,Intel,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,6,1099,Oguz,Elibol,,Intel Nervana,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,7,1099,Scott,Gray,,OpenAI,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,8,1099,Stewart,Hall,,Intel,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,9,1099,Luke,Hornof,,Intel Nervana,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,10,1099,Amir,Khosrowshahi,,Intel,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,11,1099,Carey,Kloss,,Intel,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,12,1099,Ruby,Pai,,Intel Corporation,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,13,1099,Naveen,Rao,,Intel,,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
neurips,2017,0,2097,Cameron,Musco,mit,Massachusetts Institute of Technology,cnmusco@mit.edu,Recursive Sampling for the Nystrom Method
neurips,2017,1,2097,Christopher,Musco,mit,Mass. Institute of Technology,cpmusco@mit.edu,Recursive Sampling for the Nystrom Method
neurips,2017,0,3086,Yuting,Wei,berkeley,"University of California, Berkeley",ytwei@berkeley.edu,Early stopping for kernel boosting algorithms: A general analysis with localized complexities
neurips,2017,1,3086,Fanny,Yang,berkeley,"University of California, Berkeley",fanny-yang@berkeley.edu,Early stopping for kernel boosting algorithms: A general analysis with localized complexities
neurips,2017,2,3086,Martin,Wainwright,berkeley,UC Berkeley,wainwrig@berkeley.edu,Early stopping for kernel boosting algorithms: A general analysis with localized complexities
neurips,2017,0,2098,Shixiang (Shane),Gu,cam,University of Cambridge and Max Planck Institute for Intelligent Systems,sg717@cam.ac.uk,Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning
neurips,2017,1,2098,Timothy,Lillicrap,google,Google DeepMind,countzero@google.com,Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning
neurips,2017,2,2098,Richard,Turner,cam,University of Cambridge,zoubin@eng.cam.ac.uk,Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning
neurips,2017,3,2098,Zoubin,Ghahramani,cam,Uber and University of Cambridge,ret26@cam.ac.uk,Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning
neurips,2017,4,2098,Bernhard,Schölkopf,mpg,MPI for Intelligent Systems,bs@tuebingen.mpg.de,Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning
neurips,2017,5,2098,Sergey,Levine,berkeley,UC Berkeley,svlevine@eecs.berkeley.edu,Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning
neurips,2017,0,3075,Dylan,Foster,,Cornell University,,Parameter-Free Online Learning via Model Selection
neurips,2017,1,3075,Satyen,Kale,,Google,,Parameter-Free Online Learning via Model Selection
neurips,2017,2,3075,Mehryar,Mohri,,Courant Institute and Google,,Parameter-Free Online Learning via Model Selection
neurips,2017,3,3075,Karthik,Sridharan,,Cornell University,,Parameter-Free Online Learning via Model Selection
neurips,2017,0,1035,Yichen,Wang,gatech,Georgia Tech,yichen.wang@gatech.edu,Predicting User Activity Level In Point Processes With Mass Transport Equation
neurips,2017,1,1035,Xiaojing,Ye,gsu,Georgia State University,xye@gsu.edu,Predicting User Activity Level In Point Processes With Mass Transport Equation
neurips,2017,2,1035,Hongyuan,Zha,gatech,Georgia Tech,zha@cc.gatech.edu,Predicting User Activity Level In Point Processes With Mass Transport Equation
neurips,2017,3,1035,Le,Song,gatech,Georgia Institute of Technology,lsong@cc.gatech.edu,Predicting User Activity Level In Point Processes With Mass Transport Equation
neurips,2017,0,2999,Eric,Balkanski,harvard,Harvard University,ericbalkanski@g.harvard.edu,The Importance of Communities for Learning to Influence
neurips,2017,1,2999,Nicole,Immorlica,microsoft,Microsoft Research,nicimm@microsoft.com,The Importance of Communities for Learning to Influence
neurips,2017,2,2999,Yaron,Singer,harvard,Harvard University,yaron@seas.harvard.edu,The Importance of Communities for Learning to Influence
neurips,2017,0,2923,John,Halloran,ucdavis,"University of California, Davis",jthalloran@ucdavis.edu,Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra
neurips,2017,1,2923,David,Rocke,ucdavis,"University of California, Davis",dmrocke@ucdavis.edu,Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra
neurips,2017,0,2039,Rong,Ge,duke,Duke University,rongge@cs.duke.edu,On the Optimization Landscape of Tensor Decompositions
neurips,2017,1,2039,Tengyu,Ma,stanford,Facebook AI Research,tengyuma@cs.stanford.edu,On the Optimization Landscape of Tensor Decompositions
neurips,2017,0,2152,Matt,Kusner,turing,Alan Turing Institute,crussell@turing.ac.uk,Counterfactual Fairness
neurips,2017,1,2152,Joshua,Loftus,turing,The Alan Turing Institute,mkusner@turing.ac.uk,Counterfactual Fairness
neurips,2017,2,2152,Chris,Russell,nyu,The Alan Turing Institute/ The University of Surrey,loftus@nyu.edu,Counterfactual Fairness
neurips,2017,3,2152,Ricardo,Silva,ucl,University College London,ricardo@stats.ucl.ac.uk,Counterfactual Fairness
neurips,2017,0,434,Dan,Garber,technion,Technion - Israel Institute of Technology,dangar@technion.ac.il,Efficient Online Linear Optimization with Approximation Algorithms
neurips,2017,0,1344,Pan,Li,illinois,University of Illinois Urbana-Champaign,panli2@illinois.edu,Inhomogeneous Hypergraph Clustering with Applications
neurips,2017,1,1344,Olgica,Milenkovic,illinois,University of Illinois at Urbana-Champaign,milenkov@illinois.edu,Inhomogeneous Hypergraph Clustering with Applications
neurips,2017,0,1298,Ji,Lin,tsinghua,Tsinghua University,lin-j14@mails.tsinghua.edu.cn,Runtime Neural Pruning
neurips,2017,1,1298,Yongming,Rao,gmail,Tsinghua University,raoyongming95@gmail.com,Runtime Neural Pruning
neurips,2017,2,1298,Jiwen,Lu,tsinghua,Tsinghua University,lujiwen@tsinghua.edu.cn,Runtime Neural Pruning
neurips,2017,3,1298,Jie,Zhou,tsinghua,Tsinghua University,jzhou@tsinghua.edu.cn,Runtime Neural Pruning
neurips,2017,0,1098,Elad,Hoffer,gmail,Technion,elad.hoffer@gmail.com,"Train longer, generalize better: closing the generalization gap in large batch training of neural networks"
neurips,2017,1,1098,Itay,Hubara,gmail,Technion,itayhubara@gmail.com,"Train longer, generalize better: closing the generalization gap in large batch training of neural networks"
neurips,2017,2,1098,Daniel,Soudry,gmail,Technion,daniel.soudry@gmail.com,"Train longer, generalize better: closing the generalization gap in large batch training of neural networks"
neurips,2017,0,2523,Emilie,Kaufmann,univ-lille1,CNRS & CRIStAL (SequeL),emilie.kaufmann@univ-lille1.fr,Monte-Carlo Tree Search by Best Arm Identification
neurips,2017,1,2523,Wouter,Koolen,cwi,"Centrum Wiskunde & Informatica, Amsterdam",wmkoolen@cwi.nl,Monte-Carlo Tree Search by Best Arm Identification
neurips,2017,0,2878,Xingjian,Shi,ust,HKUST,xshiab@cse.ust.hk,Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
neurips,2017,1,2878,Zhihan,Gao,ust,HKUST,zgaoag@cse.ust.hk,Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
neurips,2017,2,2878,Leonard,Lausen,ust,HKUST,lelausen@cse.ust.hk,Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
neurips,2017,3,2878,Hao,Wang,ust,HKUST,hwangaz@cse.ust.hk,Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
neurips,2017,4,2878,Dit-Yan,Yeung,ust,"HKUST, Hong Kong",dyyeung@cse.ust.hk,Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
neurips,2017,5,2878,Wai-kin,Wong,hko,HKO,wkwong@hko.gov.hk,Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
neurips,2017,6,2878,Wang-chun,WOO,hko,HKO,wcwoo@hko.gov.hk,Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
neurips,2017,0,2410,Zhao,Song,duke,Duke University,zhao.song@duke.edu,Scalable Model Selection for Belief Networks
neurips,2017,1,2410,Yusuke,Muraoka,duke,,lcarin@duke.edu,Scalable Model Selection for Belief Networks
neurips,2017,2,2410,Ryohei,Fujimaki,nec-labs,NEC Data Science Research Laboratories,ymuraoka@nec-labs.com,Scalable Model Selection for Belief Networks
neurips,2017,3,2410,Lawrence,Carin,nec-labs,Duke University,rfujimaki@nec-labs.com,Scalable Model Selection for Belief Networks
neurips,2017,0,3019,Zhanhong,Jiang,,Iowa State University,,Collaborative Deep Learning in Fixed Topology Networks
neurips,2017,1,3019,Aditya,Balu,,Iowa State University,,Collaborative Deep Learning in Fixed Topology Networks
neurips,2017,2,3019,Chinmay,Hegde,,Iowa State University,,Collaborative Deep Learning in Fixed Topology Networks
neurips,2017,3,3019,Soumik,Sarkar,,Iowa State University,,Collaborative Deep Learning in Fixed Topology Networks
neurips,2017,0,2840,Le,Song,gatech,Georgia Institute of Technology,lsong@cc.gatech.edu,On the Complexity of Learning Neural Networks
neurips,2017,1,2840,Santosh,Vempala,gatech,Georgia Tech,vempala@gatech.edu,On the Complexity of Learning Neural Networks
neurips,2017,2,2840,John,Wilmes,gatech,Georgia Institute of Technology,wilmesj@gatech.edu,On the Complexity of Learning Neural Networks
neurips,2017,3,2840,Bo,Xie,gatech,Georgia Tech,bo.xie@gatech.edu,On the Complexity of Learning Neural Networks
neurips,2017,0,2770,Vasilis,Syrgkanis,microsoft,Microsoft Research,vasy@microsoft.com,A Sample Complexity Measure with Applications to Learning Optimal Auctions
neurips,2017,0,1967,Ahmad,Beirami,harvard,Harvard University & MIT,beirami@seas.harvard.edu,On Optimal Generalizability in Parametric Learning
neurips,2017,1,1967,Meisam,Razaviyayn,usc,University of Southern California,razaviya@usc.edu,On Optimal Generalizability in Parametric Learning
neurips,2017,2,1967,Shahin,Shahrampour,harvard,Harvard University,shahin@seas.harvard.edu,On Optimal Generalizability in Parametric Learning
neurips,2017,3,1967,Vahid,Tarokh,harvard,Harvard University,vahid@seas.harvard.edu,On Optimal Generalizability in Parametric Learning
neurips,2017,0,2674,James,Newling,idiap,Idiap Research Institute & EPFL,james.newling@idiap.ch,K-Medoids For K-Means Seeding
neurips,2017,1,2674,François,Fleuret,idiap,Idiap Research Institute,francois.fleuret@idiap.ch,K-Medoids For K-Means Seeding
neurips,2017,0,2131,Dan,Xu,unitn,University of Trento,dan.xu@unitn.it,Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction
neurips,2017,1,2131,Wanli,Ouyang,sydney,The Chinese University of Hong Kong,wanli.ouyang@sydney.edu.au,Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction
neurips,2017,2,2131,Xavier,Alameda-Pineda,inria,INRIA,xavier.alameda-pineda@inria.fr,Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction
neurips,2017,3,2131,Elisa,Ricci,unipg,,elisa.ricci@unipg.it,Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction
neurips,2017,4,2131,Xiaogang,Wang,cuhk,The Chinese University of Hong Kong,xgwang@ee.cuhk.edu.hk,Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction
neurips,2017,5,2131,Nicu,Sebe,unitn,University of Trento,niculae.sebe@unitn.it,Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction
neurips,2017,0,440,Shixiang,Chen,,The Chinese University of HongKong,,Geometric Descent Method for Convex Composite Minimization
neurips,2017,1,440,Shiqian,Ma,,UC Davis,,Geometric Descent Method for Convex Composite Minimization
neurips,2017,2,440,Wei,Liu,,Tencent AI Lab,,Geometric Descent Method for Convex Composite Minimization
neurips,2017,0,142,Zelun,Luo,,Stanford University,,Label Efficient Learning of Transferable Representations acrosss Domains and Tasks
neurips,2017,1,142,Yuliang,Zou,,Virginia Tech,,Label Efficient Learning of Transferable Representations acrosss Domains and Tasks
neurips,2017,2,142,Judy,Hoffman,,UC Berkeley,,Label Efficient Learning of Transferable Representations acrosss Domains and Tasks
neurips,2017,3,142,Li,Fei-Fei,,Stanford University & Google,,Label Efficient Learning of Transferable Representations acrosss Domains and Tasks
neurips,2017,0,780,Qinshi,Wang,princeton,Princeton University,qinshiw@princeton.edu,Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications
neurips,2017,1,780,Wei,Chen,microsoft,Microsoft Research,weic@microsoft.com,Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications
neurips,2017,0,808,Nikolay,Savinov,ethz,ETH Zurich,nikolay.savinov@inf.ethz.ch,Matching neural paths: transfer from recognition to correspondence search
neurips,2017,1,808,Lubor,Ladicky,ethz,ETH Zurich,lubor.ladicky@inf.ethz.ch,Matching neural paths: transfer from recognition to correspondence search
neurips,2017,2,808,Marc,Pollefeys,ethz,ETH Zurich,marc.pollefeys@inf.ethz.ch,Matching neural paths: transfer from recognition to correspondence search
neurips,2017,0,409,Yuanzhi,Li,princeton,Princeton University,yuanzhil@cs.princeton.edu,Convergence Analysis of Two-layer Neural Networks with ReLU Activation
neurips,2017,1,409,Yang,Yuan,cornell,Cornell University,yangyuan@cs.cornell.edu,Convergence Analysis of Two-layer Neural Networks with ReLU Activation
neurips,2017,0,2051,Giuseppe,Pica,,Istituto Italiano di Tecnologia,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,1,2051,Eugenio,Piasini,,Istituto Italiano di Tecnologia,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,2,2051,Houman,Safaai,,Harvard Medical School,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,3,2051,Caroline,Runyan,,University of Pittsburgh,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,4,2051,Christopher,Harvey,,Harvard Medical School,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,5,2051,Mathew,Diamond,,International School for Advanced Studies,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,6,2051,Christoph,Kayser,,University of Glasgow,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,7,2051,Tommaso,Fellin,,Istituto Italiano di Tecnologia,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,8,2051,Stefano,Panzeri,,Istituto Italiano di Tecnologia,,Quantifying how much sensory information in a neural code is relevant for behavior
neurips,2017,0,2697,Hsiao-Yu,Tung,cmu,Carnegie Mellon University,htung@cs.cmu.edu,Self-supervised Learning of Motion Capture
neurips,2017,1,2697,Hsiao-Wei,Tung,cmu,University of Pittsburgh,katef@cs.cmu.edu,Self-supervised Learning of Motion Capture
neurips,2017,2,2697,Ersin,Yumer,pitt,Adobe Research,hst11@pitt.edu,Self-supervised Learning of Motion Capture
neurips,2017,3,2697,Katerina,Fragkiadaki,adobe,Carnegie Mellon University,yumer@adobe.com,Self-supervised Learning of Motion Capture
neurips,2017,0,1482,Chengxu,Zhuang,stanford,Stanford University,chengxuz@stanford.edu,Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System
neurips,2017,1,1482,Jonas,Kubilius,mit,Massachusetts Institute of Technology,qbilius@mit.edu,Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System
neurips,2017,2,1482,Mitra,Hartmann,stanford,Northwestern University,yamins@stanford.edu,Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System
neurips,2017,3,1482,Daniel,Yamins,northwestern,Stanford University,hartmann@northwestern.edu,Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System
neurips,2017,0,1910,Cyrus,Rashtchian,,University of Washington,,Clustering Billions of Reads for DNA Data Storage
neurips,2017,1,1910,Konstantin,Makarychev,,,,Clustering Billions of Reads for DNA Data Storage
neurips,2017,2,1910,Miklos,Racz,,Princeton University,,Clustering Billions of Reads for DNA Data Storage
neurips,2017,3,1910,Siena,Ang,,Microsoft,,Clustering Billions of Reads for DNA Data Storage
neurips,2017,4,1910,Djordje,Jevdjic,,Microsoft Research,,Clustering Billions of Reads for DNA Data Storage
neurips,2017,5,1910,Sergey,Yekhanin,,Microsoft,,Clustering Billions of Reads for DNA Data Storage
neurips,2017,6,1910,Luis,Ceze,,University of Washington,,Clustering Billions of Reads for DNA Data Storage
neurips,2017,7,1910,Karin,Strauss,,Microsoft Research,,Clustering Billions of Reads for DNA Data Storage
neurips,2017,0,1720,Marco,Cusumano-Towner,mit,Massachusetts Institute of Technology,marcoct@mit.edu,AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms
neurips,2017,1,1720,Vikash,Mansinghka,mit,Massachusetts Institute of Technology,vkm@mit.edu,AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms
neurips,2017,0,1462,Aolin,Xu,illinois,University of Illinois at Urbana-Champaign,aolinxu2@illinois.edu,Information-theoretic analysis of generalization capability of learning algorithms
neurips,2017,1,1462,Maxim,Raginsky,illinois,University of Illinois at Urbana-Champaign,maxim@illinois.edu,Information-theoretic analysis of generalization capability of learning algorithms
neurips,2017,0,374,Jiajun,Wu,,MIT,,MarrNet: 3D Shape Reconstruction via 2.5D Sketches
neurips,2017,1,374,Yifan,Wang,,ShanghaiTech University,,MarrNet: 3D Shape Reconstruction via 2.5D Sketches
neurips,2017,2,374,Tianfan,Xue,,MIT CSAIL,,MarrNet: 3D Shape Reconstruction via 2.5D Sketches
neurips,2017,3,374,Xingyuan,Sun,,Shanghai Jiao Tong University,,MarrNet: 3D Shape Reconstruction via 2.5D Sketches
neurips,2017,4,374,Bill,Freeman,,MIT/Google,,MarrNet: 3D Shape Reconstruction via 2.5D Sketches
neurips,2017,5,374,Josh,Tenenbaum,,MIT,,MarrNet: 3D Shape Reconstruction via 2.5D Sketches
neurips,2017,0,855,Jan-Matthis,Lueckmann,caesar,"research center caesar, an associate of the Max Planck Society",jan-matthis.lueckmann@caesar.de,Flexible statistical inference for mechanistic models of neural dynamics
neurips,2017,1,855,Pedro,Goncalves,caesar,"research center caesar, an associate of the Max Planck Society",pedro.goncalves@caesar.de,Flexible statistical inference for mechanistic models of neural dynamics
neurips,2017,2,855,Giacomo,Bassetto,caesar,"research center caesar, an associate of the Max Planck Society",giacomo.bassetto@caesar.de,Flexible statistical inference for mechanistic models of neural dynamics
neurips,2017,3,855,Kaan,Öcal,caesar,"research center caesar, an associate of the Max Planck Society",kaan.oecal@caesar.de,Flexible statistical inference for mechanistic models of neural dynamics
neurips,2017,4,855,Marcel,Nonnenmacher,caesar,"research center caesar, an associate of the Max Planck Society",marcel.nonnenmacher@caesar.de,Flexible statistical inference for mechanistic models of neural dynamics
neurips,2017,5,855,Jakob,Macke,caesar,"research center caesar, an associate of the Max Planck Society",jakob.macke@caesar.de,Flexible statistical inference for mechanistic models of neural dynamics
neurips,2017,0,2833,Chunyuan,Li,,Duke University,,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching
neurips,2017,1,2833,Hao,Liu,,Nanjing University,,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching
neurips,2017,2,2833,Changyou,Chen,,University at Buffalo,,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching
neurips,2017,3,2833,Yuchen,Pu,,Duke University,,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching
neurips,2017,4,2833,Liqun,Chen,,Duke University,,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching
neurips,2017,5,2833,Ricardo,Henao,,Duke University,,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching
neurips,2017,6,2833,Lawrence,Carin,,Duke University,,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching
neurips,2017,0,1197,Pan,Xu,virginia,University of Virginia,px3ds@virginia.edu,Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization
neurips,2017,1,1197,Jian,Ma,cmu,Carnegie Mellon University,jianma@cs.cmu.edu,Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization
neurips,2017,2,1197,Quanquan,Gu,virginia,University of Virginia,qg5w@virginia.edu,Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization
neurips,2017,0,2050,Sven,Peter,uni-heidelberg,University Heidelberg,sven.peter@iwr.uni-heidelberg.de,Sparse convolutional coding for neuronal assembly detection
neurips,2017,1,2050,Elke,Kirschbaum,uni-heidelberg,"HCI/IWR, Heidelberg University",elke.kirschbaum@iwr.uni-heidelberg.de,Sparse convolutional coding for neuronal assembly detection
neurips,2017,2,2050,Martin,Both,uni-heidelberg,Institute for Physiology and Pathophysiology Heidelberg University,mboth@physiologie.uni-heidelberg.de,Sparse convolutional coding for neuronal assembly detection
neurips,2017,3,2050,Lee,Campbell,nih,"Intramural Research Program, National Institute on Drug Abuse",lee.campbell@nih.gov,Sparse convolutional coding for neuronal assembly detection
neurips,2017,4,2050,Brandon,Harvey,nih,"Intramural Research Program, National Institute on Drug Abuse",bharvey@mail.nih.gov,Sparse convolutional coding for neuronal assembly detection
neurips,2017,5,2050,Conor,Heins,mpg,"Intramural Research Program, National Institute on Drug Abuse",conor.heins@ds.mpg.de,Sparse convolutional coding for neuronal assembly detection
neurips,2017,6,2050,Daniel,Durstewitz,zi-mannheim,CIMH Heidelberg University,daniel.durstewitz@zi-mannheim.de,Sparse convolutional coding for neuronal assembly detection
neurips,2017,7,2050,Ferran,Diego,bosch,Bosch,ferran.diegoandilla@de.bosch.com,Sparse convolutional coding for neuronal assembly detection
neurips,2017,8,2050,Fred,Hamprecht,uni-heidelberg,Heidelberg University,fred.hamprecht@iwr.uni-heidelberg.de,Sparse convolutional coding for neuronal assembly detection
neurips,2017,0,3218,Nikhil,Parthasarathy,gmail,New York University,nikparth@gmail.com,Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons
neurips,2017,1,3218,Eleanor,Batty,columbia,Columbia University,erb2180@columbia.edu,Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons
neurips,2017,2,3218,William,Falcon,columbia,Columbia University,waf2107@columbia.edu,Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons
neurips,2017,3,3218,Thomas,Rutten,columbia,Columbia University,tkr2112@columbia.edu,Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons
neurips,2017,4,3218,Mohit,Rajpal,columbia,Columbia University,mr3522@columbia.edu,Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons
neurips,2017,5,3218,E.J.,Chichilnisky,stanford,Stanford University,ej@stanford.edu,Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons
neurips,2017,6,3218,Liam,Paninski,columbia,Columbia University,liam@stat.columbia.edu,Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons
neurips,2017,0,2824,Caglar,Gulcehre,gmail,Deepmind,frdutil@gmail.com,"Plan, Attend, Generate: Planning for Sequence-to-Sequence Models"
neurips,2017,1,2824,Francis,Dutil,gmail,MILA,ca9lar@gmail.com,"Plan, Attend, Generate: Planning for Sequence-to-Sequence Models"
neurips,2017,2,2824,Adam,Trischler,microsoft,Microsoft,adam.trischler@microsoft.com,"Plan, Attend, Generate: Planning for Sequence-to-Sequence Models"
neurips,2017,3,2824,Yoshua,Bengio,gmail,U. Montreal,yoshua.umontreal@gmail.com,"Plan, Attend, Generate: Planning for Sequence-to-Sequence Models"
neurips,2017,0,1433,Yonatan,Belinkov,mit,MIT,belinkov@mit.edu,Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems
neurips,2017,1,1433,James,Glass,mit,Massachusetts Institute of Technology,glass@mit.edu,Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems
neurips,2017,0,2514,Aniket Anand,Deshmukh,umich,"University of Michigan, Ann Arbor",aniketde@umich.edu,Multi-Task Learning for Contextual Bandits
neurips,2017,1,2514,Urun,Dogan,skype,Microsoft,urun.dogan@skype.net,Multi-Task Learning for Contextual Bandits
neurips,2017,2,2514,Clay,Scott,umich,University of Michigan,clayscot@umich.edu,Multi-Task Learning for Contextual Bandits
neurips,2017,0,2237,Prateep,Bhattacharjee,iitm,Indian Institute of Technology Madras,1prateepb@cse.iitm.ac.in,Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks
neurips,2017,1,2237,Sukhendu,Das,iitm,IIT Madras,2sdas@iitm.ac.in,Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks
neurips,2017,0,2789,Chao,Qin,columbia,Columbia University,cqin22@gsb.columbia.edu,Improving the Expected Improvement Algorithm
neurips,2017,1,2789,Diego,Klabjan,northwestern,Northwestern University,d-klabjan@northwestern.edu,Improving the Expected Improvement Algorithm
neurips,2017,2,2789,Daniel,Russo,columbia,Columbia University,djr2174@gsb.columbia.edu,Improving the Expected Improvement Algorithm
neurips,2017,0,261,Xiaofan,Lin,dji,DJI,xiaofan.lin@dji.com,Towards Accurate Binary Convolutional Neural Network
neurips,2017,1,261,Cong,Zhao,dji,DJI,cong.zhao@dji.com,Towards Accurate Binary Convolutional Neural Network
neurips,2017,2,261,Wei,Pan,dji,DJI,wei.pan@dji.com,Towards Accurate Binary Convolutional Neural Network
neurips,2017,0,3155,Peter,Bartlett,,UC Berkeley,,Spectrally-normalized margin bounds for neural networks
neurips,2017,1,3155,Dylan,Foster,,Cornell University,,Spectrally-normalized margin bounds for neural networks
neurips,2017,2,3155,Matus,Telgarsky,,UIUC,,Spectrally-normalized margin bounds for neural networks
neurips,2017,0,1220,Carlo,Ciliberto,ucl,University College London,c.ciliberto@ucl.ac.uk,Consistent Multitask Learning with Nonlinear Output Relations
neurips,2017,1,1220,Alessandro,Rudi,ucl,University of Genova,m.pontil@ucl.ac.uk,Consistent Multitask Learning with Nonlinear Output Relations
neurips,2017,2,1220,Lorenzo,Rosasco,inria,University of Genova- MIT - IIT,alessandro.rudi@inria.fr,Consistent Multitask Learning with Nonlinear Output Relations
neurips,2017,3,1220,Massimiliano,Pontil,mit,IIT & UCL,lrosasco@mit.edu,Consistent Multitask Learning with Nonlinear Output Relations
neurips,2017,0,1662,Seunghyun,Park,,Seoul National University,,Deep Recurrent Neural Network-Based Identification of Precursor microRNAs
neurips,2017,1,1662,Seonwoo,Min,,Seoul National University,,Deep Recurrent Neural Network-Based Identification of Precursor microRNAs
neurips,2017,2,1662,Hyun-Soo,Choi,,Seoul Nation University,,Deep Recurrent Neural Network-Based Identification of Precursor microRNAs
neurips,2017,3,1662,Sungroh,Yoon,,Seoul National University,,Deep Recurrent Neural Network-Based Identification of Precursor microRNAs
neurips,2017,0,3165,Nicolò,Cesa-Bianchi,unimi,"Università degli Studi di Milano, Italy",nicolo.cesa-bianchi@unimi.it,Boltzmann Exploration Done Right
neurips,2017,1,3165,Claudio,Gentile,gmail,INRIA,gabor.lugosi@gmail.com,Boltzmann Exploration Done Right
neurips,2017,2,3165,Gabor,Lugosi,gmail,Pompeu Fabra University,cla.gentile@gmail.com,Boltzmann Exploration Done Right
neurips,2017,3,3165,Gergely,Neu,gmail,Universitat Pompeu Fabra,gergely.neu@gmail.com,Boltzmann Exploration Done Right
neurips,2017,0,2093,Tim,Rocktäschel,ox,University of Oxford,tim.rocktaschel@cs.ox.ac.uk,End-to-end Differentiable Proving
neurips,2017,1,2093,Sebastian,Riedel,ucl,University College London,s.riedel@cs.ucl.ac.uk,End-to-end Differentiable Proving
neurips,2017,0,596,Sheng,Li,adobe,Adobe Research,sheli@adobe.com,Matching on Balanced Nonlinear Representations for Treatment Effects Estimation
neurips,2017,1,596,Yun,Fu,neu,Northeastern University,yunfu@ece.neu.edu,Matching on Balanced Nonlinear Representations for Treatment Effects Estimation
neurips,2017,0,1741,Nicolò,Colombo,ucl,University College London,nicolo.colombo@ucl.ac.uk,Tomography of the London Underground: a Scalable Model for Origin-Destination Data
neurips,2017,1,1741,Ricardo,Silva,ucl,University College London,ricardo.silva@ucl.ac.uk,Tomography of the London Underground: a Scalable Model for Origin-Destination Data
neurips,2017,2,1741,Soong Moon,Kang,ucl,University College London,smkang@ucl.ac.uk,Tomography of the London Underground: a Scalable Model for Origin-Destination Data
neurips,2017,0,1981,Anqi,Wu,,Princeton University,,Gaussian process based nonlinear latent structure discovery in multivariate spike train data
neurips,2017,1,1981,Nicholas,Roy,,Princeton Neuroscience Institute,,Gaussian process based nonlinear latent structure discovery in multivariate spike train data
neurips,2017,2,1981,Stephen,Keeley,,Princeton University,,Gaussian process based nonlinear latent structure discovery in multivariate spike train data
neurips,2017,3,1981,Jonathan,Pillow,,Princeton University,,Gaussian process based nonlinear latent structure discovery in multivariate spike train data
neurips,2017,0,1915,Guy,Uziel,technion,Technion - Israel Institute of Technology,guziel@cs.technion.ac.il,Multi-Objective Non-parametric Sequential Prediction
neurips,2017,1,1915,Ran,El-Yaniv,technion,Technion,rani@cs.technion.ac.il,Multi-Objective Non-parametric Sequential Prediction
neurips,2017,0,1058,Minje,Jang,kaist,KAIST,jmj427@kaist.ac.kr,Optimal Sample Complexity of M-wise Data for Top-K Ranking
neurips,2017,1,1058,Sunghyun,Kim,re,ETRI,koishkim@etri.re.kr,Optimal Sample Complexity of M-wise Data for Top-K Ranking
neurips,2017,2,1058,Changho,Suh,kaist,KAIST,chsuh@kaist.ac.kr,Optimal Sample Complexity of M-wise Data for Top-K Ranking
neurips,2017,3,1058,Sewoong,Oh,illinois,UIUC,swoh@illinois.edu,Optimal Sample Complexity of M-wise Data for Top-K Ranking
neurips,2017,0,942,Cheng,Li,,College of William and Mary,,From which world is your graph
neurips,2017,1,942,Felix,Wong,,Google,,From which world is your graph
neurips,2017,2,942,Zhenming,Liu,,William and Mary,,From which world is your graph
neurips,2017,3,942,Varun,Kanade,,University of Oxford,,From which world is your graph
neurips,2017,0,1538,James,McInerney,spotify,Spotify Research,jamesm@spotify.com,An Empirical Bayes Approach to Optimizing Machine Learning Algorithms
neurips,2017,0,2935,Xiang,Wu,google,Google,wuxiang@google.com,Multiscale Quantization for Fast Similarity Search
neurips,2017,1,2935,Ruiqi,Guo,google,Google,guorq@google.com,Multiscale Quantization for Fast Similarity Search
neurips,2017,2,2935,Ananda Theertha,Suresh,google,Google,theertha@google.com,Multiscale Quantization for Fast Similarity Search
neurips,2017,3,2935,Sanjiv,Kumar,google,Google Research,sanjivk@google.com,Multiscale Quantization for Fast Similarity Search
neurips,2017,4,2935,Daniel,Holtmann-Rice,google,Google Inc,dhr@google.com,Multiscale Quantization for Fast Similarity Search
neurips,2017,5,2935,David,Simcha,google,Google,dsimcha@google.com,Multiscale Quantization for Fast Similarity Search
neurips,2017,6,2935,Felix,Yu,google,Google Research,felixyu@google.com,Multiscale Quantization for Fast Similarity Search
neurips,2017,0,3076,Zhan,Shi,uic,University of Illinois at Chicago,zshi22@uic.edu,Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction
neurips,2017,1,3076,Xinhua,Zhang,uic,University of Illinois at Chicago (UIC),zhangx@uic.edu,Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction
neurips,2017,2,3076,Yaoliang,Yu,uwaterloo,University of Waterloo,yaoliang.yu@uwaterloo.ca,Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction
neurips,2017,0,2634,Robert,Bamler,,Disney Research,,Perturbative Black Box Variational Inference
neurips,2017,1,2634,Cheng,Zhang,,Disney Research,,Perturbative Black Box Variational Inference
neurips,2017,2,2634,Manfred,Opper,,TU Berlin,,Perturbative Black Box Variational Inference
neurips,2017,3,2634,Stephan,Mandt,,Disney Research,,Perturbative Black Box Variational Inference
neurips,2017,0,3482,Jianbo,Chen,berkeley,"University of California, Berkeley",jianbochen@berkeley.edu,Kernel Feature Selection via Conditional Covariance Minimization
neurips,2017,1,3482,Mitchell,Stern,berkeley,UC Berkeley,wainwrig@berkeley.edu,Kernel Feature Selection via Conditional Covariance Minimization
neurips,2017,2,3482,Martin,Wainwright,berkeley,UC Berkeley,mitchell@berkeley.edu,Kernel Feature Selection via Conditional Covariance Minimization
neurips,2017,3,3482,Michael,Jordan,berkeley,UC Berkeley,jordan@berkeley.edu,Kernel Feature Selection via Conditional Covariance Minimization
neurips,2017,0,3527,Keerthiram,Murugesan,cmu,Carnegie Mellon University,kmuruges@cs.cmu.edu,Active Learning from Peers
neurips,2017,1,3527,Jaime,Carbonell,cmu,CMU,jgc@cs.cmu.edu,Active Learning from Peers
neurips,2017,0,2904,Geoff,Pleiss,,Cornell University,,On Fairness and Calibration
neurips,2017,1,2904,Manish,Raghavan,,Cornell University,,On Fairness and Calibration
neurips,2017,2,2904,Felix,Wu,,,,On Fairness and Calibration
neurips,2017,3,2904,Jon,Kleinberg,,Cornell University,,On Fairness and Calibration
neurips,2017,4,2904,Kilian,Weinberger,,Cornell University,,On Fairness and Calibration
neurips,2017,0,732,Yan,Duan,berkeley,UC Berkeley,rockyduan@eecs.berkeley.edu,One-Shot Imitation Learning
neurips,2017,1,732,Marcin,Andrychowicz,berkeley,OpenAI,jonathanho@eecs.berkeley.edu,One-Shot Imitation Learning
neurips,2017,2,732,Bradly,Stadie,berkeley,OpenAI,pabbeel@eecs.berkeley.edu,One-Shot Imitation Learning
neurips,2017,3,732,OpenAI,Jonathan Ho,openai,"OpenAI, UC Berkeley",marcin@openai.com,One-Shot Imitation Learning
neurips,2017,4,732,Jonas,Schneider,openai,OpenAI,bstadie@openai.com,One-Shot Imitation Learning
neurips,2017,5,732,Ilya,Sutskever,openai,,jonas@openai.com,One-Shot Imitation Learning
neurips,2017,6,732,Pieter,Abbeel,openai,OpenAI / UC Berkeley / Gradescope,ilyasu@openai.com,One-Shot Imitation Learning
neurips,2017,7,732,Wojciech,Zaremba,openai,OpenAI,woj@openai.com,One-Shot Imitation Learning
neurips,2017,0,2712,Zhe,Gan,,Duke University,,Triangle Generative Adversarial Networks
neurips,2017,1,2712,Liqun,Chen,,Duke University,,Triangle Generative Adversarial Networks
neurips,2017,2,2712,Weiyao,Wang,,Duke University,,Triangle Generative Adversarial Networks
neurips,2017,3,2712,Yuchen,Pu,,Duke University,,Triangle Generative Adversarial Networks
neurips,2017,4,2712,Yizhe,Zhang,,Duke university,,Triangle Generative Adversarial Networks
neurips,2017,5,2712,Hao,Liu,,Nanjing University,,Triangle Generative Adversarial Networks
neurips,2017,6,2712,Chunyuan,Li,,Duke University,,Triangle Generative Adversarial Networks
neurips,2017,7,2712,Lawrence,Carin,,Duke University,,Triangle Generative Adversarial Networks
neurips,2017,0,2950,Kevin,Tian,,Stanford University,,Learning Populations of Parameters
neurips,2017,1,2950,Weihao,Kong,,Stanford University,,Learning Populations of Parameters
neurips,2017,2,2950,Gregory,Valiant,,Stanford University,,Learning Populations of Parameters
neurips,2017,0,2169,Tomer,Koren,google,Google,tkoren@google.com,Multi-Armed Bandits with Metric Movement Costs
neurips,2017,1,2169,Roi,Livni,princeton,Princeton,rlivni@cs.princeton.edu,Multi-Armed Bandits with Metric Movement Costs
neurips,2017,2,2169,Yishay,Mansour,tau,Tel Aviv University,mansour@cs.tau.ac.il,Multi-Armed Bandits with Metric Movement Costs
neurips,2017,0,193,Maja,Rudolph,,Columbia University,,Structured Embedding Models for Grouped Data
neurips,2017,1,193,Francisco,Ruiz,,,,Structured Embedding Models for Grouped Data
neurips,2017,2,193,Susan,Athey,,Stanford University,,Structured Embedding Models for Grouped Data
neurips,2017,3,193,David,Blei,,Columbia University,,Structured Embedding Models for Grouped Data
neurips,2017,0,2115,Abbas,Kazerouni,stanford,Stanford University,abbask@stanford.edu,Conservative Contextual Linear Bandits
neurips,2017,1,2115,Mohammad,Ghavamzadeh,google,DeepMind,ghavamza@google.com,Conservative Contextual Linear Bandits
neurips,2017,2,2115,Yasin,Abbasi Yadkori,adobe,Adobe Research,abbasiya@adobe.com,Conservative Contextual Linear Bandits
neurips,2017,3,2115,Benjamin,Van Roy,stanford,Stanford University,bvr@stanford.edu,Conservative Contextual Linear Bandits
neurips,2017,0,924,Xiaoqian,Wang,gmail,University of Pittsburgh,xqwang1991@gmail.com,Regularized Modal Regression with Applications in Cognitive Impairment Prediction
neurips,2017,1,924,Hong,Chen,hzau,University of Pittsburgh,chenh@mail.hzau.edu.cn,Regularized Modal Regression with Applications in Cognitive Impairment Prediction
neurips,2017,2,924,Weidong,Cai,sydney,The University of Sydney,tom.cai@sydney.edu.au,Regularized Modal Regression with Applications in Cognitive Impairment Prediction
neurips,2017,3,924,Dinggang,Shen,unc,UNC-Chapel Hill,dinggang_shen@med.unc.edu,Regularized Modal Regression with Applications in Cognitive Impairment Prediction
neurips,2017,4,924,Heng,Huang,pitt,University of Pittsburgh,heng.huang@pitt.edu,Regularized Modal Regression with Applications in Cognitive Impairment Prediction
neurips,2017,0,1791,Kevin,Lin,,University of Washington,,Adversarial Ranking for Language Generation
neurips,2017,1,1791,Dianqi,Li,,University of Washington,,Adversarial Ranking for Language Generation
neurips,2017,2,1791,Xiaodong,He,,"Microsoft Research, Redmond, WA",,Adversarial Ranking for Language Generation
neurips,2017,3,1791,Zhengyou,Zhang,,Microsoft Research,,Adversarial Ranking for Language Generation
neurips,2017,4,1791,Ming-ting,Sun,,University of Washington,,Adversarial Ranking for Language Generation
neurips,2017,0,2089,SIYUAN,MA,ohio-state,The Ohio State University,masi@cse.ohio-state.edu,Diving into the shallows: a computational perspective on large-scale shallow learning
neurips,2017,1,2089,Mikhail,Belkin,ohio-state,Ohio State University,mbelkin@cse.ohio-state.edu,Diving into the shallows: a computational perspective on large-scale shallow learning
neurips,2017,0,758,Damien,Scieur,inria,INRIA - ENS,damien.scieur@inria.fr,Integration Methods and Optimization Algorithms
neurips,2017,1,758,Vincent,Roulet,inria,INRIA / ENS Ulm,vincent.roulet@inria.fr,Integration Methods and Optimization Algorithms
neurips,2017,2,758,Francis,Bach,inria,Inria,francis.bach@inria.fr,Integration Methods and Optimization Algorithms
neurips,2017,3,758,Alexandre,d'Aspremont,ens,CNRS - Ecole Normale Supérieure,aspremon@ens.fr,Integration Methods and Optimization Algorithms
neurips,2017,0,174,Krzysztof,Choromanski,google,Google Brain Robotics,kchoro@google.com,The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings
neurips,2017,1,174,Mark,Rowland,cam,University of Cambridge,mr504@cam.ac.uk,The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings
neurips,2017,2,174,Adrian,Weller,cam,University of Cambridge,aw665@cam.ac.uk,The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings
neurips,2017,0,3008,Ervin,Tanczos,wisc,University of Wisconsin - Madison,tanczos@wisc.edu,A KL-LUCB algorithm for Large-Scale Crowdsourcing
neurips,2017,1,3008,Robert,Nowak,wisc,University of Wisconsion-Madison,rdnowak@wisc.edu,A KL-LUCB algorithm for Large-Scale Crowdsourcing
neurips,2017,2,3008,Bob,Mankoff,hearst,Former Cartoon Editor of The New Yorker,bmankoff@hearst.com,A KL-LUCB algorithm for Large-Scale Crowdsourcing
neurips,2017,0,2572,Jianshu,Chen,microsoft,"Microsoft Research, Redmond, W",jianshuc@microsoft.com,Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes
neurips,2017,1,2572,Chong,Wang,microsoft,,lin.xiao@microsoft.com,Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes
neurips,2017,2,2572,Lin,Xiao,google,Microsoft Research,chongw@google.com,Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes
neurips,2017,3,2572,Ji,He,google,University Washington,lihong@google.com,Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes
neurips,2017,4,2572,Lihong,Li,citadel,Google Inc.,Ji.He@citadel.com,Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes
neurips,2017,5,2572,Li,Deng,citadel,Citadel,Li.Deng@citadel.com,Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes
neurips,2017,0,2145,Ethan,Elenberg,utexas,University of Texas at Austin,elenberg@utexas.edu,Streaming Weak Submodularity: Interpreting Neural Networks on the Fly
neurips,2017,1,2145,Alexandros,Dimakis,utexas,"University of Texas, Austin",dimakis@austin.utexas.edu,Streaming Weak Submodularity: Interpreting Neural Networks on the Fly
neurips,2017,2,2145,Moran,Feldman,openu,Open University of Israel,moranfe@openu.ac.il,Streaming Weak Submodularity: Interpreting Neural Networks on the Fly
neurips,2017,3,2145,Amin,Karbasi,yale,Yale,amin.karbasi@yale.edu,Streaming Weak Submodularity: Interpreting Neural Networks on the Fly
neurips,2017,0,1645,Alina,Ene,,University of Warwick,,Decomposable Submodular Function Minimization: Discrete and Continuous
neurips,2017,1,1645,Huy,Nguyen,,Northeastern University,,Decomposable Submodular Function Minimization: Discrete and Continuous
neurips,2017,2,1645,László,Végh,,London School of Economics,,Decomposable Submodular Function Minimization: Discrete and Continuous
neurips,2017,0,972,Sifei,Liu,,Nvidia,,Learning Affinity via Spatial Propagation Networks
neurips,2017,1,972,Shalini,De Mello,,NVIDIA,,Learning Affinity via Spatial Propagation Networks
neurips,2017,2,972,Jinwei,Gu,,NVIDIA Research,,Learning Affinity via Spatial Propagation Networks
neurips,2017,3,972,Guangyu,Zhong,,Dalian University of Technology,,Learning Affinity via Spatial Propagation Networks
neurips,2017,4,972,Ming-Hsuan,Yang,,UC Merced,,Learning Affinity via Spatial Propagation Networks
neurips,2017,5,972,Jan,Kautz,,NVIDIA,,Learning Affinity via Spatial Propagation Networks
neurips,2017,0,253,Jianfeng,Wang,gmail,Beijing University of Posts and Telecommunications,jianfengwang1991@gmail.com,Gated Recurrent Convolution Neural Network for OCR
neurips,2017,1,253,Xiaolin,Hu,tsinghua,Tsinghua University,xlhu@tsinghua.edu.cn,Gated Recurrent Convolution Neural Network for OCR
neurips,2017,0,3588,Mahdi,Karami,ualberta,University of Alberta,karami1@ualberta.ca,Multi-view Matrix Factorization for Linear Dynamical System Estimation
neurips,2017,1,3588,Martha,White,ualberta,,whitem@ualberta.ca,Multi-view Matrix Factorization for Linear Dynamical System Estimation
neurips,2017,2,3588,Dale,Schuurmans,ualberta,Google,daes@ualberta.ca,Multi-view Matrix Factorization for Linear Dynamical System Estimation
neurips,2017,3,3588,Csaba,Szepesvari,ualberta,University of Alberta,szepesva@ualberta.ca,Multi-view Matrix Factorization for Linear Dynamical System Estimation
neurips,2017,0,2255,Duc Thien,Nguyen,smu,Singapore Management University,dtnguyen.2014@smu.edu.sg,Policy Gradient With Value Function Approximation For Collective Multiagent Planning
neurips,2017,1,2255,Akshat,Kumar,smu,Singapore Management University,akshatkumar@smu.edu.sg,Policy Gradient With Value Function Approximation For Collective Multiagent Planning
neurips,2017,2,2255,Hoong Chuin,Lau,smu,Singapore Management University,hclau@smu.edu.sg,Policy Gradient With Value Function Approximation For Collective Multiagent Planning
neurips,2017,0,3437,Mohammad,Karimi,ethz,ETH Zurich,mkarimi@ethz.ch,Stochastic Submodular Maximization: The Case of Coverage Functions
neurips,2017,1,3437,Mario,Lucic,ethz,Google Brain (Zurich),lucic@inf.ethz.ch,Stochastic Submodular Maximization: The Case of Coverage Functions
neurips,2017,2,3437,Hamed,Hassani,ethz,UPenn,krausea@ethz.ch,Stochastic Submodular Maximization: The Case of Coverage Functions
neurips,2017,3,3437,Andreas,Krause,upenn,ETHZ,hassani@seas.upenn.edu,Stochastic Submodular Maximization: The Case of Coverage Functions
neurips,2017,0,2494,Raman,Arora,jhu,Johns Hopkins University,arora@cs.jhu.edu,Stochastic Approximation for Canonical Correlation Analysis
neurips,2017,1,2494,Teodor Vanislavov,Marinov,jhu,Johns Hopkins University,tmarino2@jhu.edu,Stochastic Approximation for Canonical Correlation Analysis
neurips,2017,2,2494,Poorya,Mianjy,ttic,Johns Hopkins University,nati@ttic.edu,Stochastic Approximation for Canonical Correlation Analysis
neurips,2017,3,2494,Nati,Srebro,jhu,TTI-Chicago,mianjy@jhu.edu,Stochastic Approximation for Canonical Correlation Analysis
neurips,2017,0,979,Daniel,Hsu,columbia,Columbia University,djhsu@cs.columbia.edu,Linear regression without correspondence
neurips,2017,1,979,Kevin,Shi,columbia,Columbia University,kshi@cs.columbia.edu,Linear regression without correspondence
neurips,2017,2,979,Xiaorui,Sun,columbia,Columbia University,xiaoruisun@cs.columbia.edu,Linear regression without correspondence
neurips,2017,0,2110,Zhijie,Deng,tsinghua,Tsinghua University,dzj17@mails.tsinghua.edu.cn,Structured Generative Adversarial Networks
neurips,2017,1,2110,Hao,Zhang,tsinghua,Carnegie Mellon University,xsz12@mails.tsinghua.edu.cn,Structured Generative Adversarial Networks
neurips,2017,2,2110,Xiaodan,Liang,cmu,Carnegie Mellon University,hao@cs.cmu.edu,Structured Generative Adversarial Networks
neurips,2017,3,2110,Luona,Yang,cmu,Carnegie Mellon University,xiaodan1@cs.cmu.edu,Structured Generative Adversarial Networks
neurips,2017,4,2110,Shizhen,Xu,cmu,Tsinghua University,luonay1@cs.cmu.edu,Structured Generative Adversarial Networks
neurips,2017,5,2110,Jun,Zhu,tsinghua,Tsinghua University,dcszj@mail.tsinghua.edu.cn,Structured Generative Adversarial Networks
neurips,2017,6,2110,Eric,Xing,cmu,Carnegie Mellon University,epxing@cs.cmu.edu,Structured Generative Adversarial Networks
neurips,2017,0,1886,Joao,Messias,morpheuslabs,Morpheus Labs,jmessias@morpheuslabs.co.uk,Dynamic-Depth Context Tree Weighting
neurips,2017,1,1886,Shimon,Whiteson,ox,Oxford University,shimon.whiteson@cs.ox.ac.uk,Dynamic-Depth Context Tree Weighting
neurips,2017,0,2542,Gauri,Jagatap,,Iowa State University,,"Fast, Sample-Efficient Algorithms for Structured Phase Retrieval"
neurips,2017,1,2542,Chinmay,Hegde,,Iowa State University,,"Fast, Sample-Efficient Algorithms for Structured Phase Retrieval"
neurips,2017,0,1188,Matteo,Ruffini,,UPC,,Hierarchical Methods of Moments
neurips,2017,1,1188,Guillaume,Rabusseau,,McGill University,,Hierarchical Methods of Moments
neurips,2017,2,1188,Borja,Balle,,,,Hierarchical Methods of Moments
neurips,2017,0,947,Sinong,Wang,osu,The Ohio State University,wang.7691@osu.edu,A New Alternating Direction Method for Linear Programming
neurips,2017,1,947,Ness,Shroff,osu,The Ohio State University,shroff.11@osu.edu,A New Alternating Direction Method for Linear Programming
neurips,2017,0,1972,Xingguo,Li,umn,University of Minnesota,jdhaupt@umn.edu,Near Optimal Sketching of Low-Rank Tensor Regression
neurips,2017,1,1972,Jarvis,Haupt,umn,University of Minnesota,lixx1661@umn.edu,Near Optimal Sketching of Low-Rank Tensor Regression
neurips,2017,2,1972,David,Woodruff,cmu,Carnegie Mellon University,dwoodruf@cs.cmu.edu,Near Optimal Sketching of Low-Rank Tensor Regression
neurips,2017,0,1198,Sergey,Ioffe,google,Google,sioffe@google.com,Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models
neurips,2017,0,2586,Junpei,Komiyama,komiyama,The University of Tokyo,junpei@komiyama.info,Position-based Multiple-play Bandit Problem with Unknown Position Bias
neurips,2017,1,2586,Junya,Honda,u-tokyo,The University of Tokyo / RIKEN,honda@stat.t.u-tokyo.ac.jp,Position-based Multiple-play Bandit Problem with Unknown Position Bias
neurips,2017,2,2586,Akiko,Takeda,ism,The Institute of Statistical Mathematics / RIKEN,atakeda@ism.ac.jp,Position-based Multiple-play Bandit Problem with Unknown Position Bias
neurips,2017,0,1708,Andrew,Gibiansky,baidu,Baidu Research,sercanarik@baidu.com,Deep Voice 2: Multi-Speaker Neural Text-to-Speech
neurips,2017,1,1708,Sercan,Arik,baidu,Baidu Research,gregdiamos@baidu.com,Deep Voice 2: Multi-Speaker Neural Text-to-Speech
neurips,2017,2,1708,Gregory,Diamos,baidu,Baidu SVAIL,gibianskyandrew@baidu.com,Deep Voice 2: Multi-Speaker Neural Text-to-Speech
neurips,2017,3,1708,John,Miller,baidu,Baidu Research,millerjohn@baidu.com,Deep Voice 2: Multi-Speaker Neural Text-to-Speech
neurips,2017,4,1708,Kainan,Peng,baidu,Baidu Research,pengkainan@baidu.com,Deep Voice 2: Multi-Speaker Neural Text-to-Speech
neurips,2017,5,1708,Wei,Ping,baidu,Baidu Research,pingwei01@baidu.com,Deep Voice 2: Multi-Speaker Neural Text-to-Speech
neurips,2017,6,1708,Jonathan,Raiman,baidu,Baidu Research,jonathanraiman@baidu.com,Deep Voice 2: Multi-Speaker Neural Text-to-Speech
neurips,2017,7,1708,Yanqi,Zhou,baidu,Baidu Research,zhouyanqi@baidu.com,Deep Voice 2: Multi-Speaker Neural Text-to-Speech
neurips,2017,0,1993,Alexander,Berardino,nyu,New York University,agb313@nyu.edu,Eigen-Distortions of Hierarchical Representations
neurips,2017,1,1993,Valero,Laparra,nyu,Universitat de València,johannes.balle@nyu.edu,Eigen-Distortions of Hierarchical Representations
neurips,2017,2,1993,Johannes,Ballé,uv,Google Inc.,valero.laparra@uv.es,Eigen-Distortions of Hierarchical Representations
neurips,2017,3,1993,Eero,Simoncelli,nyu,HHMI / New York University,eero.simoncelli@nyu.edu,Eigen-Distortions of Hierarchical Representations
neurips,2017,0,2517,Xin,Dong,ntu,Nanyang Technological Univ,n1503521a@e.ntu.edu.sg,Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon
neurips,2017,1,2517,Shangyu,Chen,ntu,"Nanyang Technological University, Singapore",schen025@e.ntu.edu.sg,Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon
neurips,2017,2,2517,Sinno,Pan,ntu,NTU,sinnopan@ntu.edu.sg,Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon
neurips,2017,0,1120,Yingce,Xia,gmail,University of Science and Technology of China,1yingce.xia@gmail.com,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
neurips,2017,1,1120,Fei,Tian,ustc,Miicrosoft Research,linjx@mail.ustc.edu.cn,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
neurips,2017,2,1120,Lijun,Wu,ustc,Sun Yat-sen University,ynh@ustc.edu.cn,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
neurips,2017,3,1120,Jianxin,Lin,microsoft,USTC,2fetia@microsoft.com,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
neurips,2017,4,1120,Tao,Qin,microsoft,Microsoft Research,taoqin@microsoft.com,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
neurips,2017,5,1120,Nenghai,Yu,microsoft,University of Science and Technology of China,tie-yan.liu@microsoft.com,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
neurips,2017,6,1120,Tie-Yan,Liu,sysu,Microsoft Research,3wulijun3@mail2.sysu.edu.cn,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
neurips,2017,0,2880,Anna,Volokitin,mit,ETH Zurich,gemmar@mit.edu,Do Deep Neural Networks Suffer from Crowding?
neurips,2017,1,2880,Gemma,Roig,mit,Massachusetts Institute of Technology,tp@csail.mit.edu,Do Deep Neural Networks Suffer from Crowding?
neurips,2017,2,2880,Tomaso,Poggio,ethz,MIT,voanna@vision.ee.ethz.ch,Do Deep Neural Networks Suffer from Crowding?
neurips,2017,0,2426,Sami,Remes,aalto,Aalto University,samuel.kaski@aalto.fi,Non-Stationary Spectral Kernels
neurips,2017,1,2426,Markus,Heinonen,aalto,Aalto University,sami.remes@aalto.fi,Non-Stationary Spectral Kernels
neurips,2017,2,2426,Samuel,Kaski,aalto,Aalto University,markus.o.heinonen@aalto.fi,Non-Stationary Spectral Kernels
neurips,2017,0,2907,Marcel,Nonnenmacher,caesar,Research center caesar,jakob.macke@caesar.de,Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations
neurips,2017,1,2907,Srinivas,Turaga,caesar,"Janelia Research Campus, Howard Hughes Medical Institute",marcel.nonnenmacher@caesar.de,Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations
neurips,2017,2,2907,Jakob,Macke,hhmi,"research center caesar, an associate of the Max Planck Society",turagas@janelia.hhmi.org,Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations
neurips,2017,0,547,Eric,Balkanski,,Harvard University,,Minimizing a Submodular Function from Samples
neurips,2017,1,547,Yaron,Singer,,Harvard University,,Minimizing a Submodular Function from Samples
neurips,2017,0,1270,Noga,Alon,,Tel Aviv University,,A graph-theoretic approach to multitasking
neurips,2017,1,1270,Daniel,Reichman,,"University of California, Berkeley",,A graph-theoretic approach to multitasking
neurips,2017,2,1270,Igor,Shinkar,,UC Berkeley,,A graph-theoretic approach to multitasking
neurips,2017,3,1270,Tal,Wagner,,MIT,,A graph-theoretic approach to multitasking
neurips,2017,4,1270,Sebastian,Musslick,,,,A graph-theoretic approach to multitasking
neurips,2017,5,1270,Jonathan,Cohen,,Princeton University,,A graph-theoretic approach to multitasking
neurips,2017,6,1270,Tom,Griffiths,,UC Berkeley,,A graph-theoretic approach to multitasking
neurips,2017,7,1270,Biswadip,dey,,Princeton University,,A graph-theoretic approach to multitasking
neurips,2017,8,1270,Kayhan,Ozcimder,,Princeton University,,A graph-theoretic approach to multitasking
neurips,2017,0,389,Rizal,Fathony,uic,University of Illinois at Chicago,rfatho2@uic.edu,Adversarial Surrogate Losses for Ordinal Regression
neurips,2017,1,389,Mohammad Ali,Bashiri,uic,University of Illinois at Chicago,mbashi4@uic.edu,Adversarial Surrogate Losses for Ordinal Regression
neurips,2017,2,389,Brian,Ziebart,uic,University of Illinois at Chicago,bziebart@uic.edu,Adversarial Surrogate Losses for Ordinal Regression
neurips,2017,0,3022,Michael,Janner,mit,MIT,janner@mit.edu,Self-Supervised Intrinsic Image Decomposition
neurips,2017,1,3022,Jiajun,Wu,mit,MIT,jiajunwu@mit.edu,Self-Supervised Intrinsic Image Decomposition
neurips,2017,2,3022,Tejas,Kulkarni,gmail,DeepMind,tejasdkulkarni@gmail.com,Self-Supervised Intrinsic Image Decomposition
neurips,2017,3,3022,Ilker,Yildirim,mit,MIT,ilkery@mit.edu,Self-Supervised Intrinsic Image Decomposition
neurips,2017,4,3022,Josh,Tenenbaum,mit,MIT,jbt@mit.edu,Self-Supervised Intrinsic Image Decomposition
neurips,2017,0,2133,Graham,Neubig,cmu,Carnegie Mellon University,gneubig@cs.cmu.edu,On-the-fly Operation Batching in Dynamic Computation Graphs
neurips,2017,1,2133,Yoav,Goldberg,biu,Bar-Ilan University,yogo@cs.biu.ac.il,On-the-fly Operation Batching in Dynamic Computation Graphs
neurips,2017,2,2133,Chris,Dyer,google,DeepMind,cdyer@google.com,On-the-fly Operation Batching in Dynamic Computation Graphs
neurips,2017,0,1447,Kohei,Hayashi,gmail,AIST / RIKEN,hayashi.kohei@gmail.com,Fitting Low-Rank Tensors in Constant Time
neurips,2017,1,1447,Yuichi,Yoshida,nii,"National Institute of Informatics and Preferred Infrastructure, Inc.",yyoshida@nii.ac.jp,Fitting Low-Rank Tensors in Constant Time
neurips,2017,0,3293,Amir-massoud,Farahmand,merl,Mitsubishi Electric Research Laboratories (MERL),farahmand@merl.com,Random Projection Filter Bank for Time Series Data
neurips,2017,1,3293,Sepideh,Pourazarm,bu,MERL,sepid@bu.edu,Random Projection Filter Bank for Time Series Data
neurips,2017,2,3293,Daniel,Nikovski,merl,,nikovski@merl.com,Random Projection Filter Bank for Time Series Data
neurips,2017,0,1530,Santiago,Balseiro,columbia,Duke University,srb2155@columbia.edu,Dynamic Revenue Sharing
neurips,2017,1,1530,Max,Lin,google,Google,whlin@google.com,Dynamic Revenue Sharing
neurips,2017,2,1530,Vahab,Mirrokni,google,Google Research NYC,mirrokni@google.com,Dynamic Revenue Sharing
neurips,2017,3,1530,Renato,Leme,google,Google Research,renatoppl@google.com,Dynamic Revenue Sharing
neurips,2017,4,1530,IIIS,Song Zuo,gmail,"IIIS, Tsinghua University",songzuo.z@gmail.com,Dynamic Revenue Sharing
neurips,2017,0,2153,Jake,Snell,,University of Toronto,,Prototypical Networks for Few-shot Learning
neurips,2017,1,2153,Kevin,Swersky,,Google Brain,,Prototypical Networks for Few-shot Learning
neurips,2017,2,2153,Richard,Zemel,,University of Toronto,,Prototypical Networks for Few-shot Learning
neurips,2017,0,565,James,Thewlis,ox,University of Oxford,jdt@robots.ox.ac.uk,Unsupervised learning of object frames by dense equivariant image labelling
neurips,2017,1,565,Hakan,Bilen,ox,University of Edinburgh,vedaldi@robots.ox.ac.uk,Unsupervised learning of object frames by dense equivariant image labelling
neurips,2017,2,565,Andrea,Vedaldi,ed,University of Oxford,hbilen@ed.ac.uk,Unsupervised learning of object frames by dense equivariant image labelling
neurips,2017,0,2264,Cesar,Caiafa,gmail,Indiana University,ccaiafa@gmail.com,Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays
neurips,2017,1,2264,Olaf,Sporns,indiana,Department of Psychological and Brain Sciences - Indiana University,osporns@indiana.edu,Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays
neurips,2017,2,2264,Andrew,Saykin,iupui,IUPUI,asaykin@iupui.edu,Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays
neurips,2017,3,2264,Franco,Pestilli,indiana,Indiana University,franpest@indiana.edu,Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays
neurips,2017,0,2208,Wojciech,Kotlowski,poznan,Poznan University of Technology,wkotlowski@cs.put.poznan.pl,Random Permutation Online Isotonic Regression
neurips,2017,1,2208,Wouter,Koolen,cwi,"Centrum Wiskunde & Informatica, Amsterdam",wmkoolen@cwi.nl,Random Permutation Online Isotonic Regression
neurips,2017,2,2208,Alan,Malek,mit,MIT,amalek@mit.edu,Random Permutation Online Isotonic Regression
neurips,2017,0,2716,Yi-An,Lai,ntu,National Taiwan University,b99202031@ntu.edu.tw,PRUNE: Preserving Proximity and Global Ranking for Network Embedding
neurips,2017,1,2716,Chin-Chi,Hsu,sinica,Academia Sinica,chinchi@iis.sinica.edu.tw,PRUNE: Preserving Proximity and Global Ranking for Network Embedding
neurips,2017,2,2716,Wen Hao,Chen,ntu,National Taiwan University,b02902023@ntu.edu.tw,PRUNE: Preserving Proximity and Global Ranking for Network Embedding
neurips,2017,3,2716,Mi-Yen,Yeh,sinica,Academia Sinica,miyen@iis.sinica.edu.tw,PRUNE: Preserving Proximity and Global Ranking for Network Embedding
neurips,2017,4,2716,Shou-De,Lin,ntu,National Taiwan University,sdlin@csie.ntu.edu.tw,PRUNE: Preserving Proximity and Global Ranking for Network Embedding
neurips,2017,0,1023,Kfir,Levy,ethz,ETH,yehuda.levy@inf.ethz.ch,"Online to Offline Conversions, Universality and Adaptive Minibatch Sizes"
neurips,2017,0,1498,Wengong,Jin,mit,MIT CSAIL,wengong@csail.mit.edu,Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network
neurips,2017,1,1498,Connor,Coley,mit,MIT,regina@csail.mit.edu,Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network
neurips,2017,2,1498,Regina,Barzilay,mit,Massachusetts Institute of Technology,tommi@csail.mit.edu,Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network
neurips,2017,3,1498,Tommi,Jaakkola,mit,MIT,ccoley@mit.edu,Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network
neurips,2017,0,183,Paroma,Varma,stanford,Stanford University,paroma@stanford.edu,Inferring Generative Model Structure with Static Analysis
neurips,2017,1,183,Bryan,He,stanford,Stanford University,bryanhe@stanford.edu,Inferring Generative Model Structure with Static Analysis
neurips,2017,2,183,Payal,Bajaj,stanford,Stanford University,pabajaj@stanford.edu,Inferring Generative Model Structure with Static Analysis
neurips,2017,3,183,Nishith,Khandwala,stanford,Stanford University,nishith@stanford.edu,Inferring Generative Model Structure with Static Analysis
neurips,2017,4,183,Imon,Banerjee,stanford,Stanford University,imonb@stanford.edu,Inferring Generative Model Structure with Static Analysis
neurips,2017,5,183,Daniel,Rubin,stanford,Stanford University,rubin@stanford.edu,Inferring Generative Model Structure with Static Analysis
neurips,2017,6,183,Christopher,Ré,stanford,Stanford,chrismre@cs.stanford.edu,Inferring Generative Model Structure with Static Analysis
neurips,2017,0,2094,Qiang,Li,ict,Institute of Computing Technol,liqiang01@ict.ac.cn,Influence Maximization with $\varepsilon$-Almost Submodular Threshold Functions
neurips,2017,1,2094,Wei,Chen,ict,Microsoft Research,sunxiaoming@ict.ac.cn,Influence Maximization with $\varepsilon$-Almost Submodular Threshold Functions
neurips,2017,2,2094,Institute of Computing,Xiaoming Sun,ict,"Institute of Computing Technology, Chinese Academy of Sciences",zhangjialin@ict.ac.cn,Influence Maximization with $\varepsilon$-Almost Submodular Threshold Functions
neurips,2017,3,2094,Institute of Computing,Jialin Zhang,microsoft,"Institute of Computing Technology, Chinese Academy of Sciences",weic@microsoft.com,Influence Maximization with $\varepsilon$-Almost Submodular Threshold Functions
neurips,2017,0,478,Lijun,Zhang,nju,Nanjing University (NJU),zhanglj@lamda.nju.edu.cn,Improved Dynamic Regret for Non-degenerate Functions
neurips,2017,1,478,Tianbao,Yang,uiowa,The University of Iowa,tianbao-yang@uiowa.edu,Improved Dynamic Regret for Non-degenerate Functions
neurips,2017,2,478,Jinfeng,Yi,tencent,Tencent AI Lab/IBM TJ Watson Research Center,jinfengyi@tencent.com,Improved Dynamic Regret for Non-degenerate Functions
neurips,2017,3,478,Rong,Jin,alibaba-inc,Alibaba,jinrong.jr@alibaba-inc.com,Improved Dynamic Regret for Non-degenerate Functions
neurips,2017,4,478,Zhi-Hua,Zhou,nju,Nanjing University,zhouzh@lamda.nju.edu.cn,Improved Dynamic Regret for Non-degenerate Functions
neurips,2017,0,2808,Ilya,Tolstikhin,mpg,MPI for Intelligent Systems,ilya@tue.mpg.de,AdaGAN: Boosting Generative Models
neurips,2017,1,2808,Sylvain,Gelly,google,Google Brain,sylvaingelly@google.com,AdaGAN: Boosting Generative Models
neurips,2017,2,2808,Olivier,Bousquet,google,Google,obousquet@google.com,AdaGAN: Boosting Generative Models
neurips,2017,3,2808,Carl-Johann,SIMON-GABRIEL,mpg,Max Planck Institute for Intelligent Systems,cjsimon@tue.mpg.de,AdaGAN: Boosting Generative Models
neurips,2017,4,2808,Bernhard,Schölkopf,mpg,MPI for Intelligent Systems,bs@tue.mpg.de,AdaGAN: Boosting Generative Models
neurips,2017,0,1339,Kinjal,Basu,linkedin,LinkedIn,kbasu@linkedin.com,Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences
neurips,2017,1,1339,Ankan,Saha,linkedin,Linkedin Corporation,asaha@linkedin.com,Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences
neurips,2017,2,1339,Shaunak,Chatterjee,linkedin,,shchatte@linkedin.com,Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences
neurips,2017,0,1806,Bo,Jiang,ahu,Anhui University,jiangbo@ahu.edu.cn,Graph Matching via Multiplicative Update Algorithm
neurips,2017,1,1806,Jin,Tang,ahu,,tj@ahu.edu.cn,Graph Matching via Multiplicative Update Algorithm
neurips,2017,2,1806,Chris,Ding,uta,University of Texas at Arlington,chqding@uta.edu,Graph Matching via Multiplicative Update Algorithm
neurips,2017,3,1806,Yihong,Gong,xjtu,,ygong@mail.xjtu.edu.cn,Graph Matching via Multiplicative Update Algorithm
neurips,2017,4,1806,Bin,Luo,ahu,,luobin@ahu.edu.cn,Graph Matching via Multiplicative Update Algorithm
neurips,2017,0,3354,Klaus,Greff,idsia,IDSIA,klaus@idsia.ch,Neural Expectation Maximization
neurips,2017,1,3354,Sjoerd,van Steenkiste,idsia,The Swiss AI Lab - IDSIA,sjoerd@idsia.ch,Neural Expectation Maximization
neurips,2017,2,3354,Jürgen,Schmidhuber,idsia,"Swiss AI Lab, IDSIA (USI & SUPSI) - NNAISENSE",juergen@idsia.ch,Neural Expectation Maximization
neurips,2017,0,73,Saurabh,Verma,umn,University of Minnesota Twin Cities,verma@cs.umn.edu,"Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs"
neurips,2017,1,73,Zhi-Li,Zhang,umn,University of Minnesota,zhang@cs.umn.edu,"Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs"
neurips,2017,0,1238,Le,Fang,buffalo,University at Buffalo-SUNY,lefang@buffalo.edu,Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems
neurips,2017,1,1238,Fan,Yang,buffalo,University at Buffalo,fyang24@buffalo.edu,Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems
neurips,2017,2,1238,Wen,Dong,buffalo,University at Buffalo,wendong@buffalo.edu,Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems
neurips,2017,3,1238,Tong,Guan,buffalo,,tongguan@buffalo.edu,Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems
neurips,2017,4,1238,Chunming,Qiao,buffalo,,qiao@buffalo.edu,Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems
neurips,2017,0,2088,Darrell,Hoy,gmail,Tremor Technologies,darrell.hoy@gmail.com,Welfare Guarantees from Data
neurips,2017,1,2088,Denis,Nekipelov,virginia,University of Virginia,denis@virginia.edu,Welfare Guarantees from Data
neurips,2017,2,2088,Vasilis,Syrgkanis,microsoft,Microsoft Research,vasy@microsoft.com,Welfare Guarantees from Data
neurips,2017,0,2851,Abhishek,Kumar,ibm,IBM Research AI,abhishk@us.ibm.com,Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference
neurips,2017,1,2851,Prasanna,Sattigeri,ibm,IBM Research,psattig@us.ibm.com,Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference
neurips,2017,2,2851,Tom,Fletcher,utah,University of Utah,fletcher@sci.utah.edu,Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference
neurips,2017,0,3501,Moustapha,Cisse,,Facebook AI Research,,Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples
neurips,2017,1,3501,Yossi,Adi,,Bar Ilan University,,Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples
neurips,2017,2,3501,Natalia,Neverova,,Facebook AI Research,,Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples
neurips,2017,3,3501,Joseph,Keshet,,Bar-Ilan University,,Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples
neurips,2017,0,3260,Aravindan,Vijayaraghavan,northwestern,Northwestern University,adutta@u.northwestern.edu,Clustering Stable Instances of Euclidean k-means.
neurips,2017,1,3260,Abhratanu,Dutta,northwestern,Northwestern University,aravindv@northwestern.edu,Clustering Stable Instances of Euclidean k-means.
neurips,2017,2,3260,Alex,Wang,northwestern,Northwestern University,alexwang@u.northwestern.edu,Clustering Stable Instances of Euclidean k-means.
neurips,2017,0,3418,Ritambhara,Singh,,University of Virginia,,Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin
neurips,2017,1,3418,Jack,Lanchantin,,University of Virginia,,Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin
neurips,2017,2,3418,Arshdeep,Sekhon,,University of Virginia,,Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin
neurips,2017,3,3418,Yanjun,Qi,,University of Virginia,,Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin
neurips,2017,0,2251,Paul,Christiano,,OpenAI,,Deep Reinforcement Learning from Human Preferences
neurips,2017,1,2251,Jan,Leike,,DeepMind,,Deep Reinforcement Learning from Human Preferences
neurips,2017,2,2251,Tom,Brown,,Google Brain,,Deep Reinforcement Learning from Human Preferences
neurips,2017,3,2251,Miljan,Martic,,DeepMind,,Deep Reinforcement Learning from Human Preferences
neurips,2017,4,2251,Shane,Legg,,DeepMind,,Deep Reinforcement Learning from Human Preferences
neurips,2017,5,2251,Dario,Amodei,,OpenAI,,Deep Reinforcement Learning from Human Preferences
neurips,2017,0,2010,Chao,Qian,ustc,University of Science and Technology of China,chaoqian@ustc.edu.cn,Subset Selection under Noise
neurips,2017,1,2010,Jing-Cheng,Shi,sustc,Nanjing University,tangk3@sustc.edu.cn,Subset Selection under Noise
neurips,2017,2,2010,Yang,Yu,nju,Nanjing University,shijc@lamda.nju.edu.cn,Subset Selection under Noise
neurips,2017,3,2010,Ke,Tang,nju,University of Science and Technology of China,yuy@lamda.nju.edu.cn,Subset Selection under Noise
neurips,2017,4,2010,Zhi-Hua,Zhou,nju,Nanjing University,zhouzh@lamda.nju.edu.cn,Subset Selection under Noise
neurips,2017,0,2650,Charles Ruizhongtai,Qi,,Stanford University,,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space
neurips,2017,1,2650,Li,Yi,,Stanford University,,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space
neurips,2017,2,2650,Hao,Su,,Stanford,,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space
neurips,2017,3,2650,Leonidas,Guibas,,stanford.edu,,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space
neurips,2017,0,1755,Benjamin,Moseley,cmu,,moseleyb@andrew.cmu.edu,"Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search"
neurips,2017,1,1755,Joshua,Wang,stanford,Stanford University,joshua.wang@cs.stanford.edu,"Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search"
neurips,2017,0,2771,Thomas,Anthony,,UCL,,Thinking Fast and Slow with Deep Learning and Tree Search
neurips,2017,1,2771,Zheng,Tian,,UCL,,Thinking Fast and Slow with Deep Learning and Tree Search
neurips,2017,2,2771,David,Barber,,University College London,,Thinking Fast and Slow with Deep Learning and Tree Search
neurips,2017,0,3183,Elias,Khalil,gatech,Georgia Tech,hanjun.dai@cc.gatech.edu,Learning Combinatorial Optimization Algorithms over Graphs
neurips,2017,1,3183,Hanjun,Dai,gatech,Georgia Tech,elias.khalil@cc.gatech.edu,Learning Combinatorial Optimization Algorithms over Graphs
neurips,2017,2,3183,Yuyu,Zhang,gatech,,yuyu.zhang@cc.gatech.edu,Learning Combinatorial Optimization Algorithms over Graphs
neurips,2017,3,3183,Bistra,Dilkina,gatech,Georgia Institute of Technology,bdilkina@cc.gatech.edu,Learning Combinatorial Optimization Algorithms over Graphs
neurips,2017,4,3183,Le,Song,gatech,Georgia Institute of Technology,lsong@cc.gatech.edu,Learning Combinatorial Optimization Algorithms over Graphs
neurips,2017,0,2497,Jeffrey,Pennington,,Google Brain,,Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice
neurips,2017,1,2497,Samuel,Schoenholz,,Google Brain,,Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice
neurips,2017,2,2497,Surya,Ganguli,,Stanford,,Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice
neurips,2017,0,2471,Feng,Nan,bu,Boston University,fnan@bu.edu,Adaptive Classification for Prediction Under a Budget
neurips,2017,1,2471,Venkatesh,Saligrama,bu,Boston University,srv@bu.edu,Adaptive Classification for Prediction Under a Budget
neurips,2017,0,917,Hao,Yu,usc,University of Southern California,yuhao@usc.edu,Online Convex Optimization with Stochastic Constraints
neurips,2017,1,917,Michael,Neely,usc,Univ. Southern California,mjneely@usc.edu,Online Convex Optimization with Stochastic Constraints
neurips,2017,2,917,Xiaohan,Wei,usc,University of Southern California,xiaohanw@usc.edu,Online Convex Optimization with Stochastic Constraints
neurips,2017,0,3407,Kirill,Neklyudov,gmail,Yandex,k.necludov@gmail.com,Structured Bayesian Pruning via Log-Normal Multiplicative Noise
neurips,2017,1,3407,Dmitry,Molchanov,hse,Yandex,dmolchanov@hse.ru,Structured Bayesian Pruning via Log-Normal Multiplicative Noise
neurips,2017,2,3407,Arsenii,Ashukha,hse,,aashukha@hse.ru,Structured Bayesian Pruning via Log-Normal Multiplicative Noise
neurips,2017,3,3407,Dmitry,Vetrov,hse,"Higher School of Economics, Yandex",dvetrov@hse.ru,Structured Bayesian Pruning via Log-Normal Multiplicative Noise
neurips,2017,0,2956,Arya,Mazumdar,umass,University of Massachusetts Amherst,arya@cs.umass.edu,Clustering with Noisy Queries
neurips,2017,1,2956,Barna,Saha,umass,University of Massachusetts Amherst,barna@cs.umass.edu,Clustering with Noisy Queries
neurips,2017,0,566,Jose,Alvarez,tri,TRI,jose.alvarez@tri.global,Compression-aware Training of Deep Networks
neurips,2017,1,566,Mathieu,Salzmann,epfl,EPFL,mathieu.salzmann@epfl.ch,Compression-aware Training of Deep Networks
neurips,2017,0,3549,Moein,Falahatgar,ucsd,UCSD,moein@ucsd.edu,Maxing and Ranking with Few Assumptions
neurips,2017,1,3549,Yi,Hao,ucsd,UCSD,yih179@ucsd.edu,Maxing and Ranking with Few Assumptions
neurips,2017,2,3549,Alon,Orlitsky,ucsd,"University of California, San Diego",alon@ucsd.edu,Maxing and Ranking with Few Assumptions
neurips,2017,3,3549,Venkatadheeraj,Pichapati,ucsd,UC San Diego,dheerajpv7@ucsd.edu,Maxing and Ranking with Few Assumptions
neurips,2017,4,3549,Vaishakh,Ravindrakumar,ucsd,UC San Diego,vaishakhr@ucsd.edu,Maxing and Ranking with Few Assumptions
neurips,2017,0,3391,Amin,Jalali,wisc,Wisconsin Institute for Discovery,amin.jalali@wisc.edu,Subspace Clustering via Tangent Cones
neurips,2017,1,3391,Rebecca,Willett,wisc,University of Wisconsin,willett@discovery.wisc.edu,Subspace Clustering via Tangent Cones
neurips,2017,0,2563,Maksims,Volkovs,,layer6.ai,,DropoutNet: Addressing Cold Start in Recommender Systems
neurips,2017,1,2563,Guangwei,Yu,,layer6.ai,,DropoutNet: Addressing Cold Start in Recommender Systems
neurips,2017,2,2563,Tomi,Poutanen,,,,DropoutNet: Addressing Cold Start in Recommender Systems
neurips,2017,0,469,Ming-Yu,Liu,nvidia,NVIDIA,mingyul@nvidia.com,Unsupervised Image-to-Image Translation Networks
neurips,2017,1,469,Thomas,Breuel,nvidia,,tbreuel@nvidia.com,Unsupervised Image-to-Image Translation Networks
neurips,2017,2,469,Jan,Kautz,nvidia,NVIDIA,jkautz@nvidia.com,Unsupervised Image-to-Image Translation Networks
neurips,2017,0,3088,Maithra,Raghu,,Cornell University and Google Brain,,SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability
neurips,2017,1,3088,Justin,Gilmer,,Google Brain,,SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability
neurips,2017,2,3088,Jason,Yosinski,,Uber,,SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability
neurips,2017,3,3088,Jascha,Sohl-Dickstein,,Google Brain,,SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability
neurips,2017,0,1322,Quentin,Berthet,cam,University of Cambridge,q.berthet@statslab.cam.ac.uk,Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe
neurips,2017,1,1322,Vianney,Perchet,normalesup,ENS Paris-Saclay & Criteo Research,vianney.perchet@normalesup.org,Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe
neurips,2017,0,2681,Honglei,Zhuang,illinois,University of Illinois,hzhuang3@illinois.edu,Identifying Outlier Arms in Multi-Armed Bandit
neurips,2017,1,2681,Chi,Wang,microsoft,Microsoft Research,wang.chi@microsoft.com,Identifying Outlier Arms in Multi-Armed Bandit
neurips,2017,2,2681,Yifan,Wang,tsinghua,Tsinghua University,yifan-wa16@mails.tsinghua.edu.cn,Identifying Outlier Arms in Multi-Armed Bandit
neurips,2017,0,2396,Hyeji,Kim,illinois,University of Illinois Urbana-Champaign,hyejikim@illinois.edu,Discovering Potential Correlations via Hypercontractivity
neurips,2017,1,2396,Weihao,Gao,illinois,UIUC,wgao9@illinois.edu,Discovering Potential Correlations via Hypercontractivity
neurips,2017,2,2396,Sreeram,Kannan,uw,University of Washington,ksreeram@uw.edu,Discovering Potential Correlations via Hypercontractivity
neurips,2017,3,2396,Sewoong,Oh,illinois,UIUC,swoh@illinois.edu,Discovering Potential Correlations via Hypercontractivity
neurips,2017,4,2396,Pramod,Viswanath,illinois,UIUC,pramodv@illinois.edu,Discovering Potential Correlations via Hypercontractivity
neurips,2017,0,878,Hongteng,Xu,gmail,Duke University,hongtengxu313@gmail.com,A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering
neurips,2017,1,878,Hongyuan,Zha,gatech,Georgia Tech,zha@cc.gatech.edu,A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering
neurips,2017,0,3476,Muhammad,Farhan,lums,LUMS,14030031@lums.edu.pk,Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification
neurips,2017,1,3476,Juvaria,Tariq,emory,Emory Univeristy,jtariq@emory.edu,Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification
neurips,2017,2,3476,Arif,Zaman,lums,LUMS,arifz@lums.edu.pk,Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification
neurips,2017,3,3476,Mudassir,Shabbir,itu,ITU,mudassir.shabbir@itu.edu.pk,Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification
neurips,2017,4,3476,Imdad Ullah,Khan,lums,LUMS,imdad.khan@lums.edu.pk,Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification
neurips,2017,0,1906,Mathieu,Blondel,mblondel,NTT,mathieu@mblondel.org,Multi-output Polynomial Networks and Factorization Machines
neurips,2017,1,1906,Vlad,Niculae,cornell,Cornell University,vlad@cs.cornell.edu,Multi-output Polynomial Networks and Factorization Machines
neurips,2017,2,1906,Takuma,Otsuka,ntt,NTT Communication Science Labs,otsuka.takuma@lab.ntt.co.jp,Multi-output Polynomial Networks and Factorization Machines
neurips,2017,3,1906,Naonori,Ueda,ntt,NTT Communication Science Laboratories/ RIKEN AIP,ueda.naonori@lab.ntt.co.jp,Multi-output Polynomial Networks and Factorization Machines
neurips,2017,0,1973,Arthur,Choi,ucla,UCLA,aychoi@cs.ucla.edu,Tractability in Structured Probability Spaces
neurips,2017,1,1973,Yujia,Shen,ucla,UCLA,yujias@cs.ucla.edu,Tractability in Structured Probability Spaces
neurips,2017,2,1973,Adnan,Darwiche,ucla,UCLA,darwiche@cs.ucla.edu,Tractability in Structured Probability Spaces
neurips,2017,0,1148,Luigi,Acerbi,,New York University,,Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search
neurips,2017,1,1148,Wei Ji,Ma,,New York University,,Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search
neurips,2017,0,2247,Matthias,Poloczek,arizona,Cornell University,poloczek@email.arizona.edu,Multi-Information Source Optimization
neurips,2017,1,2247,Jialei,Wang,cornell,IBM,jw865@cornell.edu,Multi-Information Source Optimization
neurips,2017,2,2247,Peter,Frazier,cornell,Cornell / Uber,pf98@cornell.edu,Multi-Information Source Optimization
neurips,2017,0,1835,Mikko,Heikkilä,helsinki,University of Helsinki,mikko.a.heikkila@helsinki.fi,Differentially private Bayesian learning on distributed data
neurips,2017,1,1835,Eemil,Lagerspetz,helsinki,University of Helsinki,eemil.lagerspetz@helsinki.fi,Differentially private Bayesian learning on distributed data
neurips,2017,2,1835,Samuel,Kaski,aalto,Aalto University,samuel.kaski@aalto.fi,Differentially private Bayesian learning on distributed data
neurips,2017,3,1835,Kana,Shimizu,gmail,Waseda University,shimizu.kana.g@gmail.com,Differentially private Bayesian learning on distributed data
neurips,2017,4,1835,Sasu,Tarkoma,helsinki,University of Helsinki,sasu.tarkoma@helsinki.fi,Differentially private Bayesian learning on distributed data
neurips,2017,5,1835,Antti,Honkela,helsinki,University of Helsinki,antti.honkela@helsinki.fi,Differentially private Bayesian learning on distributed data
neurips,2017,0,1315,Chun-Liang,Li,cmu,Carnegie Mellon University,chunlial@cs.cmu.edu,MMD GAN: Towards Deeper Understanding of Moment Matching Network
neurips,2017,1,1315,Wei-Cheng,Chang,cmu,Carnegie Mellon University,wchang2@cs.cmu.edu,MMD GAN: Towards Deeper Understanding of Moment Matching Network
neurips,2017,2,1315,Yu,Cheng,cmu,"AI Foundations, IBM Research",yiming@cs.cmu.edu,MMD GAN: Towards Deeper Understanding of Moment Matching Network
neurips,2017,3,1315,Yiming,Yang,cmu,CMU,bapoczos@cs.cmu.edu,MMD GAN: Towards Deeper Understanding of Moment Matching Network
neurips,2017,4,1315,Barnabas,Poczos,ibm,Carnegie Mellon University,chengyu@us.ibm.com,MMD GAN: Towards Deeper Understanding of Moment Matching Network
neurips,2017,0,3485,Bowei,Yan,utexas,University of Texas at Austin,boweiy@utexas.edu,Convergence of Gradient EM on Multi-component Mixture of Gaussians
neurips,2017,1,3485,Mingzhang,Yin,utexas,University of Texas at Austin,mzyin@utexas.edu,Convergence of Gradient EM on Multi-component Mixture of Gaussians
neurips,2017,2,3485,Purnamrita,Sarkar,utexas,UT Austin,purna.sarkar@austin.utexas.edu,Convergence of Gradient EM on Multi-component Mixture of Gaussians
neurips,2017,0,1269,Stéphanie,van der Pas,leidenuniv,Leiden University,svdpas@math.leidenuniv.nl,Bayesian Dyadic Trees and Histograms for  Regression
neurips,2017,1,1269,Veronika,Roková,ChicagoBooth,University of Chicago,Veronika.Rockova@ChicagoBooth.edu,Bayesian Dyadic Trees and Histograms for  Regression
neurips,2017,0,3506,Stefan,Bauer,ethz,ETH Zürich,bauers@inf.ethz.ch,Efficient and Flexible Inference for Stochastic Systems
neurips,2017,1,3506,Nico,Gorbach,ethz,Swiss Federal Institute of Technology Zurich (ETHZ),ngorbach@inf.ethz.ch,Efficient and Flexible Inference for Stochastic Systems
neurips,2017,2,3506,Djordje,Miladinovic,ethz,ETH Zurich,djordjem@inf.ethz.ch,Efficient and Flexible Inference for Stochastic Systems
neurips,2017,3,3506,Joachim,Buhmann,ethz,ETH Zurich,jbuhmann@inf.ethz.ch,Efficient and Flexible Inference for Stochastic Systems
neurips,2017,0,1233,Mahdi,Soltanolkotabi,usc,University of Southern california,soltanol@usc.edu,Learning ReLUs via Gradient Descent
neurips,2017,0,2655,Alberto,Garcia Duran,neclab,NEC Europe,alberto.duran@neclab.eu,Learning Graph Representations with Embedding Propagation
neurips,2017,1,2655,Mathias,Niepert,neclab,NEC Labs Europe,mathias.niepert@neclab.eu,Learning Graph Representations with Embedding Propagation
neurips,2017,0,1331,Matthias,Hein,,Saarland University,,Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation
neurips,2017,1,1331,Maksym,Andriushchenko,,Saarland University,,Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation
neurips,2017,0,2085,Boqian,Zhang,gmail,Purdue University,panjiangwei@gmail.com,Collapsed variational Bayes for Markov jump processes
neurips,2017,1,2085,Jiangwei,Pan,purdue,Facebook,zhan1977@purdue.edu,Collapsed variational Bayes for Markov jump processes
neurips,2017,2,2085,Vinayak,Rao,purdue,Purdue University,varao@purdue.edu,Collapsed variational Bayes for Markov jump processes
neurips,2017,0,1831,Matthieu,Geist,univ-lorraine,Université de Lorraine,matthieu.geist@univ-lorraine.fr,Is the Bellman residual a bad proxy?
neurips,2017,1,1831,Bilal,Piot,univ-lille1,DeepMind,bilal.piot@univ-lille1.fr,Is the Bellman residual a bad proxy?
neurips,2017,2,1831,Olivier,Pietquin,univ-lille1,DeepMind,olivier.pietquin@univ-lille1.fr,Is the Bellman residual a bad proxy?
neurips,2017,0,2233,Celestine,Dünner,ibm,IBM Research,cdu@zurich.ibm.com,Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems
neurips,2017,1,2233,Thomas,Parnell,ibm,IBM Research,tpa@zurich.ibm.com,Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems
neurips,2017,2,2233,Martin,Jaggi,epfl,EPFL,martin.jaggi@epfl.ch,Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems
neurips,2017,0,1431,Yichong,Xu,cmu,Carnegie Mellon University,yichongx@cs.cmu.edu,Noise-Tolerant Interactive Learning Using Pairwise Comparisons
neurips,2017,1,1431,Hongyang,Zhang,cmu,Carnegie Mellon University,hongyanz@cs.cmu.edu,Noise-Tolerant Interactive Learning Using Pairwise Comparisons
neurips,2017,2,1431,Kyle,Miller,cmu,Carnegie Mellon University,aarti@cs.cmu.edu,Noise-Tolerant Interactive Learning Using Pairwise Comparisons
neurips,2017,3,1431,Aarti,Singh,cmu,Carnegie Mellon University,awd@cs.cmu.edu,Noise-Tolerant Interactive Learning Using Pairwise Comparisons
neurips,2017,4,1431,Artur,Dubrawski,cmu,Carnegie Mellon University,mille856@andrew.cmu.edu,Noise-Tolerant Interactive Learning Using Pairwise Comparisons
neurips,2017,0,2421,Sanjiban,Choudhury,cmu,Carnegie Mellon University,sanjiban@cmu.edu,Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs
neurips,2017,1,2421,Shervin,Javdani,cmu,Carnegie Mellon University,sjavdani@cmu.edu,Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs
neurips,2017,2,2421,Siddhartha,Srinivasa,cmu,Carnegie Mellon University,siddh@cs.cmu.edu,Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs
neurips,2017,3,2421,Sebastian,Scherer,cmu,Carnegie Mellon University,basti@cs.cmu.edu,Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs
neurips,2017,0,1115,Richard,Combes,supelec,Centrale-Supelec,richard.combes@supelec.fr,Minimal Exploration in Structured Stochastic Bandits
neurips,2017,1,1115,Stefan,Magureanu,kth,KTH,magur@kth.se,Minimal Exploration in Structured Stochastic Bandits
neurips,2017,2,1115,Alexandre,Proutiere,kth,KTH,alepro@kth.se,Minimal Exploration in Structured Stochastic Bandits
neurips,2017,0,483,Guobin,Chen,,University of Missouri,,Learning Efficient Object Detection Models with Knowledge Distillation
neurips,2017,1,483,Wongun,Choi,,NEC Laboratories,,Learning Efficient Object Detection Models with Knowledge Distillation
neurips,2017,2,483,Xiang,Yu,,NEC Laboratories America,,Learning Efficient Object Detection Models with Knowledge Distillation
neurips,2017,3,483,Tony,Han,,University of Missouri,,Learning Efficient Object Detection Models with Knowledge Distillation
neurips,2017,4,483,Manmohan,Chandraker,,"University of California, San Diego",,Learning Efficient Object Detection Models with Knowledge Distillation
neurips,2017,0,1156,Kari,Rantanen,,University of Helsinki,,Learning Chordal Markov Networks via Branch and Bound
neurips,2017,1,1156,Antti,Hyttinen,,University of Helsinki,,Learning Chordal Markov Networks via Branch and Bound
neurips,2017,2,1156,Matti,Järvisalo,,University of Helsinki,,Learning Chordal Markov Networks via Branch and Bound
neurips,2017,0,1956,Wenbing,Huang,tencent,Tencent AI Lab,1helendhuang@tencent.com,Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding
neurips,2017,1,1956,Mehrtash,Harandi,tencent,Data61,joehhuang@tencent.com,Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding
neurips,2017,2,1956,Tong,Zhang,csiro,The Australian National University & DATA61,2mehrtash.harandi@data61.csiro.au,Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding
neurips,2017,3,1956,Lijie,Fan,anu,Tsinghua University,tong.zhang@anu.edu.cn,Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding
neurips,2017,4,1956,Fuchun,Sun,,Tsinghua University,3flj14@mails,Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding
neurips,2017,5,1956,Junzhou,Huang,tsinghua,University of Texas at Arlington / Tencent AI Lab,fcsun@mail.tsinghua.edu.cn,Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding
neurips,2017,0,34,Pan,Ji,,University of Adelaide,,Deep Subspace Clustering Networks
neurips,2017,1,34,Tong,Zhang,,The Australian National University & DATA61,,Deep Subspace Clustering Networks
neurips,2017,2,34,Hongdong,Li,,Australian National University,,Deep Subspace Clustering Networks
neurips,2017,3,34,Mathieu,Salzmann,,EPFL,,Deep Subspace Clustering Networks
neurips,2017,4,34,Ian,Reid,,University of Adelaide,,Deep Subspace Clustering Networks
neurips,2017,0,1663,Hakan,Inan,stanford,Stanford University,inanh@stanford.edu,Robust Estimation of Neural Signals in Calcium Imaging
neurips,2017,1,1663,Murat,Erdogdu,toronto,Microsoft Research,erdogdu@cs.toronto.edu,Robust Estimation of Neural Signals in Calcium Imaging
neurips,2017,2,1663,Mark,Schnitzer,stanford,Stanford University,mschnitz@stanford.edu,Robust Estimation of Neural Signals in Calcium Imaging
neurips,2017,0,3020,Asier,Mujika,ethz,ETH Zürich,asierm@ethz.ch,Fast-Slow Recurrent Neural Networks
neurips,2017,1,3020,Florian,Meier,ethz,ETH Zurich,meierflo@inf.ethz.ch,Fast-Slow Recurrent Neural Networks
neurips,2017,2,3020,Angelika,Steger,ethz,ETH Zurich,steger@inf.ethz.ch,Fast-Slow Recurrent Neural Networks
neurips,2017,0,573,Yunbo,Wang,tsinghua,Tsinghua University,wangyb15@mails.tsinghua.edu.cn,PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs
neurips,2017,1,573,Mingsheng,Long,tsinghua,Tsinghua University,mingsheng@tsinghua.edu.cn,PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs
neurips,2017,2,573,Jianmin,Wang,tsinghua,Tsinghua University,jimwang@tsinghua.edu.cn,PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs
neurips,2017,3,573,Zhifeng,Gao,tsinghua,Tsinghua University,gzf16@mails.tsinghua.edu.cn,PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs
neurips,2017,4,573,Philip,Yu,uic,UIC,psyu@uic.edu,PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs
neurips,2017,0,1526,Tu,Nguyen,deakin,Deakin University,tu.nguyen@deakin.edu.au,Dual Discriminator Generative Adversarial Nets
neurips,2017,1,1526,Trung,Le,deakin,Deakin University,trung.l@deakin.edu.au,Dual Discriminator Generative Adversarial Nets
neurips,2017,2,1526,Hung,Vu,deakin,Deakin University,hungv@deakin.edu.au,Dual Discriminator Generative Adversarial Nets
neurips,2017,3,1526,Dinh,Phung,deakin,Deakin University,dinh.phung@deakin.edu.au,Dual Discriminator Generative Adversarial Nets
neurips,2017,0,1679,Sirui,Yao,vt,Virginia Polytechnic Institute and State University,bhuang@vt.edu,Beyond Parity: Fairness Objectives for Collaborative Filtering
neurips,2017,1,1679,Bert,Huang,vt,Virginia Tech,ysirui@vt.edu,Beyond Parity: Fairness Objectives for Collaborative Filtering
neurips,2017,0,1492,Guillaume,Rabusseau,,McGill University,,Multitask Spectral Learning of Weighted Automata
neurips,2017,1,1492,Borja,Balle,,,,Multitask Spectral Learning of Weighted Automata
neurips,2017,2,1492,Joelle,Pineau,,McGill University,,Multitask Spectral Learning of Weighted Automata
neurips,2017,0,2565,Adam,Santoro,google,DeepMind,adamsantoro@google.com,A simple neural network module for relational reasoning
neurips,2017,1,2565,David,Raposo,google,DeepMind,draposo@google.com,A simple neural network module for relational reasoning
neurips,2017,2,2565,David,Barrett,google,DeepMind,barrettdavid@google.com,A simple neural network module for relational reasoning
neurips,2017,3,2565,Mateusz,Malinowski,google,DeepMind,mateuszm@google.com,A simple neural network module for relational reasoning
neurips,2017,4,2565,Razvan,Pascanu,google,Google DeepMind,razp@google.com,A simple neural network module for relational reasoning
neurips,2017,5,2565,Peter,Battaglia,google,DeepMind,peterbattaglia@google.com,A simple neural network module for relational reasoning
neurips,2017,6,2565,Timothy,Lillicrap,google,Google DeepMind,countzero@google.com,A simple neural network module for relational reasoning
neurips,2017,0,2874,Arash,Vahdat,dwavesys,D-Wave Systems Inc.,avahdat@dwavesys.com,Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks
neurips,2017,0,3547,Zhengyuan,Zhou,stanford,Stanford University,zyzhou@stanford.edu,Stochastic Mirror Descent in Variationally Coherent Optimization Problems
neurips,2017,1,3547,Panayotis,Mertikopoulos,imag,CNRS (French National Center for Scientific Research),panayotis.mertikopoulos@imag.fr,Stochastic Mirror Descent in Variationally Coherent Optimization Problems
neurips,2017,2,3547,Nicholas,Bambos,stanford,,bambos@stanford.edu,Stochastic Mirror Descent in Variationally Coherent Optimization Problems
neurips,2017,3,3547,Stephen,Boyd,stanford,Stanford University,boyd@stanford.edu,Stochastic Mirror Descent in Variationally Coherent Optimization Problems
neurips,2017,4,3547,Peter,Glynn,stanford,Stanford University,glynn@stanford.edu,Stochastic Mirror Descent in Variationally Coherent Optimization Problems
neurips,2017,0,2304,Qian,Yu,,University of Southern Califor,,Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication
neurips,2017,1,2304,Mohammad,Maddah-Ali,,Nokia Bell Labs,,Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication
neurips,2017,2,2304,Salman,Avestimehr,,USC,,Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication
neurips,2017,0,2863,Hao,He,mit,MIT,haohe@mit.edu,From Bayesian Sparsity to Gated Recurrent Nets
neurips,2017,1,2863,Bo,Xin,gmail,Microsoft Research,jimxinbo@gmail.com,From Bayesian Sparsity to Gated Recurrent Nets
neurips,2017,2,2863,Satoshi,Ikehata,gmail,National Institute of Informatics,satoshi.ikehata@gmail.com,From Bayesian Sparsity to Gated Recurrent Nets
neurips,2017,3,2863,David,Wipf,gmail,Microsoft Research,davidwipf@gmail.com,From Bayesian Sparsity to Gated Recurrent Nets
neurips,2017,0,1247,Alberto Maria,Metelli,polimi,Politecnico di Milano,albertomaria.metelli@polimi.it,Compatible Reward Inverse Reinforcement Learning
neurips,2017,1,1247,Matteo,Pirotta,inria,INRIA Lille-Nord Europe,matteo.pirotta@inria.fr,Compatible Reward Inverse Reinforcement Learning
neurips,2017,2,1247,Marcello,Restelli,polimi,Politecnico di Milano,marcello.restelli@polimi.it,Compatible Reward Inverse Reinforcement Learning
neurips,2017,0,1274,Kush,Bhatia,berkeley,UC Berkeley,kushbhatia@berkeley.edu,Consistent Robust Regression
neurips,2017,1,1274,Prateek,Jain,microsoft,Microsoft Research,prajain@microsoft.com,Consistent Robust Regression
neurips,2017,2,1274,Parameswaran,Kamalaruban,epfl,EPFL,kamalaruban.parameswaran@epfl.ch,Consistent Robust Regression
neurips,2017,3,1274,Purushottam,Kar,iitk,Indian Institute of Technology Kanpur,purushot@cse.iitk.ac.in,Consistent Robust Regression
neurips,2017,0,2503,Nico,Gorbach,ethz,Swiss Federal Institute of Technology Zurich (ETHZ),ngorbach@inf.ethz.ch,Scalable Variational Inference for Dynamical Systems
neurips,2017,1,2503,Stefan,Bauer,ethz,ETH Zürich,bauers@inf.ethz.ch,Scalable Variational Inference for Dynamical Systems
neurips,2017,2,2503,Joachim,Buhmann,ethz,ETH Zurich,jbuhmann@inf.ethz.ch,Scalable Variational Inference for Dynamical Systems
neurips,2017,0,360,Sylvestre-Alvise,Rebuffi,ox,University of Oxford,srebuffi@robots.ox.ac.uk,Learning multiple visual domains with residual adapters
neurips,2017,1,360,Hakan,Bilen,ox,University of Edinburgh,hbilen@robots.ox.ac.uk,Learning multiple visual domains with residual adapters
neurips,2017,2,360,Andrea,Vedaldi,ox,University of Oxford,vedaldi@robots.ox.ac.uk,Learning multiple visual domains with residual adapters
neurips,2017,0,2104,Di,Kang,cityu,City University of Hong Kong,dkang5-c@my.cityu.edu.hk,Incorporating Side Information by Adaptive Convolution
neurips,2017,1,2104,Debarun,Dhar,cityu,City University of Hong Kong,ddhar2-c@my.cityu.edu.hk,Incorporating Side Information by Adaptive Convolution
neurips,2017,2,2104,Antoni,Chan,cityu,City University of Hong Kong,abchan@cityu.edu.hk,Incorporating Side Information by Adaptive Convolution
neurips,2017,0,3139,Vincent,Cohen-Addad,gmail,University of Copenhagen,vcohenad@gmail.com,Hierarchical Clustering Beyond the Worst-Case
neurips,2017,1,3139,Varun,Kanade,ox,University of Oxford,varunk@cs.ox.ac.uk,Hierarchical Clustering Beyond the Worst-Case
neurips,2017,2,3139,Frederik,Mallmann-Trenn,mit,ENS,mallmann@mit.edu,Hierarchical Clustering Beyond the Worst-Case
neurips,2017,0,3474,Geoffrey,Roeder,toronto,University of Toronto,roeder@cs.toronto.edu,"Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference"
neurips,2017,1,3474,Yuhuai,Wu,toronto,University of Toronto,ywu@cs.toronto.edu,"Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference"
neurips,2017,2,3474,David,Duvenaud,toronto,University of Toronto,duvenaud@cs.toronto.edu,"Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference"
neurips,2017,0,3000,Gerasimos,Palaiopanos,yahoo,SUTD,gerasimosath@yahoo.com,"Multiplicative Weights Update with Constant Step-Size in Congestion Games:  Convergence, Limit Cycles and Chaos"
neurips,2017,1,3000,Ioannis,Panageas,mit,MIT,ioannis@csail.mit.edu,"Multiplicative Weights Update with Constant Step-Size in Congestion Games:  Convergence, Limit Cycles and Chaos"
neurips,2017,2,3000,Georgios,Piliouras,sutd,Singapore University of Technology and Design,georgios@sutd.edu.sg,"Multiplicative Weights Update with Constant Step-Size in Congestion Games:  Convergence, Limit Cycles and Chaos"
neurips,2017,0,2460,Peter,Karkus,nus,NUS,karkus@comp.nus.edu.sg,QMDP-Net: Deep Learning for Planning under Partial Observability
neurips,2017,1,2460,David,Hsu,nus,National University of Singapore,dyhsu@comp.nus.edu.sg,QMDP-Net: Deep Learning for Planning under Partial Observability
neurips,2017,2,2460,Wee Sun,Lee,nus,National University of Singapore,leews@comp.nus.edu.sg,QMDP-Net: Deep Learning for Planning under Partial Observability
neurips,2017,0,1448,Qi,Li,ia,"Institute of Automation, Chinese Academy of Sciences",qli@nlpr.ia.ac.cn,Deep Supervised Discrete Hashing
neurips,2017,1,1448,Zhenan,Sun,ia,"Institute of Automation, Chinese Academy of Sciences (CASIA)",znsun@nlpr.ia.ac.cn,Deep Supervised Discrete Hashing
neurips,2017,2,1448,Ran,He,ia,CASIA,rhe@nlpr.ia.ac.cn,Deep Supervised Discrete Hashing
neurips,2017,3,1448,Tieniu,Tan,ia,Chinese Academy of Sciences,tnt@nlpr.ia.ac.cn,Deep Supervised Discrete Hashing
neurips,2017,0,3328,Karl,Bringmann,mpg,Saarland University,kbringma@mpi-inf.mpg.de,Approximation Algorithms for $\ell_0$-Low Rank Approximation
neurips,2017,1,3328,Pavel,Kolev,mpg,Max-Planck-Institut für Informatik,pkolev@mpi-inf.mpg.de,Approximation Algorithms for $\ell_0$-Low Rank Approximation
neurips,2017,2,3328,David,Woodruff,cmu,Carnegie Mellon University,dwoodruf@cs.cmu.edu,Approximation Algorithms for $\ell_0$-Low Rank Approximation
neurips,2017,0,844,Yi,Xu,uiowa,The University of Iowa,yi-xu@uiowa.edu,ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization
neurips,2017,1,844,Mingrui,Liu,uiowa,The University of Iowa,mingrui-liu@uiowa.edu,ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization
neurips,2017,2,844,Qihang,Lin,uiowa,University of Iowa,qihang-lin@uiowa.edu,ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization
neurips,2017,3,844,Tianbao,Yang,uiowa,The University of Iowa,tianbao-yang@uiowa.edu,ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization
neurips,2017,0,3110,Yuanbin,Wu,ecnu,East China Normal University,ybwu@cs.ecnu.edu.cn,A Learning Error Analysis for Structured Prediction with Approximate Inference
neurips,2017,1,3110,Man,Lan,ecnu,,mlan@cs.ecnu.edu.cn,A Learning Error Analysis for Structured Prediction with Approximate Inference
neurips,2017,2,3110,Shiliang,Sun,ecnu,East China Normal University,slsun@cs.ecnu.edu.cn,A Learning Error Analysis for Structured Prediction with Approximate Inference
neurips,2017,3,3110,Qi,Zhang,fudan,Fudan University,qz@fudan.edu.cn,A Learning Error Analysis for Structured Prediction with Approximate Inference
neurips,2017,4,3110,Xuanjing,Huang,fudan,Fudan University,xjhuang@fudan.edu.cn,A Learning Error Analysis for Structured Prediction with Approximate Inference
neurips,2017,0,2384,Ping,Li,gmail,Rugters University,pingli98@gmail.com,Simple strategies for recovering inner products from coarsely quantized random projections
neurips,2017,1,2384,Martin,Slawski,gmu,,mslawsk3@gmu.edu,Simple strategies for recovering inner products from coarsely quantized random projections
neurips,2017,0,2356,Song,Liu,bristol,University of Bristol,song.liu@bristol.ac.uk,Trimmed Density Ratio Estimation
neurips,2017,1,2356,Akiko,Takeda,ism,The Institute of Statistical Mathematics / RIKEN,atakeda@ism.ac.jp,Trimmed Density Ratio Estimation
neurips,2017,2,2356,Taiji,Suzuki,ism,taiji@mist.i.u-tokyo.ac.jp,fukumizu@ism.ac.jp,Trimmed Density Ratio Estimation
neurips,2017,3,2356,Kenji,Fukumizu,u-tokyo,Institute of Statistical Mathematics,taiji@mist.i.u-tokyo.ac.jp,Trimmed Density Ratio Estimation
neurips,2017,0,2019,Matteo,Papini,polimi,Politecnico di Milano,matteo.papini@polimi.it,Adaptive Batch Size for Safe Policy Gradients
neurips,2017,1,2019,Matteo,Pirotta,inria,INRIA Lille-Nord Europe,matteo.pirotta@inria.fr,Adaptive Batch Size for Safe Policy Gradients
neurips,2017,2,2019,Marcello,Restelli,polimi,Politecnico di Milano,marcello.restelli@polimi.it,Adaptive Batch Size for Safe Policy Gradients
neurips,2017,0,1380,Rebecca,Morrison,mit,Massachusetts Institute of Technology,rmorriso@mit.edu,Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting
neurips,2017,1,1380,Ricardo,Baptista,mit,MIT,rsb@mit.edu,Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting
neurips,2017,2,1380,Youssef,Marzouk,mit,Massachusetts Institute of Technology,ymarz@mit.edu,Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting
neurips,2017,0,1508,George,Tucker,google,Google Brain,gjt@google.com,"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models"
neurips,2017,1,1508,Andriy,Mnih,google,DeepMind,amnih@google.com,"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models"
neurips,2017,2,1508,Chris,Maddison,google,University of Oxford / DeepMind,dieterichl@google.com,"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models"
neurips,2017,3,1508,John,Lawson,google,Google Brain,jaschasd@google.com,"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models"
neurips,2017,4,1508,Jascha,Sohl-Dickstein,ox,Google Brain,cmaddis@stats.ox.ac.uk,"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models"
neurips,2017,0,1037,Noga,Alon,tau,Tel Aviv University,nogaa@tau.ac.il,Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues
neurips,2017,1,1037,Moshe,Babaioff,microsoft,Microsoft Research,moshe@microsoft.com,Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues
neurips,2017,2,1037,Yannai A.,Gonczarowski,gonch,The Hebrew University of Jerusalem and Microsoft Research,yannai@gonch.name,Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues
neurips,2017,3,1037,Yishay,Mansour,tau,Tel Aviv University,mansour@tau.ac.il,Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues
neurips,2017,4,1037,Shay,Moran,gmail,"IAS, Princeton",shaymoran1@gmail.com,Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues
neurips,2017,5,1037,Amir,Yehudayoff,gmail,Technion - Israel institue of Technology,amir.yehudayoff@gmail.com,Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues
neurips,2017,0,867,Soheil,Feizi,stanford,Stanford University,sfeizi@stanford.edu,Tensor Biclustering
neurips,2017,1,867,Hamid,Javadi,stanford,Stanford University,hrhakim@stanford.edu,Tensor Biclustering
neurips,2017,2,867,David,Tse,stanford,Stanford University,dntse@stanford.edu,Tensor Biclustering
neurips,2017,0,299,Iku,Ohama,panasonic,Panasonic Corporation,ohama.iku@jp.panasonic.com,On the Model Shrinkage Effect of Gamma Process Edge Partition Models
neurips,2017,1,299,Issei,Sato,u-tokyo,The University of Tokyo/RIKEN,sato@k.u-tokyo.ac.jp,On the Model Shrinkage Effect of Gamma Process Edge Partition Models
neurips,2017,2,299,Takuya,Kida,hokudai,Hokkaido University,kida@ist.hokudai.ac.jp,On the Model Shrinkage Effect of Gamma Process Edge Partition Models
neurips,2017,3,299,Hiroki,Arimura,hokudai,Hokkaido University,arim@ist.hokudai.ac.jp,On the Model Shrinkage Effect of Gamma Process Edge Partition Models
neurips,2017,0,3050,Weihao,Gao,illinois,UIUC,wgao9@illinois.edu,Estimating Mutual Information for Discrete-Continuous Mixtures
neurips,2017,1,3050,Sreeram,Kannan,uw,University of Washington,ksreeram@uw.edu,Estimating Mutual Information for Discrete-Continuous Mixtures
neurips,2017,2,3050,Sewoong,Oh,illinois,UIUC,swoh@illinois.edu,Estimating Mutual Information for Discrete-Continuous Mixtures
neurips,2017,3,3050,Pramod,Viswanath,illinois,UIUC,pramodv@illinois.edu,Estimating Mutual Information for Discrete-Continuous Mixtures
neurips,2017,0,2227,Yamur,Güçlütürk,,Radboud University,,Reconstructing perceived faces from brain activations with deep adversarial neural decoding
neurips,2017,1,2227,Umut,Güçlü,,Donders Institute,,Reconstructing perceived faces from brain activations with deep adversarial neural decoding
neurips,2017,2,2227,Katja,Seeliger,,"Donders Institute for Brain, Cognition and Behaviour",,Reconstructing perceived faces from brain activations with deep adversarial neural decoding
neurips,2017,3,2227,Sander,Bosch,,Radboud University,,Reconstructing perceived faces from brain activations with deep adversarial neural decoding
neurips,2017,4,2227,Rob,van Lier,,"Donders Institute for Brain, Cognition and Behaviour, Radboud University",,Reconstructing perceived faces from brain activations with deep adversarial neural decoding
neurips,2017,5,2227,Marcel,van Gerven,,Radboud Universiteit,,Reconstructing perceived faces from brain activations with deep adversarial neural decoding
neurips,2017,0,1394,Kai,Fan,duke,Duke University,kai.fan@stat.duke.edu,An inner-loop free solution to inverse problems using deep neural networks
neurips,2017,1,1394,Qi,Wei,duke,Duke University,qi.wei@duke.edu,An inner-loop free solution to inverse problems using deep neural networks
neurips,2017,2,1394,Lawrence,Carin,duke,Duke University,lcarin@duke.edu,An inner-loop free solution to inverse problems using deep neural networks
neurips,2017,3,1394,Katherine,Heller,duke,Duke,kheller@stat.duke.edu,An inner-loop free solution to inverse problems using deep neural networks
neurips,2017,0,3039,Fanny,Yang,berkeley,"University of California, Berkeley",fanny-yang@berkeley.edu,A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control
neurips,2017,1,3039,Aaditya,Ramdas,washington,"University of California, Berkeley",jamieson@cs.washington.edu,A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control
neurips,2017,2,3039,Kevin,Jamieson,berkeley,UC Berkeley,ramdas@berkeley.edu,A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control
neurips,2017,3,3039,Martin,Wainwright,berkeley,UC Berkeley,wainwrig@berkeley.edu,A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control
neurips,2017,0,106,Lin,Chen,,Yale University,,Interactive Submodular Bandit
neurips,2017,1,106,Andreas,Krause,,ETHZ,,Interactive Submodular Bandit
neurips,2017,2,106,Amin,Karbasi,,Yale,,Interactive Submodular Bandit
neurips,2017,0,2544,Dan,Tito Svenstrup,dtu,DTU,dsve@dtu.dk,Hash Embeddings for Efficient Word Representations
neurips,2017,1,2544,Jonas,Hansen,findzebra,Findzebra,jonas@findzebra.com,Hash Embeddings for Efficient Word Representations
neurips,2017,2,2544,Ole,Winther,dtu,Technical University of Denmark,olwi@dtu.dk,Hash Embeddings for Efficient Word Representations
neurips,2017,0,2185,Blake,Mason,umich,University of Wisconsin - Madison,lalitj@umich.edu,Learning Low-Dimensional Metrics
neurips,2017,1,2185,Lalit,Jain,wisc,University of Michigan,bmason3@wisc.edu,Learning Low-Dimensional Metrics
neurips,2017,2,2185,Robert,Nowak,wisc,University of Wisconsion-Madison,rdnowak@wisc.edu,Learning Low-Dimensional Metrics
neurips,2017,0,2009,Yu,Liu,,Citadel LLC,,Unsupervised Sequence Classification using Sequential Output Statistics
neurips,2017,1,2009,Jianshu,Chen,,"Microsoft Research, Redmond, W",,Unsupervised Sequence Classification using Sequential Output Statistics
neurips,2017,2,2009,Li,Deng,,Citadel,,Unsupervised Sequence Classification using Sequential Output Statistics
neurips,2017,0,1939,Manzil,Zaheer,cmu,Carnegie Mellon University,manzilz@cs.cmu.edu,Deep Sets
neurips,2017,1,1939,Satwik,Kottur,cmu,Carnegie Mellon University,skottur@cs.cmu.edu,Deep Sets
neurips,2017,2,1939,Siamak,Ravanbakhsh,cmu,CMU/UBC,mravanba@cs.cmu.edu,Deep Sets
neurips,2017,3,1939,Barnabas,Poczos,cmu,Carnegie Mellon University,bapoczos@cs.cmu.edu,Deep Sets
neurips,2017,4,1939,Russ,Salakhutdinov,cmu,,rsalakhu@cs.cmu.edu,Deep Sets
neurips,2017,5,1939,Alexander,Smola,cmu,Amazon - We are hiring!,smola@cs.cmu.edu,Deep Sets
neurips,2017,0,3120,Danny,Barash,huji,The Hebrew University Of Jerusalem,danny.barash@mail.huji.ac.il,Optimal Shrinkage of Singular Values Under Random Data Contamination
neurips,2017,1,3120,Matan,Gavish,huji,Hebrew University,gavish@cs.huji.ac.il,Optimal Shrinkage of Singular Values Under Random Data Contamination
neurips,2017,0,3303,Aditi,Raghunathan,stanford,Stanford University,aditir@stanford.edu,Learning Mixture of Gaussians with Streaming Data
neurips,2017,1,3303,Prateek,Jain,microsoft,Microsoft Research,prajain@microsoft.com,Learning Mixture of Gaussians with Streaming Data
neurips,2017,2,3303,Ravishankar,Krishnawamy,microsoft,Microsoft Research India,rakri@microsoft.com,Learning Mixture of Gaussians with Streaming Data
neurips,2017,0,1836,Alexander,Ratner,stanford,Stanford,ajratner@cs.stanford.edu,Learning to Compose Domain-Specific Transformations for Data Augmentation
neurips,2017,1,1836,Henry,Ehrenberg,stanford,Stanford University,henryre@cs.stanford.edu,Learning to Compose Domain-Specific Transformations for Data Augmentation
neurips,2017,2,1836,Zeshan,Hussain,stanford,Stanford University,zeshanmh@cs.stanford.edu,Learning to Compose Domain-Specific Transformations for Data Augmentation
neurips,2017,3,1836,Jared,Dunnmon,stanford,Stanford University,jdunnmon@cs.stanford.edu,Learning to Compose Domain-Specific Transformations for Data Augmentation
neurips,2017,4,1836,Christopher,Ré,stanford,Stanford,chrismre@cs.stanford.edu,Learning to Compose Domain-Specific Transformations for Data Augmentation
neurips,2017,0,324,Sekitoshi,Kanai,ntt,NTT,kanai.sekitoshi@lab.ntt.co.jp,Preventing Gradient Explosions in Gated Recurrent Units
neurips,2017,1,324,Yasuhiro,Fujiwara,ntt,NTT Software Innovation Center,fujiwara.yasuhiro@lab.ntt.co.jp,Preventing Gradient Explosions in Gated Recurrent Units
neurips,2017,2,324,Sotetsu,Iwamura,ntt,NTT Software Innovation center,iwamura.sotetsu@lab.ntt.co.jp,Preventing Gradient Explosions in Gated Recurrent Units
neurips,2017,0,1877,Thang,Bui,cam,University of Cambridge,tdb40@cam.ac.uk,Streaming Sparse Gaussian Process Approximations
neurips,2017,1,1877,Cuong,Nguyen,cam,University of Cambridge,vcn22@cam.ac.uk,Streaming Sparse Gaussian Process Approximations
neurips,2017,2,1877,Richard,Turner,cam,University of Cambridge,ret26@cam.ac.uk,Streaming Sparse Gaussian Process Approximations
neurips,2017,0,1544,Di,Wang,buffalo,State University of New York at Buffalo,dwang45@buffalo.edu,Differentially Private Empirical Risk Minimization Revisited: Faster and More General
neurips,2017,1,1544,Minwei,Ye,buffalo,University at Buffalo,minweiye@buffalo.edu,Differentially Private Empirical Risk Minimization Revisited: Faster and More General
neurips,2017,2,1544,Jinhui,Xu,buffalo,SUNY at Buffalo,jinhui@buffalo.edu,Differentially Private Empirical Risk Minimization Revisited: Faster and More General
neurips,2017,0,3079,Edouard,Grave,fb,Facebook AI Research,egrave@fb.com,Unbounded cache model for online language modeling with open vocabulary
neurips,2017,1,3079,Moustapha,Cisse,fb,Facebook AI Research,moustaphacisse@fb.com,Unbounded cache model for online language modeling with open vocabulary
neurips,2017,2,3079,Armand,Joulin,fb,Facebook AI research,ajoulin@fb.com,Unbounded cache model for online language modeling with open vocabulary
neurips,2017,0,847,Zhoutong,Zhang,,MIT,,Shape and Material from Sound
neurips,2017,1,847,Qiujia,Li,,University of Cambridge,,Shape and Material from Sound
neurips,2017,2,847,Zhengjia,Huang,,Shanghaitech,,Shape and Material from Sound
neurips,2017,3,847,Jiajun,Wu,,MIT,,Shape and Material from Sound
neurips,2017,4,847,Josh,Tenenbaum,,MIT,,Shape and Material from Sound
neurips,2017,5,847,Bill,Freeman,,MIT/Google,,Shape and Material from Sound
neurips,2017,0,49,Heinrich,Jiang,gmail,Google,heinrich.jiang@gmail.com,On the Consistency of Quick Shift
neurips,2017,0,1846,Shuai,Xiao,sjtu,Georgia Institute of Technology,benjaminforever@sjtu.edu.cn,Wasserstein Learning of Deep Generative Point Process Models
neurips,2017,1,1846,Mehrdad,Farajtabar,gatech,Georgia Tech,mehrdad@gatech.edu,Wasserstein Learning of Deep Generative Point Process Models
neurips,2017,2,1846,Xiaojing,Ye,gsu,Georgia State University,xye@gsu.edu,Wasserstein Learning of Deep Generative Point Process Models
neurips,2017,3,1846,Junchi,Yan,ibm,IBM Research - China,yanjc@cn.ibm.com,Wasserstein Learning of Deep Generative Point Process Models
neurips,2017,4,1846,Le,Song,gatech,Georgia Institute of Technology,lsong@cc.gatech.edu,Wasserstein Learning of Deep Generative Point Process Models
neurips,2017,5,1846,Hongyuan,Zha,gatech,Georgia Tech,zha@cc.gatech.edu,Wasserstein Learning of Deep Generative Point Process Models
neurips,2017,0,94,Peva,Blanchard,epfl,,peva.blanchard@epfl.ch,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent
neurips,2017,1,94,El Mahdi,El Mhamdi,epfl,EPFL,rachid.guerraoui@epfl.ch,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent
neurips,2017,2,94,Rachid,Guerraoui,epfl,,elmahdi.elmhamdi@epfl.ch,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent
neurips,2017,3,94,Julien,Stainer,epfl,,julien.stainer@epfl.ch,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent
neurips,2017,0,3278,Alex,Fout,colostate,Colorado State University,jonbyrd@colostate.edu,Protein Interface Prediction using Graph Convolutional Networks
neurips,2017,1,3278,Jonathon,Byrd,colostate,Colorado State University,fout@colostate.edu,Protein Interface Prediction using Graph Convolutional Networks
neurips,2017,2,3278,Basir,Shariat,colostate,Colorado State University,basir@cs.colostate.edu,Protein Interface Prediction using Graph Convolutional Networks
neurips,2017,3,3278,Asa,Ben-Hur,colostate,Colorado State University,asa@cs.colostate.edu,Protein Interface Prediction using Graph Convolutional Networks
neurips,2017,0,2482,Linxi,Liu,columbia,Columbia University,ll3098@columbia.edu,Convergence rates of a partition based Bayesian multivariate density estimation method
neurips,2017,1,2482,Dangna,Li,stanford,Stanford University,dangna@stanford.edu,Convergence rates of a partition based Bayesian multivariate density estimation method
neurips,2017,2,2482,Wing Hung,Wong,stanford,Stanford university,whwong@stanford.edu,Convergence rates of a partition based Bayesian multivariate density estimation method
neurips,2017,0,450,Niki,Kilbertus,mpg,MPI Tuebingen & Cambridge,nkilbertus@tue.mpg.de,Avoiding Discrimination through Causal Reasoning
neurips,2017,1,450,Mateo,Rojas Carulla,mpg,"University of Cambridge, Max Planck for Intelligent Systems",mrojas@tue.mpg.de,Avoiding Discrimination through Causal Reasoning
neurips,2017,2,450,Giambattista,Parascandolo,mpg,Max Planck Institute for Intelligent Systems and and Max Planck ETH CLS,gparascandolo@tue.mpg.de,Avoiding Discrimination through Causal Reasoning
neurips,2017,3,450,Moritz,Hardt,berkeley,UC Berkeley,hardt@berkeley.edu,Avoiding Discrimination through Causal Reasoning
neurips,2017,4,450,Dominik,Janzing,mpg,MPI Tübingen,janzing@tue.mpg.de,Avoiding Discrimination through Causal Reasoning
neurips,2017,5,450,Bernhard,Schölkopf,mpg,MPI for Intelligent Systems,bs@tue.mpg.de,Avoiding Discrimination through Causal Reasoning
neurips,2017,0,1618,Sheng,Chen,umn,University of Minnesota,shengc@cs.umn.edu,Alternating Estimation for Structured High-Dimensional Multi-Response Models
neurips,2017,1,1618,Arindam,Banerjee,umn,University of Minnesota,banerjee@cs.umn.edu,Alternating Estimation for Structured High-Dimensional Multi-Response Models
neurips,2017,0,375,Ilija,Ilievski,nus,National University of Singapore,ilija.ilievski@u.nus.edu,Multimodal Learning and Reasoning for Visual Question Answering
neurips,2017,1,375,Jiashi,Feng,nus,National University of Singapore,elefjia@nus.edu.sg,Multimodal Learning and Reasoning for Visual Question Answering
neurips,2017,0,1442,Yung-Kyun,Noh,snu,Seoul National University,nohyung@snu.ac.kr,Generative Local Metric Learning for Kernel Regression
neurips,2017,1,1442,Masashi,Sugiyama,u-tokyo,RIKEN / University of Tokyo,sugi@k.u-tokyo.ac.jp,Generative Local Metric Learning for Kernel Regression
neurips,2017,2,1442,Kee-Eung,Kim,kaist,KAIST,kekim@cs.kaist.ac.kr,Generative Local Metric Learning for Kernel Regression
neurips,2017,3,1442,Frank,Park,snu,Seoul National University,fcp@snu.ac.kr,Generative Local Metric Learning for Kernel Regression
neurips,2017,4,1442,Daniel,Lee,upenn,University of Pennsylvania,ddlee@seas.upenn.edu,Generative Local Metric Learning for Kernel Regression
neurips,2017,0,2434,Sang-Woo,Lee,snu,Seoul National University,slee@bi.snu.ac.kr,Overcoming Catastrophic Forgetting by Incremental Moment Matching
neurips,2017,1,2434,Jin-Hwa,Kim,snu,Seoul National University,jhkim@bi.snu.ac.kr,Overcoming Catastrophic Forgetting by Incremental Moment Matching
neurips,2017,2,2434,Jaehyun,Jun,snu,Seoul National University,jhjun@bi.snu.ac.kr,Overcoming Catastrophic Forgetting by Incremental Moment Matching
neurips,2017,3,2434,Jung-Woo,Ha,navercorp,"Clova, NAVER Corp.",jungwoo.ha@navercorp.com,Overcoming Catastrophic Forgetting by Incremental Moment Matching
neurips,2017,4,2434,Byoung-Tak,Zhang,snu,Seoul National University & Surromind Robotics,btzhang@bi.snu.ac.kr,Overcoming Catastrophic Forgetting by Incremental Moment Matching
neurips,2017,0,2767,Xiangru,Lian,yandex,University of Rochester,xiangru@yandex.com,Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent
neurips,2017,1,2767,Ce,Zhang,ethz,ETH Zurich,ce.zhang@inf.ethz.ch,Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent
neurips,2017,2,2767,Huan,Zhang,gmail,,victzhang@gmail.com,Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent
neurips,2017,3,2767,Cho-Jui,Hsieh,ucdavis,UC Davis,chohsieh@ucdavis.edu,Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent
neurips,2017,4,2767,Wei,Zhang,ibm,IBM T.J.Watson Research Center,weiz@us.ibm.com,Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent
neurips,2017,5,2767,Ji,Liu,gmail,University of Rochester,ji.liu.uwisc@gmail.com,Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent
neurips,2017,0,688,Simon,Du,cmu,Carnegie Mellon University,ssdu@cs.cmu.edu,Gradient Descent Can Take Exponential Time to Escape Saddle Points
neurips,2017,1,688,Chi,Jin,berkeley,UC Berkeley,chijin@berkeley.edu,Gradient Descent Can Take Exponential Time to Escape Saddle Points
neurips,2017,2,688,Jason,Lee,usc,USC,jasonlee@marshall.usc.edu,Gradient Descent Can Take Exponential Time to Escape Saddle Points
neurips,2017,3,688,Michael,Jordan,berkeley,UC Berkeley,jordan@cs.berkeley.edu,Gradient Descent Can Take Exponential Time to Escape Saddle Points
neurips,2017,4,688,Aarti,Singh,cmu,CMU,bapoczos@cs.cmu.edu,Gradient Descent Can Take Exponential Time to Escape Saddle Points
neurips,2017,5,688,Barnabas,Poczos,cmu,Carnegie Mellon University,aartisingh@cmu.edu,Gradient Descent Can Take Exponential Time to Escape Saddle Points
neurips,2017,0,2333,Yunpeng,Chen,,National University of Singapore,,Dual Path Networks
neurips,2017,1,2333,Jianan,Li,,Beijing Institute of Technology,,Dual Path Networks
neurips,2017,2,2333,Huaxin,Xiao,,NUDT,,Dual Path Networks
neurips,2017,3,2333,Xiaojie,Jin,,National University of Singapore & Snap Research,,Dual Path Networks
neurips,2017,4,2333,Shuicheng,Yan,,Qihoo 360 AI Institute,,Dual Path Networks
neurips,2017,5,2333,Jiashi,Feng,,National University of Singapore,,Dual Path Networks
neurips,2017,0,1980,Laurence,Aitchison,gmail,University of Cambridge,laurence.aitchison@gmail.com,Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit
neurips,2017,1,1980,Lloyd,Russell,gmail,University College London,llerussell@gmail.com,Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit
neurips,2017,2,1980,Adam,Packer,gmail,University College London,adampacker@gmail.com,Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit
neurips,2017,3,1980,Jinyao,Yan,hhmi,Janelia Research Campus,yanj11@janelia.hhmi.org,Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit
neurips,2017,4,1980,Philippe,Castonguay,gmail,University of Montreal,ph.castonguay@gmail.com,Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit
neurips,2017,5,1980,Michael,Hausser,ucl,UCL,m.hausser@ucl.ac.uk,Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit
neurips,2017,6,1980,Srinivas,Turaga,hhmi,"Janelia Research Campus, Howard Hughes Medical Institute",turagas@janelia.hhmi.org,Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit
neurips,2017,0,2086,Jaouad,Mourtada,polytechnique,Ecole Polytechnique,jaouad.mourtada@polytechnique.edu,Universal consistency and minimax rates for online Mondrian Forests
neurips,2017,1,2086,Stéphane,Gaïffas,polytechnique,Ecole polytechnique,stéphane.gaiffas@polytechnique.edu,Universal consistency and minimax rates for online Mondrian Forests
neurips,2017,2,2086,Erwan,Scornet,polytechnique,Ecole Polytechnique,erwan.scornet@polytechnique.edu,Universal consistency and minimax rates for online Mondrian Forests
neurips,2017,0,3235,David,Lopez-Paz,fb,Facebook AI Research,dlp@fb.com,Gradient Episodic Memory for Continual Learning
neurips,2017,1,3235,Marc'Aurelio,Ranzato,fb,Facebook,ranzato@fb.com,Gradient Episodic Memory for Continual Learning
neurips,2017,0,2671,Ching-An,Cheng,gatech,Georgia Tech,cacheng@gatech.edu,Variational Inference for Gaussian Process Models with Linear Complexity
neurips,2017,1,2671,Byron,Boots,gatech,Georgia Tech / Google Brain,bboots@cc.gatech.edu,Variational Inference for Gaussian Process Models with Linear Complexity
neurips,2017,0,1321,Aidan,Gomez,toronto,University of Toronto,aidan@cs.toronto.edu,The Reversible Residual Network: Backpropagation Without Storing Activations
neurips,2017,1,1321,Mengye,Ren,toronto,University of Toronto,mren@cs.toronto.edu,The Reversible Residual Network: Backpropagation Without Storing Activations
neurips,2017,2,1321,Raquel,Urtasun,toronto,University of Toronto,urtasun@cs.toronto.edu,The Reversible Residual Network: Backpropagation Without Storing Activations
neurips,2017,3,1321,Roger,Grosse,toronto,University of Toronto,rgrosse@cs.toronto.edu,The Reversible Residual Network: Backpropagation Without Storing Activations
neurips,2017,0,1861,Joseph,Suarez,stanford,Stanford University,joseph15@stanford.edu,Language Modeling with Recurrent Highway Hypernetworks
neurips,2017,0,164,Haotian,Pang,,Princeton University,,Parametric Simplex Method for Sparse Learning
neurips,2017,1,164,Han,Liu,,Tencent AI Lab,,Parametric Simplex Method for Sparse Learning
neurips,2017,2,164,Robert,Vanderbei,,Princeton University,,Parametric Simplex Method for Sparse Learning
neurips,2017,3,164,Tuo,Zhao,,Georgia Tech,,Parametric Simplex Method for Sparse Learning
neurips,2017,0,3298,Chris,Maddison,,Oxford,,Filtering Variational Objectives
neurips,2017,1,3298,John,Lawson,,Google Brain,,Filtering Variational Objectives
neurips,2017,2,3298,George,Tucker,,Google Brain,,Filtering Variational Objectives
neurips,2017,3,3298,Nicolas,Heess,,Google DeepMind,,Filtering Variational Objectives
neurips,2017,4,3298,Mohammad,Norouzi,,,,Filtering Variational Objectives
neurips,2017,5,3298,Andriy,Mnih,,DeepMind,,Filtering Variational Objectives
neurips,2017,6,3298,Arnaud,Doucet,,Oxford,,Filtering Variational Objectives
neurips,2017,7,3298,Yee,Teh,,DeepMind,,Filtering Variational Objectives
neurips,2017,0,1608,Nan,Ding,google,Google,dingnan@google.com,Cold-Start Reinforcement Learning with Softmax Policy Gradient
neurips,2017,1,1608,Radu,Soricut,google,Google,rsoricut@google.com,Cold-Start Reinforcement Learning with Softmax Policy Gradient
neurips,2017,0,1564,Ofir,Nachum,google,Google,ofirnachum@google.com,Bridging the Gap Between Value and Policy Based Reinforcement Learning
neurips,2017,1,1564,Mohammad,Norouzi,google,,mnorouzi@google.com,Bridging the Gap Between Value and Policy Based Reinforcement Learning
neurips,2017,2,1564,Kelvin,Xu,google,Google,kelvinxx@google.com,Bridging the Gap Between Value and Policy Based Reinforcement Learning
neurips,2017,3,1564,Dale,Schuurmans,ualberta,Google,daes@ualberta.ca,Bridging the Gap Between Value and Policy Based Reinforcement Learning
neurips,2017,0,3132,Tao,Sun,163,National university of defense technology,nudtsuntao@163.com,Asynchronous Coordinate Descent under More Realistic Assumptions
neurips,2017,1,3132,Robert,Hannah,ucla,UCLA,RobertHannah89@math.ucla.edu,Asynchronous Coordinate Descent under More Realistic Assumptions
neurips,2017,2,3132,Wotao,Yin,ucla,"University of California, Los Angeles",wotaoyin@math.ucla.edu,Asynchronous Coordinate Descent under More Realistic Assumptions
neurips,2017,0,2786,Yogatheesan,Varatharajah,,University of Illinois at Urbana Champaign,,"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms"
neurips,2017,1,2786,Min Jin,Chong,,University of Illinois at Urbana-Champaign,,"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms"
neurips,2017,2,2786,Krishnakant,Saboo,,,,"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms"
neurips,2017,3,2786,Brent,Berry,,Mayo Clinic,,"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms"
neurips,2017,4,2786,Benjamin,Brinkmann,,Mayo Clinic,,"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms"
neurips,2017,5,2786,Gregory,Worrell,,"Mayo Clinic, Rochester",,"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms"
neurips,2017,6,2786,Ravishankar,Iyer,,,,"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms"
neurips,2017,0,1276,Zhongwen,Xu,google,DeepMind,zhongwen@google.com,Natural Value Approximators: Learning when to Trust Past Estimates
neurips,2017,1,1276,Joseph,Modayil,google,Deepmind,modayil@google.com,Natural Value Approximators: Learning when to Trust Past Estimates
neurips,2017,2,1276,Hado,van Hasselt,google,DeepMind,hado@google.com,Natural Value Approximators: Learning when to Trust Past Estimates
neurips,2017,3,1276,Andre,Barreto,google,DeepMind,andrebarreto@google.com,Natural Value Approximators: Learning when to Trust Past Estimates
neurips,2017,4,1276,David,Silver,google,DeepMind,davidsilver@google.com,Natural Value Approximators: Learning when to Trust Past Estimates
neurips,2017,5,1276,Tom,Schaul,google,DeepMind,schaul@google.com,Natural Value Approximators: Learning when to Trust Past Estimates
neurips,2017,0,2588,Garrett,Andersen,prowler,PROWLER.io,garrett@prowler.io,Active Exploration for Learning Symbolic Representations
neurips,2017,1,2588,George,Konidaris,brown,Brown University,gdk@cs.brown.edu,Active Exploration for Learning Symbolic Representations
neurips,2017,0,2439,Kiran,Garimella,aalto,Aalto University,kiran.garimella@aalto.fi,Balancing information exposure in social networks
neurips,2017,1,2439,Aristides,Gionis,aalto,Aalto University,aristides.gionis@aalto.fi,Balancing information exposure in social networks
neurips,2017,2,2439,Nikos,Parotsidis,uniroma2,University of Rome Tor Vergata,nikos.parotsidis@uniroma2.it,Balancing information exposure in social networks
neurips,2017,3,2439,Nikolaj,Tatti,aalto,Aalto University,nikolaj.tatti@aalto.fi,Balancing information exposure in social networks
neurips,2017,0,2134,Damien,Scieur,inria,INRIA - ENS,damien.scieur@inria.fr,Nonlinear Acceleration of Stochastic Algorithms
neurips,2017,1,2134,Francis,Bach,inria,Inria,francis.bach@inria.fr,Nonlinear Acceleration of Stochastic Algorithms
neurips,2017,2,2134,Alexandre,d'Aspremont,ens,CNRS - Ecole Normale Supérieure,aspremon@ens.fr,Nonlinear Acceleration of Stochastic Algorithms
neurips,2017,0,1494,Mikhail,Yurochkin,umich,University of Michigan,moonfolk@umich.edu,Multi-way Interacting Regression via Factorization Machines
neurips,2017,1,1494,XuanLong,Nguyen,umich,University of Michigan,xuanlong@umich.edu,Multi-way Interacting Regression via Factorization Machines
neurips,2017,2,1494,nikolaos,Vasiloglou,logicblox,LogicBlox,nikolaos.vasiloglou@logicblox.com,Multi-way Interacting Regression via Factorization Machines
neurips,2017,0,2328,Arun,Suggala,,Carnegie Mellon University,,The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities
neurips,2017,1,2328,Mladen,Kolar,,University of Chicago,,The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities
neurips,2017,2,2328,Pradeep,Ravikumar,,Carnegie Mellon University,,The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities
neurips,2017,0,1200,Jamie,Hayes,ucl,University College London,j.hayes@cs.ucl.ac.uk,Generating steganographic images via adversarial training
neurips,2017,1,1200,George,Danezis,ucl,University College London,g.danezis@ucl.ac.uk,Generating steganographic images via adversarial training
neurips,2017,0,982,Fei,Xia,stanford,Stanford University,feixia@stanford.edu,NeuralFDR: Learning Discovery Thresholds from Hypothesis Features
neurips,2017,1,982,Martin,Zhang,stanford,Stanford University,jinye@stanford.edu,NeuralFDR: Learning Discovery Thresholds from Hypothesis Features
neurips,2017,2,982,James,Zou,stanford,Stanford,jamesz@stanford.edu,NeuralFDR: Learning Discovery Thresholds from Hypothesis Features
neurips,2017,3,982,David,Tse,stanford,Stanford University,dntse@stanford.edu,NeuralFDR: Learning Discovery Thresholds from Hypothesis Features
neurips,2017,0,1004,Tor,Lattimore,gmail,DeepMind,tor.lattimore@gmail.com,A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis
neurips,2017,0,3108,Junhyuk,Oh,,University of Michigan,,Value Prediction Network
neurips,2017,1,3108,Satinder,Singh,,University of Michigan,,Value Prediction Network
neurips,2017,2,3108,Honglak,Lee,,Google / U. Michigan,,Value Prediction Network
neurips,2017,0,575,Jaime,Ide,,Yale University,,Detrended Partial Cross Correlation for Brain Connectivity Analysis
neurips,2017,1,575,Fábio,Cappabianco,,Federal University of Sao Paulo,,Detrended Partial Cross Correlation for Brain Connectivity Analysis
neurips,2017,2,575,Fabio,Faria,,Federal University of Sao Paulo,,Detrended Partial Cross Correlation for Brain Connectivity Analysis
neurips,2017,3,575,Chiang-shan,Li,,Yale University,,Detrended Partial Cross Correlation for Brain Connectivity Analysis
