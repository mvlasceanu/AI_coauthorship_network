{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T16:48:17.389579Z",
     "start_time": "2019-08-22T16:48:15.021522Z"
    }
   },
   "outputs": [],
   "source": [
    "# from preprocessing import tokenize, export_to_csv\n",
    "from gsdmm import MovieGroupProcess\n",
    "# from topic_allocation import top_words, topic_attribution\n",
    "# from visualisation import plot_topic_notebook, save_topic_html\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import pickle\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import ast\n",
    "import os, re, itertools\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Short-text topic modeling on ML paper titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (PILOT DATASET) 20NewsGroups\n",
    "\n",
    "Note: loadtime is intense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T16:48:21.581187Z",
     "start_time": "2019-08-22T16:48:20.019532Z"
    }
   },
   "outputs": [],
   "source": [
    "# cats = ['talk.politics.mideast', 'comp.windows.x', 'sci.space']\n",
    "\n",
    "# newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "# newsgroups_train_subject = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "# data = newsgroups_train.data\n",
    "# data_subject = newsgroups_train_subject.data\n",
    "\n",
    "# targets = newsgroups_train.target.tolist()\n",
    "# target_names = newsgroups_train.target_names\n",
    "\n",
    "# # Let's see if our topics are evenly distributed\n",
    "# df_targets = pd.DataFrame({'targets': targets})\n",
    "# order_list = df_targets.targets.value_counts()\n",
    "# order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T16:48:32.623638Z",
     "start_time": "2019-08-22T16:48:32.598981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>topic_true_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elevator to the top floor Reading from a Amoco...</td>\n",
       "      <td>1</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title for XTerm Yet again,the escape sequences...</td>\n",
       "      <td>0</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From Israeli press. Madness. Before getting ex...</td>\n",
       "      <td>2</td>\n",
       "      <td>mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accounts of Anti-Armenian Human Right Violatio...</td>\n",
       "      <td>2</td>\n",
       "      <td>mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many israeli soldiers does it take to kill...</td>\n",
       "      <td>2</td>\n",
       "      <td>mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  topic_id topic_true_name\n",
       "0  Elevator to the top floor Reading from a Amoco...         1           space\n",
       "1  Title for XTerm Yet again,the escape sequences...         0               x\n",
       "2  From Israeli press. Madness. Before getting ex...         2         mideast\n",
       "3  Accounts of Anti-Armenian Human Right Violatio...         2         mideast\n",
       "4  How many israeli soldiers does it take to kill...         2         mideast"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def extract_first_sentence(data_subject):\n",
    "#     list_first_sentence = []\n",
    "#     for text icats = ['talk.politics.mideast', 'comp.windows.x', 'sci.space']\n",
    "\n",
    "# newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "# newsgroups_train_subject = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "# data = newsgroups_train.data\n",
    "# data_subject = newsgroups_train_subject.data\n",
    "\n",
    "# targets = newsgroups_train.target.tolist()\n",
    "# target_names = newsgroups_train.target_names\n",
    "\n",
    "# # Let's see if our topics are evenly distributed\n",
    "# df_targets = pd.DataFrame({'targets': targets})\n",
    "# order_list = df_targets.targets.value_counts()\n",
    "# order_listn data:\n",
    "#         first_sentence = text.split(\".\")[0].replace(\"\\n\", \"\")\n",
    "#         list_first_sentence.append(first_sentence)\n",
    "#     return list_first_sentence\n",
    "\n",
    "# def extract_subject(data):\n",
    "#     c = 0\n",
    "#     s = \"Subject:\"\n",
    "#     list_subjects = []\n",
    "#     for new in data_subject:    \n",
    "#         lines = new.split(\"\\n\")\n",
    "#         b = 0 # loop out at the first \"Subject:\", they may be several and we want first one only\n",
    "#         for line in lines:\n",
    "#             if s in line and b == 0:\n",
    "#                 subject = \" \".join(line.split(\":\")[1:]).strip()\n",
    "#                 subject = subject.replace('Re', '').strip()\n",
    "#                 list_subjects.append(subject)\n",
    "#                 c += 1\n",
    "#                 b = 1\n",
    "#     return list_subjects\n",
    "    \n",
    "# def concatenate(list_first_sentence, list_subjects):\n",
    "#     list_docs = []\n",
    "#     for i in range(len(list_first_sentence)):\n",
    "#         list_docs.append(list_subjects[i] + \" \" + list_first_sentence[i])\n",
    "#     return list_docs\n",
    "\n",
    "\n",
    "# list_first_sentence = extract_first_sentence(data)\n",
    "# list_subjects = extract_subject(data_subject)\n",
    "# list_docs = concatenate(list_first_sentence, list_subjects)\n",
    "\n",
    "# df = pd.DataFrame(columns=['content', 'topic_id', 'topic_true_name'])\n",
    "# df['content'] = list_docs\n",
    "# df['topic_id'] = targets\n",
    "\n",
    "# def true_topic_name(x, target_names):\n",
    "#     return target_names[x].split('.')[-1]\n",
    "\n",
    "# df['topic_true_name'] = df['topic_id'].apply(lambda x: true_topic_name(x, target_names))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML journal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T16:48:29.738066Z",
     "start_time": "2019-08-22T16:48:29.704974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computational Perspectives on Social Good and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Computational Model of Commonsense Moral Dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ethical Challenges in Data-Driven Dialogue Sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exploiting Moral Values to Choose the Right Norms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norms, Rewards, and the Intentional Stance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  Computational Perspectives on Social Good and ...\n",
       "1  A Computational Model of Commonsense Moral Dec...\n",
       "2  Ethical Challenges in Data-Driven Dialogue Sys...\n",
       "3  Exploiting Moral Values to Choose the Right Norms\n",
       "4         Norms, Rewards, and the Intentional Stance"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originaldf = pd.read_excel(os.path.expanduser(\"data.xlsx\"))\n",
    "\n",
    "df = pd.DataFrame({\"content\": df.title[~df.title.isna()].unique()})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001841016C0A8>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001841016C408>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001841027A208>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001841027A048>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x00000184136D0C88>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x00000184153AA548>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computational Perspectives on Social Good and ...</td>\n",
       "      <td>[computational, perspectives, social, good, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Computational Model of Commonsense Moral Dec...</td>\n",
       "      <td>[computational, model, commonsense, moral, dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ethical Challenges in Data-Driven Dialogue Sys...</td>\n",
       "      <td>[ethical, challenge, data, driven, dialogue, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exploiting Moral Values to Choose the Right Norms</td>\n",
       "      <td>[exploit, moral, value, choose, right, norm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norms, Rewards, and the Intentional Stance</td>\n",
       "      <td>[norm, rewards, intentional, stance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Computational Perspectives on Social Good and ...   \n",
       "1  A Computational Model of Commonsense Moral Dec...   \n",
       "2  Ethical Challenges in Data-Driven Dialogue Sys...   \n",
       "3  Exploiting Moral Values to Choose the Right Norms   \n",
       "4         Norms, Rewards, and the Intentional Stance   \n",
       "\n",
       "                                              tokens  \n",
       "0  [computational, perspectives, social, good, ac...  \n",
       "1  [computational, model, commonsense, moral, dec...  \n",
       "2  [ethical, challenge, data, driven, dialogue, s...  \n",
       "3       [exploit, moral, value, choose, right, norm]  \n",
       "4               [norm, rewards, intentional, stance]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create documents for all titles\n",
    "docs = list(nlp.pipe(df.content))\n",
    "\n",
    "# lemmatize each title\n",
    "def lemmatize(doc):\n",
    "    # Take the `token.lemma_` of each non-stop word\n",
    "    L = [re.sub(r\"[^a-zA-Z0-9]\", r\"\", token.lemma_.lower()) for token in doc if not (token.is_stop or token.is_punct)]\n",
    "    L = list(filter(None, L))\n",
    "    return L\n",
    "df['tokens'] = list(map(lemmatize, docs))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T16:48:58.174722Z",
     "start_time": "2019-08-22T16:48:58.163603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of token: 17\n",
      "Mean number of token: 6.2511230907457325\n",
      "Voc size: 4432\n",
      "Number of documents: 4452\n"
     ]
    }
   ],
   "source": [
    "print(\"Max number of token:\", df.tokens.apply(len).max())\n",
    "print(\"Mean number of token:\", df.tokens.apply(len).mean())\n",
    "\n",
    "# Input format for the model : list of strings (list of tokens)\n",
    "docs = df.tokens.to_list()\n",
    "vocab = set(itertools.chain.from_iterable(docs))\n",
    "n_terms = len(vocab)\n",
    "\n",
    "print(\"Voc size:\", n_terms)\n",
    "print(\"Number of documents:\", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The value of β thus affects the granularity of the model: a corpus of documents can be sensibly factorized into a set of topics at several different scales, and the particular scale assessed by the model will be set by β. With scientific documents, a large value of β would lead the model to find a relatively small number of topics, perhaps at the level of scientific disciplines, whereas smaller values of β will produce more topics that address specific areas of research.\n",
    "\n",
    "Finding scientific topics, Thomas L. Griffiths and Mark Steyvers, PNAS 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T09:12:53.917711Z",
     "start_time": "2019-08-21T09:07:51.566603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 3738 clusters with 20 clusters populated\n",
      "In stage 1: transferred 2296 clusters with 20 clusters populated\n",
      "In stage 2: transferred 1688 clusters with 20 clusters populated\n",
      "In stage 3: transferred 1400 clusters with 20 clusters populated\n",
      "In stage 4: transferred 1272 clusters with 20 clusters populated\n",
      "In stage 5: transferred 1238 clusters with 20 clusters populated\n",
      "In stage 6: transferred 1136 clusters with 20 clusters populated\n",
      "In stage 7: transferred 1068 clusters with 20 clusters populated\n",
      "In stage 8: transferred 1076 clusters with 20 clusters populated\n",
      "In stage 9: transferred 1046 clusters with 20 clusters populated\n",
      "In stage 10: transferred 972 clusters with 20 clusters populated\n",
      "In stage 11: transferred 947 clusters with 20 clusters populated\n",
      "In stage 12: transferred 941 clusters with 20 clusters populated\n",
      "In stage 13: transferred 901 clusters with 20 clusters populated\n",
      "In stage 14: transferred 943 clusters with 20 clusters populated\n",
      "In stage 15: transferred 895 clusters with 20 clusters populated\n",
      "In stage 16: transferred 872 clusters with 20 clusters populated\n",
      "In stage 17: transferred 899 clusters with 20 clusters populated\n",
      "In stage 18: transferred 910 clusters with 20 clusters populated\n",
      "In stage 19: transferred 874 clusters with 20 clusters populated\n",
      "In stage 20: transferred 843 clusters with 20 clusters populated\n",
      "In stage 21: transferred 860 clusters with 20 clusters populated\n",
      "In stage 22: transferred 854 clusters with 20 clusters populated\n",
      "In stage 23: transferred 855 clusters with 20 clusters populated\n",
      "In stage 24: transferred 872 clusters with 20 clusters populated\n",
      "In stage 25: transferred 847 clusters with 20 clusters populated\n",
      "In stage 26: transferred 856 clusters with 20 clusters populated\n",
      "In stage 27: transferred 808 clusters with 20 clusters populated\n",
      "In stage 28: transferred 865 clusters with 20 clusters populated\n",
      "In stage 29: transferred 847 clusters with 20 clusters populated\n",
      "In stage 30: transferred 849 clusters with 20 clusters populated\n",
      "In stage 31: transferred 850 clusters with 20 clusters populated\n",
      "In stage 32: transferred 835 clusters with 20 clusters populated\n",
      "In stage 33: transferred 820 clusters with 20 clusters populated\n",
      "In stage 34: transferred 833 clusters with 20 clusters populated\n",
      "In stage 35: transferred 789 clusters with 20 clusters populated\n",
      "In stage 36: transferred 809 clusters with 20 clusters populated\n",
      "In stage 37: transferred 833 clusters with 20 clusters populated\n",
      "In stage 38: transferred 844 clusters with 20 clusters populated\n",
      "In stage 39: transferred 854 clusters with 20 clusters populated\n",
      "In stage 40: transferred 788 clusters with 20 clusters populated\n",
      "In stage 41: transferred 808 clusters with 20 clusters populated\n",
      "In stage 42: transferred 809 clusters with 20 clusters populated\n",
      "In stage 43: transferred 802 clusters with 20 clusters populated\n",
      "In stage 44: transferred 814 clusters with 20 clusters populated\n",
      "In stage 45: transferred 796 clusters with 20 clusters populated\n",
      "In stage 46: transferred 825 clusters with 20 clusters populated\n",
      "In stage 47: transferred 848 clusters with 20 clusters populated\n",
      "In stage 48: transferred 809 clusters with 20 clusters populated\n",
      "In stage 49: transferred 838 clusters with 20 clusters populated\n"
     ]
    }
   ],
   "source": [
    "# Train a new model \n",
    "\n",
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "mgp = MovieGroupProcess(K=20, alpha=0.2, beta=0.05, n_iters=50)\n",
    "\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "n_docs = len(docs)\n",
    "\n",
    "# Fit the model on the data given the chosen seeds\n",
    "y = mgp.fit(docs, n_terms)\n",
    "\n",
    "# Save model\n",
    "with open('data/gsdmm.model', \"wb\") as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T16:49:01.839951Z",
     "start_time": "2019-08-22T16:49:01.824734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topics : [169  96  97 200  88 119 284 133 317 300 268  83 333 106 503 151 326 201\n",
      " 256 422]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [14 19 12 16  8  9  6 10 18 17]\n",
      "********************\n",
      "Topic 14:  [('learning', 72), ('regression', 53), ('estimation', 46), ('optimal', 41), ('robust', 37)] \n",
      " -----------------------------\n",
      "Topic 19:  [('learning', 87), ('neural', 78), ('gaussian', 60), ('inference', 50), ('processes', 47)] \n",
      " -----------------------------\n",
      "Topic 12:  [('learning', 143), ('deep', 62), ('networks', 39), ('neural', 36), ('learn', 35)] \n",
      " -----------------------------\n",
      "Topic 16:  [('learning', 88), ('regret', 42), ('optimization', 38), ('online', 36), ('optimal', 31)] \n",
      " -----------------------------\n",
      "Topic 8:  [('learning', 165), ('reinforcement', 74), ('policy', 37), ('multi', 31), ('model', 31)] \n",
      " -----------------------------\n",
      "Topic 9:  [('neural', 145), ('networks', 103), ('network', 77), ('deep', 67), ('training', 35)] \n",
      " -----------------------------\n",
      "Topic 6:  [('learning', 67), ('image', 39), ('end', 35), ('adversarial', 28), ('generative', 28)] \n",
      " -----------------------------\n",
      "Topic 10:  [('neural', 70), ('networks', 52), ('network', 47), ('image', 45), ('deep', 43)] \n",
      " -----------------------------\n",
      "Topic 18:  [('optimization', 63), ('stochastic', 58), ('gradient', 52), ('descent', 35), ('convex', 29)] \n",
      " -----------------------------\n",
      "Topic 17:  [('ai', 43), ('fairness', 32), ('artificial', 21), ('intelligence', 20), ('learning', 19)] \n",
      " -----------------------------\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "# Show the top 5 words by cluster, it helps to make the topic_dict below\n",
    "for i in top_index:\n",
    "    ordered = {k: v for k, v in sorted(mgp.cluster_word_distribution[i].items(), key=lambda item: item[1], reverse=True)}\n",
    "    print(\"Topic %d: \" % i, list(ordered.items())[:5], \"\\n\", \"-\"*29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H[\\mathbf{p}]=-\\sum_i p_i\\log p_i=\\sum_i p_i\\log 1/p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>topicprobs</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computational Perspectives on Social Good and ...</td>\n",
       "      <td>[computational, perspectives, social, good, ac...</td>\n",
       "      <td>[1.7840914238946043e-10, 1.8815398684715577e-0...</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Computational Model of Commonsense Moral Dec...</td>\n",
       "      <td>[computational, model, commonsense, moral, dec...</td>\n",
       "      <td>[7.753955514186043e-08, 1.37399502369604e-12, ...</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ethical Challenges in Data-Driven Dialogue Sys...</td>\n",
       "      <td>[ethical, challenge, data, driven, dialogue, s...</td>\n",
       "      <td>[1.463508575219674e-11, 1.5588810036292163e-08...</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exploiting Moral Values to Choose the Right Norms</td>\n",
       "      <td>[exploit, moral, value, choose, right, norm]</td>\n",
       "      <td>[3.4714184616619764e-10, 1.4121115736217413e-0...</td>\n",
       "      <td>0.000341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norms, Rewards, and the Intentional Stance</td>\n",
       "      <td>[norm, rewards, intentional, stance]</td>\n",
       "      <td>[0.0008505911376271891, 3.943614451884207e-06,...</td>\n",
       "      <td>0.036749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Computational Perspectives on Social Good and ...   \n",
       "1  A Computational Model of Commonsense Moral Dec...   \n",
       "2  Ethical Challenges in Data-Driven Dialogue Sys...   \n",
       "3  Exploiting Moral Values to Choose the Right Norms   \n",
       "4         Norms, Rewards, and the Intentional Stance   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [computational, perspectives, social, good, ac...   \n",
       "1  [computational, model, commonsense, moral, dec...   \n",
       "2  [ethical, challenge, data, driven, dialogue, s...   \n",
       "3       [exploit, moral, value, choose, right, norm]   \n",
       "4               [norm, rewards, intentional, stance]   \n",
       "\n",
       "                                          topicprobs   entropy  \n",
       "0  [1.7840914238946043e-10, 1.8815398684715577e-0...  0.000004  \n",
       "1  [7.753955514186043e-08, 1.37399502369604e-12, ...  0.000017  \n",
       "2  [1.463508575219674e-11, 1.5588810036292163e-08...  0.000175  \n",
       "3  [3.4714184616619764e-10, 1.4121115736217413e-0...  0.000341  \n",
       "4  [0.0008505911376271891, 3.943614451884207e-06,...  0.036749  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"topicprobs\" in df.columns:\n",
    "    df = df.drop(labels=[\"topicprobs\", \"entropy\"], axis=\"columns\")\n",
    "df.insert(2, \"topicprobs\", df.tokens.apply(mgp.score))\n",
    "df.insert(3, \"entropy\", df.topicprobs.apply(stats.entropy))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"data/title_topicmodels.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
